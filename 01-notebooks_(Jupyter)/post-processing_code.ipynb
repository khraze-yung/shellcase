{"cells":[{"cell_type":"markdown","metadata":{"id":"6197c579"},"source":["# DATA CLEANING SCRIPT - (For Metabase CSVs)"],"id":"6197c579"},{"cell_type":"markdown","metadata":{"id":"71ab2b0c"},"source":["## üìò INSTRUCTIONS\n","\n","\n","### üîß SETUP YOUR FILE PATHS\n","\n","Before running the notebook, please provide the following input and output file paths in the **\"FILE PATHS\"** section:\n","\n","### üìù INPUT INFORMATION (Required)\n","\n","- **Country**: Type the source country name (`\"Zimbabwe\"` or `\"ZIM\"` or `\"Malawi\"` or `\"MWI\"`) ‚Äî case insensitive.\n","- **CSV File to Clean**: Full file path to the raw `.csv` file that needs cleaning.\n","- **Country-Specific Data Dictionary**: Excel file containing the variable mappings for your chosen country.\n","  - üîó [MWI Dictionary (Google Sheet)](https://docs.google.com/spreadsheets/d/132qyY3YDJPto6NISo6Bg6mcfhTeya5o1njVGZPWbAaM/edit?gid=0#gid=0)\n","  - üîó [ZIM Dictionary (Google Sheet)](https://docs.google.com/spreadsheets/d/1IDfRghpXX971MYcd7mLkYX4TiN2ydUV2tOGsdTcHosI/edit?gid=913417091#gid=913417091)\n","- **Validation Dictionary**: Excel file that holds cross-country validation rules.\n","  - üîó [Validation Dictionary (Google Sheet)](https://docs.google.com/spreadsheets/d/1_OLakvc_kNANlZ3MDi5Me6mBDaRDmo6Ma-gu01Z-efE/edit?gid=0#gid=0)\n","\n","> üì• **Please download all the dictionaries above as Excel (.xlsx) files before running the notebook.**\n","\n","### üíæ OUTPUT INFORMATION (Required)\n","- **Pickle Output Path**: Where to save the cleaned data as a `.pkl` file.\n","- **CSV Output Path**: Where to save the cleaned data as a `.csv` file.\n","\n","\n","## ‚ñ∂Ô∏è HOW TO RUN\n","\n","Once all file paths have been filled in:\n","1. Click **`Run All`** in the top menu.\n","2. Wait for the notebook to finish execution ‚Äî the output files will be saved to the specified locations.\n","\n","> üõ†Ô∏è Make sure all paths are correct before running to avoid errors.\n","---\n"],"id":"71ab2b0c"},{"cell_type":"markdown","metadata":{"id":"6e5094ca"},"source":["## CONFIGURATION"],"id":"6e5094ca"},{"cell_type":"markdown","metadata":{"id":"77f50efc"},"source":["### FILE PATHS"],"id":"77f50efc"},{"cell_type":"markdown","metadata":{"id":"be7b5e3f"},"source":["#### Mandatory File Paths"],"id":"be7b5e3f"},{"cell_type":"code","execution_count":1,"metadata":{"id":"30a2cda2","executionInfo":{"status":"ok","timestamp":1750152465326,"user_tz":-120,"elapsed":55,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# INPUT DATA\n","#-----------------\n","# Specify Source Country - \"zim\"  for \"zimbabwe\"/   \"mwi\"  for \"malawi\"\n","country = \"zim\"\n","\n","# File To Clean\n","csv_filepath = r\" \"# <--------------- Put csv path here"],"id":"30a2cda2"},{"cell_type":"code","execution_count":2,"metadata":{"id":"dfbf645f","executionInfo":{"status":"ok","timestamp":1750152465400,"user_tz":-120,"elapsed":27,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# DICTIONARIES\n","#------------------\n","# Data Dictionary ZIM\n","dict_filepath_zim = r\" \"  # <--------------- Put Data dictionary path here\n","# Data Dictionary MWI\n","dict_filepath_mwi = r\" \" # <--------------- Put Data dictionary path here\n","# Validation Dictionary\n","feature_dict_filepath  = r\" \"  # <--------------- Put Validation dictionary path here"],"id":"dfbf645f"},{"cell_type":"code","execution_count":3,"metadata":{"id":"4baf5157","executionInfo":{"status":"ok","timestamp":1750152465446,"user_tz":-120,"elapsed":42,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# OUTPUT DATA\n","#------------------\n","# CLEAN File\n","merged_data_csv = r\" \" # <--------------- Put output csv path here\n","merged_data_pkl = r\" \" # <--------------- Put output pkl path here"],"id":"4baf5157"},{"cell_type":"markdown","metadata":{"id":"8f7eb444"},"source":["#### Optional File Paths"],"id":"8f7eb444"},{"cell_type":"markdown","metadata":{"id":"713111da"},"source":["##### 1st Stage Cleaning"],"id":"713111da"},{"cell_type":"code","execution_count":4,"metadata":{"id":"81698673","executionInfo":{"status":"ok","timestamp":1750152465452,"user_tz":-120,"elapsed":3,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# #-------REPORTS----------\n","# pre_process_report_filepath_zim = \" \"      # <--------------- Put preprocess report path column here\n","# column_cleaning_report_filepath_zim = \"  \" # <--------------- Put column cleaning report path column here\n","# forward_fill_report_path_zim = \" \"         # <--------------- Put forward filling report path here\n","# dtype_conversion_report_filepath_zim = \" \" # <--------------- Put dtype convertion report path here\n","# post_process_report_filepath_zim =     \" \" # <--------------- Put post process report path here\n","# report_frame_shift_zim = \" \"\n","# dropped_columns_zim = \" \"\n","# forward_fill_nones_report_zim = \" \"\n","\n","# #--------OUTPUTS-------------\n","# # frame Shift data\n","# frame_shift_csv_zim= \" \"  # <--------------- frame shift csv path here\n","# frame_shift_pkl_zim= \" \"  # <--------------- frame shift pkl path here\n","# # summary dictionary in excel\n","# output_csv_filepath_zim= \" \"   # <--------------- output summary csv\n","# output_excel_filepath_zim= \" \" # <--------------- output summary excel"],"id":"81698673"},{"cell_type":"markdown","metadata":{"id":"adadabea"},"source":["##### Numeric Validation"],"id":"adadabea"},{"cell_type":"code","execution_count":5,"metadata":{"id":"8a3bb0d3","executionInfo":{"status":"ok","timestamp":1750152465479,"user_tz":-120,"elapsed":25,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# # ------REPORTS--------\n","# # Flags - Features Flagged With Out of Range Values\n","# report_numeric_flags_zim = \" \"           # <--------------- File Path\n","# # Flags - Features Flagged With Out of Range Values After Fixes\n","# report_numeric_disallowed_ZIM = \" \"     # <--------------- File Path\n","# # Deleted Values - All deleted Non-Numeric Values\n","# report_dtypes_disallowed_ZIM = \" \"      # <--------------- File Path"],"id":"8a3bb0d3"},{"cell_type":"markdown","metadata":{"id":"17495fb0"},"source":["##### Boolean Validation"],"id":"17495fb0"},{"cell_type":"code","execution_count":6,"metadata":{"id":"36e05a97","executionInfo":{"status":"ok","timestamp":1750152465533,"user_tz":-120,"elapsed":29,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# # ------REPORTS--------\n","# # Deleted & Fixed Values - All Non-Boolean Values & ALL Standardized Boolean Values\n","# report_dtypes_disallowed_ZIM = r\" \" # <--------------- File Path"],"id":"36e05a97"},{"cell_type":"markdown","metadata":{"id":"fba3b88f"},"source":["##### Categorical Validation"],"id":"fba3b88f"},{"cell_type":"code","execution_count":7,"metadata":{"id":"ac759bc9","executionInfo":{"status":"ok","timestamp":1750152465607,"user_tz":-120,"elapsed":44,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# # ------REPORTS--------\n","# # flags\n","# report_non_numeric_flags_zim = r\" \"# <--------------- File Path\n","# # Value Replacements\n","\n","# # Disallowed Deletes\n","\n","# # Value Mapping"],"id":"ac759bc9"},{"cell_type":"markdown","metadata":{"id":"ccd51d37"},"source":["##### DateTime Validation"],"id":"ccd51d37"},{"cell_type":"code","execution_count":8,"metadata":{"id":"b4a20722","executionInfo":{"status":"ok","timestamp":1750152465612,"user_tz":-120,"elapsed":2,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# # ------REPORTS--------\n","# report_dtypes_disallowed_ZIM = \" \""],"id":"b4a20722"},{"cell_type":"code","execution_count":8,"metadata":{"id":"0d550d4a","executionInfo":{"status":"ok","timestamp":1750152465617,"user_tz":-120,"elapsed":3,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":[],"id":"0d550d4a"},{"cell_type":"markdown","metadata":{"id":"b6e660f0"},"source":["###\n","---"],"id":"b6e660f0"},{"cell_type":"markdown","metadata":{"id":"c59bf214"},"source":["## CLEANING & VALIDATION PIPELINE"],"id":"c59bf214"},{"cell_type":"markdown","metadata":{"id":"d00dcc7b"},"source":["### SETUP"],"id":"d00dcc7b"},{"cell_type":"markdown","metadata":{"id":"1e09e6a2"},"source":["#### Envrionment Setup"],"id":"1e09e6a2"},{"cell_type":"code","execution_count":9,"metadata":{"id":"4f3c6895","executionInfo":{"status":"ok","timestamp":1750152466556,"user_tz":-120,"elapsed":937,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# Import Neccessary Libraries & Dependencies\n","#--------------------------------------------\n","import pandas as pd\n","import numpy as np\n","import logging\n","import random\n","import re\n","import os\n","from datetime import datetime\n","from collections import defaultdict\n","from decimal import Decimal, InvalidOperation\n","from typing import Dict, Any, Optional, Collection\n","import time\n","\n","\n","# Warnings\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","\n","# Logging Configuration\n","# -------------------------\n","logging.basicConfig(\n","    level=logging.INFO,\n","    format='%(asctime)s %(levelname)s: %(message)s',\n","    handlers=[\n","        logging.FileHandler(\"data_cleaning.log\"),\n","        logging.StreamHandler()\n","    ]\n",")\n","\n","# Runtime\n","#--------------\n","notebook_start_time = time.time()\n"],"id":"4f3c6895"},{"cell_type":"markdown","metadata":{"id":"367b92fe"},"source":["#### Country-Switch Configuration"],"id":"367b92fe"},{"cell_type":"code","execution_count":10,"metadata":{"id":"910d7292","executionInfo":{"status":"ok","timestamp":1750152466758,"user_tz":-120,"elapsed":191,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# ‚îÄ‚îÄ‚îÄ LISTS & OBJECTS TO SWITCH ‚îÄ‚îÄ‚îÄ\n","\n","# NUMERIC FEATURES\n","#---------------------------\n","numeric_features_zim = [\"facility\",\"uid\",\"uniquekey\",\"startedatdischarge\", \"completedatdischarge\", \"ingestedatdischarge\",\n","# common features\n","\"admissionweight.value\", \"age.value\", \"agec.value\", \"aw.value\", \"birthweight.value\", \"bloodsugarmg.value\", \"bloodsugarmmol.value\", \"bsmg.value\", \"bsmmol.value\", \"bwtdis.value\", \"dischhr.value\", \"dischrr.value\",\n","\"dischsats.value\", \"dischtemp.value\", \"dischweight.value\", \"durationlab.value\", \"durcpr.value\", \"gestation.value\", \"gestbirth.value\", \"hr.value\", \"length.value\", \"lengthdis.value\", \"lengthofstay.value\", \"lpgluc.value\",\n","\"lpprot.value\", \"manualhr.value\", \"nnuadmtemp.value\", \"ofc.value\", \"ofcdis.value\", \"par.value\", \"resprate.value\", \"rr.value\", \"rrmother.value\", \"satsair.value\", \"satso2.value\", \"temperature.value\", \"thompscore.value\",\n","\"timespent\", \"timespentdischarge\",\"alb.value\", \"alb2.value\", \"alp1r.value\", \"alp2r.value\", \"antenatalcare.value\", \"apgar1.value\", \"apgar10.value\", \"apgar5.value\", \"balscore.value\", \"balscorewks.value\",\n","\"bbadur.value\", \"bcisol.value\", \"bili1r.value\", \"bilirubin2r.value\", \"bpdur.value\", \"ca1r.value\", \"calcium2r.value\", \"corrected.value\",\"creat1r.value\", \"creatinine2r.value\", \"crp1r.value\", \"crp2r.value\", \"cyanosis.value\",\n","\"downescr.value\", \"duramox.value\", \"duramp.value\", \"durcef.value\", \"durclox.value\", \"durgent.value\", \"durimi.value\", \"durmet.value\", \"durproc.value\", \"durvanc.value\", \"foedisttime.value\", \"gluc1r.value\", \"glucose2r.value\",\n","\"grav.value\", \"hb1r.value\", \"hb2r.value\", \"lchild.value\", \"mag1r.value\", \"magnesium2r.value\", \"matage.value\", \"matageyrs.value\", \"phos1r.value\", \"phosphate2r.value\", \"plt1r.value\", \"plt2r.value\", \"pot1r.value\",\n","\"potassium2r.value\", \"sod1r.value\", \"sodium2r.value\", \"transnumber.value\", \"txhb.value\", \"ur1r.value\", \"urea2r.value\", \"vlnumber.value\", \"wcc1r.value\", \"wcc2r.value\",\n","\"airentry.value\",\n","\"nmarmr.value\", \"nmbreast.value\", \"nmeye.value\", \"nmgen.value\", \"nmheelear.value\", \"nmlan.value\", \"nmplant.value\", \"nmpop.value\", \"nmposture.value\", \"nmscarf.value\", \"nmskin.value\", \"nmsquare.value\",\n","\"thompalert.value\", \"thompfeed.value\", \"thompfont.value\", \"thompgrasp.value\", \"thompmoro.value\", \"thomprefl.value\", \"thompresp.value\", \"thompseiz.value\", \"thomptone.value\"\n","# unique features zim\n","# problematic features zim\n","]\n","\n","numeric_features_mwi = [\"facility\",\"uid\",\"uniquekey\",\"startedatdischarge\", \"completedatdischarge\", \"ingestedatdischarge\",\n","# common features\n","\"admissionweight.value\", \"age.value\", \"agec.value\", \"apgar1.value\", \"apgar10.value\", \"apgar10dc.value\", \"apgar1dc.value\", \"apgar5.value\", \"apgar5dc.value\", \"balscore.value\", \"balscorewks.value\", \"bidageb.value\",\n","\"birthweight.value\", \"bloodsugarmg.value\", \"bloodsugarmmol.value\", \"bsmg.value\", \"bsmmol.value\", \"bwbid.value\", \"dischhr.value\", \"dischrr.value\", \"dischsats.value\", \"dischtemp.value\", \"dischweight.value\",\n","\"durationlab.value\", \"gestation.value\", \"hr.value\", \"lengthoflife.value\", \"lengthresus.value\", \"manualhr.value\", \"matageyrs.value\", \"modedeliverydc.value\", \"mothcell.value\", \"mothersatso2.value\", \"ofc.value\", \"par.value\",\n","\"paritydeadchildren.value\", \"paritylivingchildren.value\", \"rr.value\", \"satsair.value\", \"satso2.value\", \"temperature.value\", \"temperatureonarrival.value\", \"tempthermia.value\", \"thompscore.value\", \"lengthofstay.value\",\n","\"timespent\", \"timespentdischarge\", \"aw.value\", \"bw.value\",\n","\"fitsth.value\",\n","\"modedelivery.value\",\n","\"fontth.value\", \"graspth.value\", \"locth.value\", \"moroth.value\", \"postth.value\", \"respth.value\", \"suckth.value\", \"toneth.value\",\n","\"nmarmr.value\", \"nmbreast.value\", \"nmeye.value\", \"nmgen.value\", \"nmheelear.value\", \"nmlan.value\", \"nmplant.value\", \"nmpop.value\", \"nmposture.value\", \"nmscarf.value\", \"nmskin.value\", \"nmsquare.value\"\n","# unique features mwi\n","]\n","\n","\n","\n","# BOOLEAN FEATURES\n","#---------------------------\n","\n","bool_features_zim      = [\"facility\",\"uid\",\"uniquekey\",\"startedatdischarge\", \"completedatdischarge\", \"ingestedatdischarge\",\n","# commnon features\n","\"agea.value\", \"anmatsyphtreat.value\", \"babycrytriage.value\", \"bc.value\", \"bc2.value\", \"bcsens.value\", \"bf.value\", \"bili.value\", \"bili2.value\", \"birthplacesame.value\", \"bldgrpyn.value\", \"bone.value\", \"bone2.value\",\n","\"bsmonyn.value\", \"corddelay.value\", \"cpap.value\", \"crp.value\", \"crp2.value\", \"datevdrlsamehiv.value\", \"disccovidrisk.value\", \"dobyn.value\", \"dysmorphic.value\", \"envtemp.value\", \"fbc.value\", \"fbc2.value\", \"feversr.value\",\n","\"foedist.value\", \"foehrtdoc.value\", \"foehrtrec.value\", \"gluc.value\", \"gluc2.value\", \"haart.value\", \"hivpcr.value\", \"hivpcrinf.value\", \"hyposxyn.value\", \"hyposymptoms.value\", \"inorout.value\", \"kmc.value\",\n","\"matadm.value\", \"matadmit.value\", \"matbldgrpyn.value\", \"matchorio.value\", \"matdischarge.value\", \"mathivp.value\", \"mathivtest.value\", \"matsub.value\", \"mec.value\", \"mf.value\", \"mgso4.value\", \"murmur.value\", \"none.value\",\n","\"none2.value\", \"nvpazt.value\", \"nvpgiven.value\", \"nvplr.value\", \"phototherapy.value\", \"placebirthtown.value\", \"prom.value\", \"readmission.value\", \"revclin.value\", \"review.value\", \"riskcovid.value\", \"rom.value\",\n","\"rpr.value\", \"sent.value\", \"stethoscope.value\", \"surf.value\", \"talipessev.value\", \"torch.value\", \"transfusion.value\", \"travelhistory.value\", \"ue.value\", \"ue2.value\", \"ycolour.value\", \"brprobs.value\", \"externalsource.value\",\n","\"glustx.value\", \"typebirthot.value\"\n","# unique features zim\n","# problematic features\n","]\n","bool_features_mwi      = [\"facility\",\"uid\",\"uniquekey\",\"startedatdischarge\", \"completedatdischarge\", \"ingestedatdischarge\",\n","# commnon features\n","\"agea.value\", \"anmatsyphtreat.value\", \"babycrytriage.value\", \"bidagea.value\", \"biddobyn.value\", \"birthplacesame.value\", \"bsmonyn.value\", \"datevdrlsamehiv.value\", \"disccovidrisk.value\", \"dobyn.value\", \"dysmorphic.value\",\n","\"feversr.value\", \"haart.value\", \"healthed.value\", \"hyposymptoms.value\", \"inorout.value\", \"itn.value\", \"mathivtest.value\", \"motherpresent.value\", \"nvpgiven.value\", \"ortolani.value\", \"phototherapy.value\",\"readmission.value\",\n","\"revclin.value\", \"riskcovid.value\", \"stethoscope.value\", \"talipessev.value\", \"ycolour.value\", \"brprobs.value\", \"hyposxyn.value\", \"lbwbinarydischarge\", \"prematuritywithrespiratorydistresssyndrome.value\", \"symptomsyn.value\"\n","# unique features mwi\n","# problematic features mwi\n","]\n","\n","\n","\n","# CATEGORICAL FEATURES\n","#---------------------------\n","cat_features_zim       = [\"facility\",\"uid\",\"uniquekey\",\"startedatdischarge\", \"completedatdischarge\", \"ingestedatdischarge\",\n","# common features\n","\"abdomen.value\", \"activity.value\", \"admittedfrom.value\", \"admreason.value\", \"admreasonadd.value\", \"admward.value\", \"ageb.value\", \"agecat.value\", \"ageestimate.value\", \"anstercrse.value\", \"ansteroids.value\",\n","\"anus2.value\", \"anvdrl.value\", \"anvdrlreport.value\", \"anvdrlresult.value\", \"bbadel.value\", \"bbaloc.value\", \"bc1othr.value\", \"bc1r.value\", \"bc2r.value\", \"bcresist.value\",\n","\"birthfacility.value\", \"birthplace.value\", \"bsunit.value\", \"cadre.value\", \"cadredis.value\", \"causedeath.value\", \"chestausc.value\", \"colour.value\", \"contcausedeath.value\", \"corrected.value\", \"covidconfirmation.value\",\n","\"covidrepresults.value\", \"cprout.value\", \"crt.value\", \"crybirth.value\", \"csforg.value\", \"csreason.value\", \"dangersigns.value\", \"dangersigns2.value\", \"delivinter.value\", \"diagdis1.value\", \"diagnoses.value\",\n","\"downesinterp.value\", \"ethnicity.value\", \"feeddsch.value\", \"feedsadm.value\", \"feedsdsc.value\", \"femorals.value\", \"feverons.value\", \"firm.value\", \"foebrad.value\", \"foedec.value\", \"foetach.value\", \"folate.value\",\n","\"fontanelle.value\", \"furthertriage.value\", \"gastons.value\", \"gender.value\", \"genitalia.value\", \"goodprog.value\", \"gscvsom.value\", \"headshape.value\", \"highrisk.value\", \"hivpcrinfr.value\", \"hivtestreport.value\",\n","\"hivtestresult.value\", \"imaging.value\", \"iron.value\", \"jaundice.value\", \"lengthhaart.value\", \"lp.value\", \"lpglucavail.value\", \"lpprotavail.value\", \"maritalstat.value\", \"mataddrhadistrict.value\", \"mataddrmwdistrict.value\",\n","\"mataddrprovince.value\", \"matadmplace.value\", \"matbldgrp.value\", \"matcomorbidities.value\", \"mathivpr.value\", \"matoutcome.value\", \"matpart.value\", \"matrpr.value\", \"matrprr.value\", \"matrprt.value\", \"matsymptoms.value\",\n","\"medsdis.value\", \"medsgiven.value\", \"methodestgest.value\", \"moretrans.value\", \"motherdobyn.value\", \"mothersdiagnosis.value\", \"mskproblems.value\", \"neorons.value\", \"neotreeoutcome.value\", \"ortolani.value\", \"othprobs.value\",\n","\"palate.value\", \"partnertrsyph.value\", \"passedmec.value\", \"placebirth.value\",\"pregconditions.value\", \"premdia.value\", \"presentation.value\", \"probslab.value\", \"puinfant.value\", \"punewborn.value\",\n","\"reason.value\", \"refdia.value\", \"referredfrom.value\", \"referredfrom2.value\", \"religion.value\", \"respons.value\", \"respsr.value\", \"respsup.value\", \"resus.value\", \"retractions.value\", \"revclintyp.value\", \"reviewcadre.value\",\n","\"rfsepsis.value\", \"romlength.value\", \"rvi.value\", \"sexdis.value\", \"signsdehydrations.value\", \"signsrd.value\", \"skin.value\", \"specrev.value\", \"specrevtyp.value\", \"spine.value\", \"srneuroother.value\", \"stoolsinfant.value\",\n","\"suckreflex.value\", \"tempadm.value\", \"tempbirth.value\", \"teo.value\", \"testthispreg.value\", \"tone.value\", \"torchre.value\", \"transtype.value\", \"ttv.value\", \"typebirth.value\", \"umbilicus.value\", \"vitk.value\", \"vlknown.value\",\n","\"vomiting.value\", \"wob.value\", \"$symptomreviewneurology.value\",\n","\"agecategory\", \"awgroup.value\", \"birthtrauma.value\", \"birthweightcategory\", \"birthweightcategorydischarge\", \"bloodsfinal.value\", \"bloodsinitial.value\", \"bornbeforearrival.value\", \"bwgroup.value\", \"cleftlipand/orpalatewithrd.value\",\n","\"cleftpalate.value\", \"congenitalabnormality.value\", \"congenitaldislocationofthehipcdh.value\", \"considercongenitalheartdisease.value\", \"convulsions.value\", \"dehydration.value\", \"delivery.value\", \"difficultyfeeding.value\",\n","\"edlizsummarytablescore.value\", \"extremelylowbirthweight<1000g.value\", \"extremelypremature<28weeks.value\", \"feedingreview.value\", \"gastroschisis.value\",\"gestgroup.value\", \"hivhighrisk.value\", \"hivlowrisk.value\",\n","\"hivunknown.value\", \"hyperthermia.value\", \"hypoglycaemianotsymptomatic.value\", \"hypoglycaemiasymptomatic.value\", \"hypoxicischaemicencephalopathy.value\", \"lengthoflife.value\", \"lowbirthweight15002499g.value\",\n","\"macrosomia>4000g.value\", \"macrosomiabigbaby.value\", \"meconiumexposureasymptomaticbaby.value\", \"mildhypothermia.value\", \"mildtalipesclubfoot.value\", \"moderatehypothermia.value\", \"moderatetalipesclubfoot.value\", \"myelomeningocele.value\",\n","\"neonatalsepsis.value\", \"normalbaby.value\", \"omphalocele.value\", \"other.value\", \"pathologicaljaundice.value\", \"physiologicaljaundice.value\", \"plandrugamin.value\", \"pneumonia/bronchiolitis.value\", \"possiblemeconiumaspiration.value\",\n","\"premature3236weeks.value\", \"premature32to37weeks.value\", \"prematuritywithrd.value\", \"prematuritywithrds.value\", \"prescom.value\", \"riskfactorsforsepsisasymptomaticbaby.value\", \"riskofhypoglycaemia.value\", \"safekeeping.value\",\n","\"severehypothermia.value\", \"suspectedhypoxicischaemicencephalopathy.value\", \"suspectedhypoxicischaemicencephalopathyhie.value\", \"suspectedneonatalsepsis.value\", \"symptomreviewneurology.value\", \"tempgroup.value\",\n","\"tempthermia.value\", \"termwithrd.value\", \"transienttachypnoeaofnewbornttn.value\", \"verylowbirthweight10001499g.value\", \"verypremature2831+6weeks.value\", \"verypremature2831weeks.value\",\n","\"modedelivery.value\", \"abandonedbaby.value\"\n","# unique features zim\n","# problematic features zim\n","]\n","\n","\n","cat_features_mwi = [\"facility\",\"uid\",\"uniquekey\",\"startedatdischarge\", \"completedatdischarge\", \"ingestedatdischarge\",\n","# common features\n","\"abdomen.value\", \"activity.value\", \"admittedfrom.value\", \"admreaconganom.value\", \"admreason.value\", \"admreasurgcond.value\", \"agecat.value\", \"ageestimate.value\", \"ansteroids.value\", \"antenatalcare.value\", \"anus2.value\",\n","\"anvdrl.value\", \"anvdrlresult.value\", \"bidagecat.value\", \"birthfacility.value\", \"bsunit.value\", \"cadre.value\", \"cadredis.value\", \"causebid.value\", \"causedeath.value\", \"chestausc.value\", \"chlor.value\", \"colour.value\",\n","\"contcausedeath.value\", \"covidconfirmation.value\", \"covidrepresults.value\", \"crt.value\", \"crybirth.value\", \"csreason.value\", \"dangersigns.value\", \"dangersigns2.value\", \"diagdis1.value\", \"discdiagsurgicalcond.value\",\n","\"ethnicity.value\", \"feedsadm.value\", \"fefo.value\", \"femorals.value\", \"feverons.value\", \"fontanelle.value\", \"furthertriage.value\", \"gastons.value\", \"gender.value\", \"genitalia.value\", \"gscvsom.value\", \"headshape.value\",\n","\"hivtestresult.value\", \"hivtestresultdc.value\", \"ipt.value\", \"jaundice.value\", \"lengthhaart.value\", \"lengthresusknown.value\", \"maritalstat.value\", \"matcomorbidities.value\", \"matsymptoms.value\", \"mecpresent.value\",\n","\"mecthickthin.value\", \"medsgiven.value\", \"methodestgest.value\", \"mothersdiagnosis.value\", \"mskproblems.value\", \"murmur.value\", \"neorons.value\", \"neotreeoutcome.value\", \"otherprobs.value\", \"palate.value\", \"paritycod.value\",\n","\"passedmec.value\", \"placebirth.value\", \"pregconditions.value\", \"presentation.value\", \"probslab.value\", \"puinfant.value\", \"punewborn.value\", \"reason.value\", \"referredfrom.value\", \"referredfrom2.value\", \"religion.value\",\n","\"respons.value\", \"respsr.value\", \"respsup.value\", \"resus.value\", \"revclintyp.value\", \"rfsepsis.value\", \"rom.value\", \"romlength.value\", \"signsdehydrations.value\", \"signsrd.value\", \"skin.value\", \"spine.value\", \"srneuroother.value\",\n","\"stdsother.value\", \"stoolsinfant.value\", \"suckreflex.value\", \"testthispreg.value\", \"tetraeye.value\", \"thermcare.value\", \"tone.value\", \"tribe.value\", \"ttv.value\", \"typebid.value\", \"typebirth.value\", \"umbilicus.value\",\n","\"vitk.value\", \"vomiting.value\", \"wob.value\",\n","\"<28wks/1kg.value\", \"abdominalobstruction.value\", \"abscess.value\", \"agecategory\", \"ambiguousgenetalia.value\", \"anaemia.value\", \"apnoeaofprematurity.value\", \"atriskofhypoglycaemia.value\", \"awgroup.value\", \"birthasphyxia.value\",\n","\"birthtrauma.value\", \"birthweightcategory\", \"birthweightcategorydischarge\", \"bowelobstruction.value\",\"bwgroup.value\", \"cleftlipand/orpalate.value\", \"cleftlipand/orpalatewithrd.value\", \"congenitalabnormality.value\",\n","\"congenitaldislocationofthehipcdh.value\", \"congenitalheartdisease.value\", \"convulsions.value\", \"dehydration.value\", \"difficultyfeeding.value\", \"extremelylowbirthweight<1000g.value\", \"extremelypremature<28weeks.value\",\n","\"feedingreview.value\", \"gastroschisis.value\", \"gestationdc.value\", \"gestgroup.value\", \"highbirthweight>4000gatbirth.value\", \"hivhighrisk.value\", \"hivlowrisk.value\", \"hivunknown.value\", \"hyperthermia.value\", \"hypoglycaemianotsymptomatic.value\",\n","\"hypoglycaemiasymptomatic.value\", \"hypoxicischaemicencephalopathy.value\", \"lbwbinary\", \"lowbirthweight15002499g.value\", \"meningitis.value\", \"mildhypothermia.value\", \"mildtalipesclubfoot.value\", \"moderatehypothermia.value\",\n","\"moderatetalipesclubfoot.value\", \"myelomeningocele.value\", \"neonatalsepsis.value\", \"neonatalsepsisearlyonsetasymptomatic.value\", \"neonatalsepsisearlyonsetsymptomatic.value\", \"neonatalsepsislateonsetasymptomatic.value\",\n","\"omphalocele.value\", \"pathologicaljaundice.value\", \"physiologicaljaundice.value\", \"pneumonia/bronchiolitis.value\", \"possiblemeconiumaspiration.value\", \"premature3236weeks.value\", \"prematuritywithrd.value\", \"prolongedjaundice.value\",\n","\"severehypothermia.value\", \"suspectedhypoxicischaemicencephalopathy.value\", \"suspectedneonatalsepsis.value\", \"symptomreviewneurology.value\", \"tempgroup.value\", \"termwithrd.value\", \"transienttachypnoeaofnewbornttn.value\",\n","\"umbilicalhernia.value\", \"untreatedmaternalsyphilis.value\", \"verylowbirthweight10001499g.value\", \"verypremature2831weeks.value\", \"dumpedbaby.value\"\n","# unique features mwi\n","]\n","\n","\n","\n","# OBJECT FEATURES\n","#---------------------------\n","obj_features_zim       = [\"facility\",\"uid\",\"uniquekey\",\"startedatdischarge\", \"completedatdischarge\", \"ingestedatdischarge\",\n","\"admreasonoth.value\", \"axrr.value\", \"babyhospnum.value\",\"bbacord.value\",\"bbadelo.value\", \"bbaot.value\", \"bc2oth.value\", \"bc2rcont.value\", \"bcsecondisol.value\", \"causedeathother.value\", \"cleftlip.value\",\n","\"contriboth.value\", \"csreasonoth.value\", \"csreasonother.value\", \"cussr.value\", \"cxrr.value\", \"diagdis1oth.value\", \"diagnosesoth.value\", \"drid.value\", \"echor.value\", \"grunting.value\", \"hcwid.value\", \"hcwiddis.value\",\n","\"kinaddress.value\", \"kincell.value\", \"kinname.value\", \"lxrr.value\", \"matadmplaceoth.value\", \"mathospnum.value\", \"med1.value\", \"med2.value\", \"med3.value\", \"med4.value\", \"medoth.value\", \"medsdisoth.value\",\n","\"motherscell.value\", \"otherbirthfacility.value\", \"otherreferralfacility.value\", \"othhardistrict.value\", \"othmwdistrict.value\", \"othpregcond.value\", \"othprobsoth.value\", \"othxrr.value\", \"planoth.value\", \"prescomoth.value\",\n","\"prog.value\", \"reasonother.value\", \"religionother.value\", \"revclinoth.value\", \"reviewid.value\", \"revinfo.value\", \"specrevd.value\", \"specrevtypoth.value\", \"syphact.value\", \"travelhint.value\", \"travelhreg.value\",\n","\"trhospital.value\", \"troward.value\", \"trowardother.value\",\n","\"anaemia.value\", \"cleftlipand/orpalate.value\", \"neotreeid.value\", \"nuids.value\", \"othsym.value\", \"plandrugabx.value\", \"plandrugamik.value\", \"plandrugamp.value\", \"plandrugazt.value\",\n","\"plandrugcaff.value\", \"plandrugcef.value\", \"plandrugcefm.value\", \"plandruggent.value\", \"plandrugmet.value\", \"plandrugnvp.value\", \"prolongedjaundice.value\", \"uniquekeydischarge\", \"plan.value\"\n","]\n","\n","\n","obj_features_mwi       = [\"facility\",\"uid\",\"uniquekey\",\"startedatdischarge\", \"completedatdischarge\", \"ingestedatdischarge\",\n","\"admreasonoth.value\", \"admreasonothconganom.value\", \"babyfirstname.value\", \"babysurname.value\", \"causebidoth.value\", \"causedeathoth.value\", \"causedeathother.value\", \"cleftlip.value\", \"contriboth.value\",\n","\"csreasonoth.value\", \"diagdis1oth.value\", \"ethnicityother.value\", \"hcwid.value\", \"hcwiddis.value\", \"hcwsig.value\", \"hcwsigdis.value\", \"matphysaddressdistrict.value\", \"med1.value\", \"med2.value\", \"med3.value\", \"med4.value\",\n","\"medoth.value\", \"modfactor1.value\", \"modfactor2.value\", \"modfactor3.value\", \"motheraddressvillage.value\", \"motherfirstname.value\", \"mothersurname.value\", \"mothlm.value\", \"otherbirthfacility.value\", \"otherprobsoth.value\",\n","\"otherreferralfacility.value\", \"paritycodother.value\", \"planoth.value\", \"reasonother.value\", \"religionother.value\", \"revclinoth.value\", \"stuid.value\", \"tribeother.value\", \"uidbid.value\",\n","\"ageest.value\",\n","\"babyfirstnamedc.value\", \"babysurnamedc.value\", \"cleftlip/cleftpalate.value\",\"diagnoses.value\", \"diagnosesoth.value\", \"diagnosissurgicalcond.value\", \"edlizsummarytablescore.value\", \"externalsource.value\",\n","\"motherfirstnamedc.value\", \"mothersurnamedc.value\", \"neotreeid.value\", \"nuids.value\", \"other.value\", \"uiddc.value\", \"uniquekeydischarge\", \"plan.value\"\n","]\n","\n","\n","\n","# DATETIME FEATURES\n","#---------------------------\n","dt_features_zim        = [\"facility\",\"uid\",\"uniquekey\",\"startedatdischarge\", \"completedatdischarge\", \"ingestedatdischarge\", \"dobtob.value\", \"datetimeadmission.value\",\"datetimedischarge.value\",\n","# commnon features\n","\"anvdrldate.value\", \"axrdate.value\", \"bcdate1.value\", \"bcdate2.value\", \"bilidate1.value\", \"bilidate2.value\", \"bonedate1.value\", \"bonedate2.value\", \"clinrevdat.value\", \"crpdate1.value\", \"crpdate2.value\", \"cussdate.value\",\n","\"cxrdate.value\", \"datedischvitals.value\", \"datedischweight.value\", \"datefbc2.value\", \"datehivtest.value\", \"datelastattendedbaby.value\", \"datelastattendedmother.value\", \"datelp.value\",\n","\"datetimedeath.value\", \"datetimedeathmother.value\", \"dateweaned.value\", \"echodate.value\", \"fbcdate1.value\", \"glucdate1.value\", \"glucdate2.value\", \"hivpcrinfd.value\",\n","\"lxrdate.value\", \"mathivpd.value\", \"othxrdate.value\", \"torchdate.value\", \"uedate1.value\", \"uedate2.value\", \"completedat\", \"endscriptdatetime.value\", \"ingestedat\", \"startedat\",\n","]\n","\n","dt_features_mwi        = [\"facility\",\"uid\",\"uniquekey\",\"startedatdischarge\", \"completedatdischarge\", \"ingestedatdischarge\", \"dobtob.value\", \"datetimeadmission.value\",\"datetimedischarge.value\",\n","# commnon features\n","\"anvdrldate.value\", \"clinrevdat.value\", \"dateadmission.value\", \"datedischvitals.value\", \"datedischweight.value\", \"datehivtest.value\", \"datetimedeath.value\",\n","\"datetimetemponarrival.value\", \"dateweaned.value\", \"endscriptdatetime.value\", \"completedat\", \"dateadmissiondc.value\", \"ingestedat\", \"startedat\", \"symptomsdt.value\",\n","]\n","\n","\n","\n","# WEIGHT FEATURES - Numeric Validation\n","#--------------------------------------\n","weight_cols_zim        = ['admissionweight', 'aw', 'birthweight','bwtdis'] #\n","weight_cols_mwi        = ['admissionweight', 'birthweight'] #\n","\n","\n","# Skip Validation - Numeric Validation\n","#--------------------------------------\n","skip_columns_zim       = ['age','bloodsugarmmol', 'dischrr', 'hr', 'rr', 'length', 'nnuadmtemp', 'wcc1r']\n","skip_columns_mwi       = ['age', 'bloodsugarmg', 'bloodsugarmmol', 'dischhr', 'dischrr', 'bsmg', 'hr', 'rr', 'dischtemp']\n","\n","\n","\n","# VALUES TO DELETE - Categorical Validation\n","# ---------------------------------------------\n","\n","values_to_delete_zim = {\n","\n","(\"birthfacility\"): [\"Name of Birth Facility\"],\n","\n","(\"causedeath\"): [\"2023-10-19 22:00:00\", \"AS\", \"2024-07-16 16:56:00\", \"2024-10-24 11:06:00\",\"2023-10-19 22:00:00\", \"2024-07-16 16:56:00\", \"2024-10-24 11:06:00\"],\n","\n","(\"csreason\"): [\"Reason for C-Section\"],\n","\n","(\"diagdis1\"): [\"J\", \"CA\", \"2024-07-14 08:05:00\", \"2024-07-22 08:03:00\", \"2024-10-25 11:00:00\", \"2024-07-14 08:05:00\", \"2024-07-22 08:03:00\", \"2024-10-25 11:00:00\"],\n","\n","\n","(\"jaundice\"): [\"MJ\", \"DJ\", \"3.0\", \"5.0\", \"5.0\", \"4.0\", \"2.0\", \"3.0\", \"1.0\", \"2.0\", \"Jaundice Kramer staging\"],\n","\n","\n","(\"medsgiven\"): [\"{Medications given during admission,Medications given during admission}\"],\n","\n","(\"referredfrom\"): [\"Name of Referring Facility\"],\n","\n","(\"referredfrom2\"): [\"Referred from?\"],\n","\n","(\"respsup\"): [\"Respiratory Support\", \"{Respiratory Support,Respiratory Support}\"],\n","\n","(\"signsdehydrations\"): [\"32.0\", \"26.0\", \"29.0\", \"35.0\", \"34.0\", \"30.0\", \"37.0\", \"33.0\", \"38.0\", \"40.0\"],\n","\n","(\"admward\"): [\"Was the baby admitted through Embassy or Admissions?\"],\n","\n","(\"anvdrlreport\"): [\"How did you receive the information about syphilis testing?\"],\n","\n","(\"bbadel\"): [\"Who delivered the baby?\"],\n","\n","(\"bbaloc\"): [\"Baby born where?\"],\n","\n","(\"bc1othr\"): [\"If other what?\"],\n","\n","(\"bc1r\"): [\"Initial blood culture result\"],\n","\n","(\"bc2r\"): [\"Result of final blood culture\"],\n","\n","(\"birthplace\"): [\"Place of birth\"],\n","\n","(\"testthispreg\"): [\"A\"],\n","\n","(\"delivinter\"): [\"0.0\"],\n","\n","(\"goodprog\"): [\"What progress did the baby make? \"],\n","\n","(\"highrisk\"): [\"Does this baby need a high risk sticker?\"],\n","\n","(\"hivpcrinfr\"): [\"HIV PCR Result for Infant\"],\n","\n","(\"hivtestreport\"): [\"Where did you get the HIV testing information from?\"],\n","\n","(\"imaging\"): [\"Imaging during admission\"],\n","\n","(\"lp\"): [\"Lumbar puncture performed?\"],\n","\n","(\"mataddrhadistrict\"): [\"If Harare, which District?\"],\n","\n","(\"matdischarge\"): [\"If the mother was admitted, is she now discharged?\"],\n","\n","(\"mathivpr\"): [\"Maternal HIV test result (postnatal)\"],\n","\n","(\"matrprr\"): [\"Maternal RPR Result\"],\n","\n","(\"medsdis\"): [\"Medications prescribed on discharge\"],\n","\n","(\"moretrans\"): [\"Platelets and packed cells\", \"If more than one kind then what kinds?\", \"Packed cells and platelets\", \"If more than one type then what type?\"],\n","\n","(\"motherdobyn\"): [\"Do you know the mother's date of birth?\"],\n","\n","(\"othprobs\"): [\"Other Problems\"],\n","\n","(\"partnertrsyph\"): [\"Was the partner treated?\"],\n","\n","(\"reviewcadre\"): [\"Cadre of Reviewer\"],\n","\n","(\"specrev\"): [\"Was the baby reviewed by a specialty?\"],\n","\n","(\"specrevtyp\"): [\"Which Specialities? (add all)\", \"Which Speciality?\"],\n","\n","(\"transtype\"): [\"What kind of transfusion?\", \"What type of transfusion?\"]\n","\n","}\n","\n","\n","# MWI\n","# Delete Not Allowed Values\n","values_to_delete_mwi = {\n","(\"contcausedeath\"): [\"March 18, 2022, 7:45 AM\", \"March 21, 2022, 6:55 AM\", \"March 21, 2022, 11:30 PM\", \"March 24, 2022, 8:20 AM\", \"March 25, 2022, 2:25 PM\", \"March 29, 2022, 9:35 AM\", \"March 26, 2022, 12:30 PM\",\n","                     \"March 26, 2022, 2:30 PM\", \"April 29, 2022, 9:25 AM\", \"March 28, 2022, 12:30 PM\", \"April 7, 2022, 10:00 AM\", \"March 31, 2022, 11:40 AM\", \"April 19, 2022, 12:40 AM\", \"April 23, 2022, 11:38 PM\",\n","                     \"April 12, 2022, 3:35 PM\", \"April 20, 2022, 10:46 PM\", \"April 26, 2022, 7:52 AM\", \"April 30, 2022, 7:45 AM\", \"April 28, 2022, 9:00 AM\", \"April 28, 2022, 11:50 AM\", \"April 28, 2022, 4:30 PM\",\n","                     \"April 29, 2022, 9:00 PM\", \"May 2, 2022, 12:50 PM\", \"April 30, 2022, 6:00 PM\", \"May 4, 2022, 9:55 AM\", \"May 12, 2022, 3:55 AM\", \"May 7, 2022, 12:15 PM\", \"May 14, 2022, 6:20 AM\",\n","                     \"May 15, 2022, 8:08 AM\", \"May 21, 2022, 5:00 PM\", \"May 25, 2022, 10:20 AM\", \"May 26, 2022, 3:45 AM\", \"May 28, 2022, 6:40 AM\", \"May 24, 2022, 1:02 PM\", \"March 12, 2022, 8:30 PM\",\n","                     \"March 13, 2022, 12:10 AM\", \"March 17, 2022, 11:45 PM\"],\n","\n","\n","(\"hivtestresultdc\"): [\"March 22, 2022, 10:43 AM\", \"March 19, 2022, 11:19 AM\", \"March 23, 2022, 9:37 AM\", \"March 11, 2022, 4:23 AM\", \"April 4, 2022, 9:15 AM\", \"March 12, 2022, 6:06 AM\", \"April 6, 2022, 9:43 AM\",\n","                      \"March 19, 2022, 11:52 AM\", \"March 31, 2022, 9:18 AM\", \"March 19, 2022, 9:38 AM\", \"March 16, 2022, 9:18 AM\", \"March 18, 2022, 8:08 PM\", \"March 17, 2022, 12:58 PM\", \"March 23, 2022, 9:52 AM\",\n","                      \"March 29, 2022, 11:13 AM\", \"March 21, 2022, 6:47 AM\", \"March 21, 2022, 6:06 AM\", \"April 23, 2022, 7:46 AM\", \"March 24, 2022, 7:35 PM\", \"April 27, 2022, 9:54 AM\", \"April 19, 2022, 9:01 AM\",\n","                      \"March 25, 2022, 4:46 AM\", \"March 29, 2022, 10:02 AM\", \"March 25, 2022, 1:58 PM\", \"March 26, 2022, 3:53 PM\", \"March 28, 2022, 11:39 AM\", \"March 28, 2022, 6:29 PM\", \"March 28, 2022, 2:24 PM\",\n","                      \"April 7, 2022, 2:09 PM\", \"April 12, 2022, 6:31 AM\", \"March 31, 2022, 12:28 PM\", \"April 19, 2022, 1:20 AM\", \"May 17, 2022, 9:25 AM\", \"April 28, 2022, 9:59 AM\", \"May 15, 2022, 12:47 PM\",\n","                      \"April 12, 2022, 9:36 AM\", \"May 18, 2022, 11:55 AM\", \"April 18, 2022, 10:10 AM\", \"April 25, 2022, 10:00 AM\", \"April 23, 2022, 11:13 AM\", \"April 9, 2022, 8:11 AM\", \"April 24, 2022, 12:40 PM\",\n","                      \"April 23, 2022, 10:17 AM\", \"April 12, 2022, 6:53 AM\", \"May 5, 2022, 10:20 AM\", \"April 20, 2022, 4:25 AM\", \"April 25, 2022, 7:55 AM\", \"April 28, 2022, 10:44 AM\", \"April 28, 2022, 1:39 PM\",\n","                      \"May 8, 2022, 9:57 AM\", \"April 28, 2022, 12:31 PM\", \"May 4, 2022, 10:56 AM\", \"April 28, 2022, 9:09 PM\", \"April 29, 2022, 3:34 AM\", \"April 29, 2022, 1:05 PM\", \"April 29, 2022, 6:33 PM\",\n","                      \"May 3, 2022, 1:38 PM\", \"May 10, 2022, 4:36 AM\", \"May 22, 2022, 8:17 AM\", \"May 20, 2022, 8:45 AM\", \"May 7, 2022, 2:33 PM\", \"May 13, 2022, 10:38 AM\", \"May 14, 2022, 8:57 AM\",\n","                      \"May 15, 2022, 10:27 AM\", \"May 15, 2022, 10:34 AM\", \"June 16, 2022, 8:48 AM\", \"May 14, 2022, 7:16 AM\", \"May 22, 2022, 9:21 AM\", \"May 15, 2022, 7:40 AM\", \"May 16, 2022, 8:46 AM\",\n","                      \"June 7, 2022, 10:07 AM\", \"May 27, 2022, 12:05 PM\", \"May 21, 2022, 1:43 PM\", \"June 20, 2022, 12:23 PM\", \"May 24, 2022, 8:26 AM\", \"May 24, 2022, 12:36 PM\", \"May 26, 2022, 4:33 AM\",\n","                      \"May 26, 2022, 9:05 AM\", \"June 7, 2022, 10:15 AM\", \"May 24, 2022, 2:41 PM\"],\n","\n","\n","(\"hivtestresultdc\"): [\"March 22, 2022, 10:43 AM\", \"March 19, 2022, 11:19 AM\", \"March 23, 2022, 9:37 AM\", \"March 11, 2022, 4:23 AM\", \"April 4, 2022, 9:15 AM\", \"March 12, 2022, 6:06 AM\",\n","                      \"April 6, 2022, 9:43 AM\", \"March 19, 2022, 11:52 AM\", \"March 31, 2022, 9:18 AM\", \"March 19, 2022, 9:38 AM\", \"March 16, 2022, 9:18 AM\", \"March 18, 2022, 8:08 PM\",\n","                      \"March 17, 2022, 12:58 PM\", \"March 23, 2022, 9:52 AM\", \"March 29, 2022, 11:13 AM\", \"March 21, 2022, 6:47 AM\", \"March 21, 2022, 6:06 AM\", \"April 23, 2022, 7:46 AM\",\n","                      \"March 24, 2022, 7:35 PM\", \"April 27, 2022, 9:54 AM\", \"April 19, 2022, 9:01 AM\", \"March 25, 2022, 4:46 AM\", \"March 29, 2022, 10:02 AM\", \"March 25, 2022, 1:58 PM\", \"March 26, 2022, 3:53 PM\",\n","                      \"March 28, 2022, 11:39 AM\", \"March 28, 2022, 6:29 PM\", \"March 28, 2022, 2:24 PM\", \"April 7, 2022, 2:09 PM\", \"April 12, 2022, 6:31 AM\", \"March 31, 2022, 12:28 PM\", \"April 19, 2022, 1:20 AM\",\n","                      \"May 17, 2022, 9:25 AM\", \"April 28, 2022, 9:59 AM\", \"May 15, 2022, 12:47 PM\", \"April 12, 2022, 9:36 AM\", \"May 18, 2022, 11:55 AM\", \"April 18, 2022, 10:10 AM\", \"April 25, 2022, 10:00 AM\",\n","                      \"April 23, 2022, 11:13 AM\", \"April 9, 2022, 8:11 AM\", \"April 24, 2022, 12:40 PM\", \"April 23, 2022, 10:17 AM\", \"April 12, 2022, 6:53 AM\", \"May 5, 2022, 10:20 AM\", \"April 20, 2022, 4:25 AM\",\n","                      \"April 25, 2022, 7:55 AM\", \"April 28, 2022, 10:44 AM\", \"April 28, 2022, 1:39 PM\", \"May 8, 2022, 9:57 AM\", \"April 28, 2022, 12:31 PM\", \"May 4, 2022, 10:56 AM\", \"April 28, 2022, 9:09 PM\",\n","                      \"April 29, 2022, 3:34 AM\", \"April 29, 2022, 1:05 PM\", \"April 29, 2022, 6:33 PM\", \"May 3, 2022, 1:38 PM\", \"May 10, 2022, 4:36 AM\", \"May 22, 2022, 8:17 AM\", \"May 20, 2022, 8:45 AM\",\n","                      \"May 7, 2022, 2:33 PM\", \"May 13, 2022, 10:38 AM\", \"May 14, 2022, 8:57 AM\", \"May 15, 2022, 10:27 AM\", \"May 15, 2022, 10:34 AM\", \"June 16, 2022, 8:48 AM\", \"May 14, 2022, 7:16 AM\",\n","                      \"May 22, 2022, 9:21 AM\", \"May 15, 2022, 7:40 AM\", \"May 16, 2022, 8:46 AM\", \"June 7, 2022, 10:07 AM\", \"May 27, 2022, 12:05 PM\", \"May 21, 2022, 1:43 PM\", \"June 20, 2022, 12:23 PM\",\n","                      \"May 24, 2022, 8:26 AM\", \"May 24, 2022, 12:36 PM\", \"May 26, 2022, 4:33 AM\", \"May 26, 2022, 9:05 AM\", \"June 7, 2022, 10:15 AM\", \"May 24, 2022, 2:41 PM\"],\n","\n","(\"respsup\"): [\"40.0\", \"48.0\", \"46.0\", \"51.0\", \"53.0\", \"50.0\", \"52.0\", \"65.0\", \"38.0\", \"42.0\", \"20.0\", \"24.0\", \"98.0\", \"44.0\", \"30.0\", \"28.0\", \"57.0\", \"49.0\", \"45.0\", \"32.0\", \"56.0\", \"47.0\", \"60.0\", \"36.0\", \"43.0\",\n","              \"58.0\", \"54.0\", \"4.0\", \"64.0\", \"12.0\", \"69.0\", \"39.0\", \"41.0\", \"21.0\"],\n","\n","\n","(\"otherprobs\"): [\"2022-03-29T10:45:16.139Z\", \"2022-03-24T11:21:05.456Z\", \"2022-03-31T09:39:49.521Z\", \"2022-04-13T09:45:49.513Z\", \"2022-03-24T11:54:47.029Z\", \"2022-04-01T09:26:04.134Z\", \"2022-03-25T09:45:19.971Z\",\n","                 \"2022-03-31T09:57:34.948Z\", \"2022-04-07T11:16:31.754Z\", \"2022-04-25T07:49:23.173Z\", \"2022-05-05T09:57:16.553Z\", \"2022-04-28T09:03:35.718Z\", \"2022-04-07T11:42:10.626Z\", \"2022-05-26T09:31:00.919Z\",\n","                 \"2022-05-05T10:03:36.992Z\", \"2022-05-16T12:51:33.519Z\", \"2022-04-14T09:42:07.117Z\", \"2022-05-26T12:04:15.038Z\", \"2022-04-25T10:15:04.993Z\", \"2022-05-05T10:04:39.048Z\", \"2022-04-28T11:17:18.006Z\",\n","                 \"2022-04-25T10:19:58.107Z\", \"2022-04-23T11:17:53.618Z\", \"2022-05-12T10:23:23.650Z\", \"2022-05-12T10:00:09.455Z\", \"2022-05-05T11:02:14.071Z\", \"2022-05-26T08:21:08.877Z\", \"2022-05-21T08:47:33.462Z\",\n","                 \"2022-05-16T10:41:40.396Z\", \"2022-05-20T09:01:01.379Z\", \"2022-05-19T10:30:50.709Z\", \"2022-05-19T10:36:23.850Z\", \"2022-06-23T08:55:49.181Z\", \"2022-06-15T09:22:55.734Z\", \"2022-05-17T08:48:06.510Z\",\n","                 \"2022-06-16T10:10:51.821Z\", \"2022-06-02T12:07:27.955Z\", \"2022-06-23T12:27:52.105Z\", \"2022-05-26T10:10:49.493Z\", \"2022-06-08T10:18:20.742Z\"],\n","\n","\n","(\"hcwsigdis\"): [\"29.0\", \"33.0\", \"32.0\", \"34.0\", \"35.0\", \"28.0\", \"27.0\", \"36.0\", \"25.0\", \"30.0\", \"37.0\", \"31.0\"],\n","\n","(\"thermcare\"): [\"AM\", \"{AM,VitK,TEO}\", \"{BP,GENT,AM}\", \"{CEF,AM,OTH}\", \"{BP,GENT,AM,VitK}\", \"{BP,GENT}\", \"{GENT,BP,AM}\", \"CEF\", \"NONE\", \"{BP,GENT,AM,OTH}\", \"{BP,GENT,VitK,TEO}\", \"{NVP,AM}\", \"{BP,GENT,CEF,AM}\",\n","                \"{CHLX,VitK,TEO}\", \"{BP,GENT,CEF}\", \"{AM,NVP,CEF}\", \"{BP,GENT,AM,NVP}\", \"{CEF,AM,BP,GENT}\", \"{BP,GENT,AM,CEF,OTH}\", \"{BP,GENT,MET}\", \"{CEF,AM,MET}\", \"{BP,GENT,AM,VitK,TEO}\", \"{CEF,BP,GENT,AM}\",\n","                \"{CEF,AM,FLU,Mero}\", \"{BP,AM,GENT,VitK,TEO}\", \"{BP,GENT,AM,CEF}\", \"{BP,GENT,PCM}\", \"{VitK,TEO}\", \"{GENT,BP,VitK,TEO}\", \"{BP,GENT,CEF,VitK,TEO}\", \"{BP,GENT,NVP}\", \"{GENT,AM,BP,VitK,TEO}\",\n","                \"{BP,GENT,CHLX,AM,VitK,TEO}\"],\n","\n","(\"neotreeoutcome\"): [\"FALSE\"],\n","\n","(\"medsgiven\"): [\"EMMP\", \"ELTE\", \"MPGR\", \"PRDI\", \"PECH\", \"DAMU\", \"PRMA\", \"JEJE\", \"BRNA\", \"VEKU\", \"NAKH\", \"NAME\", \"WAKU\", \"BLPH\", \"FEZA\"]\n","\n","}\n","\n","\n","\n","# VALUE ALIGNMENT - Categorical Validation\n","# ---------------------------------------------\n","\n","# ZIM\n","value_mappings_zim = {\n","    \"admreason\": {\n","        \"GSch\": [\"GSchis\"],\n","        \"Conv\": [\"CONV\"],\n","        \"Fev\": [\"FEV\"]\n","    },\n","    \"ageestimate\": {\n","        \"INF\": [\"inf\"]\n","    },\n","    \"feedsadm\": {\n","        \"NFY\": [\"Has not had a breast feed yet\"]\n","    },\n","    \"fontanelle\": {\n","        \"Flat\": [\"Full\"]\n","    },\n","    \"furthertriage\": {\n","        \"Stable\": [\"Stable \"]\n","    },\n","    \"puinfant\": {\n","        \"Unk\": [\"Not sure\"]\n","    },\n","    \"stoolsinfant\": {\n","        \"Unk\": [\"Not sure\"]\n","    },\n","    \"birthplace\": {\n","        \"SMCH\": [\"HCH\"]\n","    },\n","    \"matadmplace\": {\n","        \"ICU\": [\" ICU\"]\n","    },\n","    \"delivinter\": {\n","        \"NONE\": [\"None\"]\n","    },\n","    \"none\": {\n","        \"True\": [\"TRUE\"]\n","    },\n","     \"none2\": {\n","        \"True\": [\"TRUE\"]\n","    },\n","    \"passedmec\": {\n","        \"Not sure\": [\"Unk\"]\n","    }\n","}\n","\n","# Malawi Alignment:\n","# Malawi Alignment:\n","\n","value_mappings_mwi = {\n","    \"activity\": {\n","        \"Convulsions\": [\"Conv\"]\n","    },\n","    \"admreason\": {\n","        \"FEV\": [\"Fev\"]\n","    },\n","    \"ageestimate\": {\n","        \"INF\": [\"inf\"]\n","    },\n","    \"dangersigns\": {\n","        \"Convulsions\": [\"Conv\"]\n","    },\n","    \"fontanelle\": {\n","        \"Full\": [\"Flat\"]\n","    },\n","    \"furthertriage\": {\n","        \"Stable\": [\"Stable \"]\n","    },\n","    \"passedmec\": {\n","        \"Unk\": [\"Not sure\"]\n","    },\n","    \"srneuroother\": {\n","        \"Fl\": [\"FL\"],\n","        \"Convulsions\": [\"Conv\"]\n","    },\n","    \"ttv\": {\n","        \"TTV3\": [\"TTV3 \"],\n","        \"TTV4+\": [\"TTV4\", \"TTV5\", \"TTV5orM\"]\n","    },\n","    \"fefo\": {\n","        \"FeFo4+\": [\"FeFo4orM\"]\n","    },\n","     \"colour\": {\n","        \"Yellow\": [\"Yell\"]\n","    },\n","     \"tribe\": {\n","        \"N\": [\" N\"]\n","    },\n","    \"ipt\": {\n","        \"IPT5+\": [\"IPT5orM\"]\n","    }\n","}\n"],"id":"910d7292"},{"cell_type":"code","execution_count":11,"metadata":{"id":"0776904b","outputId":"ee355bf5-94f5-4ca5-a004-eb44e935ec79","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1750152466846,"user_tz":-120,"elapsed":49,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["‚Üí Running for ZIM\n","   ‚Ä¢ num           : list, len=126\n","   ‚Ä¢ bool          : list, len=84\n","   ‚Ä¢ cat           : list, len=227\n","   ‚Ä¢ obj           : list, len=83\n","   ‚Ä¢ dt            : list, len=47\n","   ‚Ä¢ weight_cols   : list, len=4\n","   ‚Ä¢ skip_columns  : list, len=8\n","   ‚Ä¢ values_to_delete: dict, len=39\n","   ‚Ä¢ value_mappings: dict, len=13\n","   ‚Ä¢ dict_filepath : str, len=151\n","   ‚Ä¢ sheet_name: numeric_zim\n"]}],"source":["# ‚îÄ‚îÄ‚îÄ Build cfg via globals() ‚îÄ‚îÄ‚îÄ\n","\n","# 1. Normalize to a suffix\n","suffix = \"mwi\" if country.lower() in (\"malawi\", \"mwi\") else \"zim\"\n","\n","# 2. Define how your short keys map to the base variable names\n","keys_map = {\n","    \"num\":             \"numeric_features\",\n","    \"bool\":            \"bool_features\",\n","    \"cat\":             \"cat_features\",\n","    \"obj\":             \"obj_features\",\n","    \"dt\":              \"dt_features\",\n","    \"weight_cols\":     \"weight_cols\",\n","    \"skip_columns\":    \"skip_columns\",\n","    \"values_to_delete\":\"values_to_delete\",\n","    \"value_mappings\":  \"value_mappings\",\n","    \"dict_filepath\":   \"dict_filepath\",\n","}\n","\n","# 3. Grab each country‚Äêspecific global into cfg\n","cfg = {\n","    key: globals()[f\"{base}_{suffix}\"]\n","    for key, base in keys_map.items()\n","}\n","\n","# 4. Sheet name - Numeric Validation\n","sheet_name = f\"numeric_{suffix}\"\n","\n","\n","# 6. Sanity check (optional)\n","print(f\"‚Üí Running for {suffix.upper()}\")\n","for k, v in cfg.items():\n","    print(f\"   ‚Ä¢ {k:<14}: {type(v).__name__}, len={len(v)}\")\n","print(\"   ‚Ä¢ sheet_name:\", sheet_name)\n"],"id":"0776904b"},{"cell_type":"markdown","metadata":{"id":"5900640b"},"source":["### Functions"],"id":"5900640b"},{"cell_type":"markdown","metadata":{"id":"139aa893"},"source":["#### Universal Functions"],"id":"139aa893"},{"cell_type":"code","execution_count":12,"metadata":{"id":"7dfa0301","executionInfo":{"status":"ok","timestamp":1750152466898,"user_tz":-120,"elapsed":4,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# FUNCTION - Get DataFrame Status Report\n","# Summarizes column dtypes, duplicate rows/columns, and fully-missing columns. Optional report path.\n","# ---------------------------------------------------------------------------------------------------\n","def get_report(\n","    df: pd.DataFrame,\n","    txt_file_path: Optional[str] = None\n",") -> None:\n","    \"\"\"\n","    Steps:\n","    1. Compute summary: dtypes, duplicate rows, duplicate columns, fully-missing columns.\n","    2. If `txt_file_path` is provided, attempt to write the report; skip on error.\n","    3. Otherwise, do nothing (report is optional).\n","    \"\"\"\n","    # Step 1: Build report lines\n","    lines: list[str] = []\n","    lines.append('Summary of Column Data Types')\n","    lines.append('=============================')\n","    dtype_counts = df.dtypes.apply(lambda x: str(x)).value_counts()  # Count columns by dtype\n","    for dtype, count in dtype_counts.items():\n","        lines.append(f'{dtype}: {count} columns')  # Add each dtype count\n","\n","    # Duplicate rows\n","    duplicate_count = df.duplicated().sum()\n","    lines.append(f\"\\nDuplicate Rows: {duplicate_count}\")\n","    lines.append('===================')\n","\n","    # Duplicate columns\n","    duplicate_cols = df.columns[df.columns.duplicated()]\n","    num_duplicate_cols = len(duplicate_cols)\n","    lines.append(f\"\\nDuplicate Columns: {num_duplicate_cols}\")\n","    lines.append('===================\\n')\n","    if num_duplicate_cols > 0:\n","        lines.append(\"Columns with duplicates:\")\n","        for col in duplicate_cols:\n","            lines.append(f\" - {col}\")  # List each duplicated column\n","\n","    # Columns with 100% missing values\n","    missing_100 = df.isna().all()\n","    missing_100_count = missing_100.sum()\n","    lines.append(f\"\\nColumns with 100% Missing Values: {missing_100_count}\")\n","    if missing_100_count > 0:\n","        lines.append(\"Columns with no values:\")\n","        for col in df.columns[missing_100]:\n","            lines.append(f\" - {col}\")  # List fully-missing columns\n","\n","    lines.append('===================')\n","\n","    # Step 2: Optional writing of the report\n","    if txt_file_path:\n","        try:\n","            with open(txt_file_path, 'w', encoding='utf-8') as f:\n","                f.write(\"\\n\".join(lines))  # Write report content\n","        except Exception:\n","            pass  # Skip report if path invalid or write fails\n"],"id":"7dfa0301"},{"cell_type":"code","execution_count":13,"metadata":{"id":"185394ea","executionInfo":{"status":"ok","timestamp":1750152466921,"user_tz":-120,"elapsed":19,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# FUNCTION - Inspect Random Column\n","# Samples and prints key metrics for a randomly selected DataFrame column.\n","# ------------------------------------------------------------------------------\n","def inspect_random_column(df: pd.DataFrame) -> None:\n","    \"\"\"\n","    Randomly selects one column from the DataFrame and prints:\n","      1. Column name and data type.\n","      2. Total number of rows and percentage missing.\n","      3. Number and list of unique values.\n","      4. A sample of up to 10 non-missing values.\n","    \"\"\"\n","    # 1. Select a random column\n","    selected_column_name = random.choice(df.columns)  # Choose one column name at random\n","    col_series = df[selected_column_name]             # Extract the column as a Series\n","\n","    # 2. Compute basic metrics\n","    total_entries = len(col_series)                    # Total entries in the column\n","    total_missing = col_series.isna().sum()            # Count of missing (NaN) entries\n","\n","    # 3. Print summary information\n","    print(f'Column: {selected_column_name}')          # Display the chosen column name\n","    print(f'Column dtype: {col_series.dtype}')        # Display the data type of the column\n","    print(f'\\nTotal rows: {total_entries}')          # Show total number of rows\n","    print(f'% Missing: {total_missing} ({(total_missing / total_entries) * 100:.3f}% missing)')  # Show missing percentage\n","    print('Number of Unique Values:', col_series.nunique())  # Display count of unique values\n","    print('\\nAll Unique Values in Column:\\n', col_series.unique())  # List all unique values\n","\n","    # 4. Display a sample of non-missing values\n","    non_missing_series = col_series.dropna()           # Filter out missing values\n","    if not non_missing_series.empty:\n","        sample_size = min(10, len(non_missing_series)) # Limit sample size to up to 10\n","        print('\\nSample Values:')                     # Header for sample values\n","        print(\n","            non_missing_series\n","            .sample(n=sample_size, random_state=42)\n","            .to_string(index=False)\n","        )  # Print sampled values without indices\n","    else:\n","        print('\\n(No non-missing values to sample)')     # Message if no values to sample\n"],"id":"185394ea"},{"cell_type":"code","execution_count":14,"metadata":{"id":"320d06ff","executionInfo":{"status":"ok","timestamp":1750152466968,"user_tz":-120,"elapsed":23,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# FUNCTION - Remove Suffixes\n","# Simplifies column names by stripping any suffix after the first period ('.').\n","# ------------------------------------------------------------------------------\n","def remove_suffixes(\n","    df: pd.DataFrame\n",") -> pd.DataFrame:\n","    \"\"\"\n","    1. Rename each column by keeping only the text before the first period.\n","    2. Return the DataFrame with simplified column names.\n","\n","    Parameters:\n","        df (pd.DataFrame): Input DataFrame with potentially suffixed column names.\n","\n","    Returns:\n","        pd.DataFrame: DataFrame with column names simplified.\n","    \"\"\"\n","    # Step 1: Strip suffix from columns\n","    df = df.rename(columns=lambda x: x.split('.')[0])  # Split on first period and use the prefix as new column name\n","\n","    # Step 2: Return updated DataFrame\n","    return df  # DataFrame now has no suffixes on column names"],"id":"320d06ff"},{"cell_type":"code","execution_count":15,"metadata":{"id":"e233cb54","executionInfo":{"status":"ok","timestamp":1750152467003,"user_tz":-120,"elapsed":31,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# FUNCTION - Log in Notebook Runtime\n","start_time = time.time()\n","def format_time(seconds):\n","    minutes = int(seconds // 60)\n","    remaining_seconds = seconds % 60\n","    return f\"{minutes} minutes and {remaining_seconds:.2f} seconds\""],"id":"e233cb54"},{"cell_type":"markdown","metadata":{"id":"f5404d8c"},"source":["#### 1st Stage Cleaning Functions"],"id":"f5404d8c"},{"cell_type":"code","execution_count":16,"metadata":{"id":"e7a46bd6","executionInfo":{"status":"ok","timestamp":1750152467046,"user_tz":-120,"elapsed":39,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# FUNCTION - Clean Column Headers\n","# Remove all whitespace and normalize dot placement, then lowercase every column name.\n","# ------------------------------------------------------------------------------\n","def clean_columns(\n","    df: pd.DataFrame\n",") -> pd.DataFrame:\n","    \"\"\"\n","    Steps:\n","    1. Define a helper `clean_name` to process individual column names.\n","    2. Apply `clean_name` to each column in the DataFrame.\n","    3. Return the DataFrame with cleaned headers.\n","    \"\"\"\n","    # Step 1: Helper to clean a single column name\n","    def clean_name(name):\n","        name = name.strip()                      # Remove leading/trailing whitespace\n","        name = re.sub(r\"\\s*\\.\\s*\", \".\", name)  # Normalize spaces around dots\n","        name = re.sub(r\"\\s+\", \" \", name)        # Collapse multiple spaces to single\n","        name = name.replace(' ', '')               # Remove all remaining spaces\n","        name = name.lower()                        # Convert to lowercase\n","        return name\n","\n","    # Step 2: Apply cleaning function to each column name\n","    df.columns = [clean_name(col) for col in df.columns]  # Update DataFrame headers\n","\n","    # Step 3: Return the modified DataFrame\n","    return df  # DataFrame now has standardized, lowercase column names\n"],"id":"e7a46bd6"},{"cell_type":"code","execution_count":17,"metadata":{"id":"ad43ca60","executionInfo":{"status":"ok","timestamp":1750152467079,"user_tz":-120,"elapsed":28,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# FUNCTION - Remove Frame Shift (with optional reporting)\n","# Cleans rows where 'UID' lacks a dash, saves dirty data, and optionally writes a report.\n","# ---------------------------------------------------------------------------------------\n","def remove_frame_shift(\n","    df: pd.DataFrame,\n","    report_filepath: str = None,\n","    pkl_filepath: str = None,\n","    csv_filepath: str = None\n",") -> pd.DataFrame:\n","    \"\"\"\n","    Steps:\n","    1. Identify rows where 'UID' does not contain '-'.\n","    2. Split into dirty_df (no dash) and clean_df (with dash).\n","    3. Optionally save dirty_df to pickle and CSV if file paths are valid.\n","    4. Optionally write a report of affected rows to report_filepath.\n","    5. Return clean_df for further use.\n","    \"\"\"\n","    # Step 1: Detect frame shift issues (UID missing dash)\n","    has_dash = df['UID'].astype(str).str.contains('-')  # Boolean mask\n","\n","    # Step 2: Create dirty and clean subsets\n","    dirty_df = df[~has_dash].copy()  # Rows missing dash\n","    clean_df = df[has_dash].copy()   # Rows with dash\n","\n","    # Step 3: Optionally save dirty data\n","    if pkl_filepath:\n","        try:\n","            dirty_df.to_pickle(pkl_filepath)\n","        except Exception:\n","            pass  # Skip if path invalid or write fails\n","    if csv_filepath:\n","        try:\n","            dirty_df.to_csv(csv_filepath, index=False)\n","        except Exception:\n","            pass  # Skip if path invalid or write fails\n","\n","    # Step 4: Optionally generate report\n","    if report_filepath:\n","        try:\n","            total_affected = len(dirty_df)\n","            affected_uids = sorted(dirty_df['UID'].astype(str).unique())\n","            report_lines = [\n","                f\"Total number of affected rows: {total_affected}\",\n","                \"Affected UIDs:\"\n","            ]\n","            report_lines.extend(affected_uids)\n","            with open(report_filepath, 'w') as report_file:\n","                report_file.write(\"\\n\".join(report_lines))  # Overwrite existing report\n","        except Exception:\n","            pass  # Skip reporting on error\n","\n","    # Step 5: Return the cleaned DataFrame\n","    return clean_df  # DataFrame without frame-shifted rows\n"],"id":"ad43ca60"},{"cell_type":"code","execution_count":18,"metadata":{"id":"c19a1f81","executionInfo":{"status":"ok","timestamp":1750152467112,"user_tz":-120,"elapsed":30,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# FUNCTION - Merge Duplicate Columns\n","# Consolidates columns with identical names by keeping the one with the most non-null values\n","# and filling its missing entries from the less complete duplicates.\n","# ------------------------------------------------------------------------------\n","def merge_duplicate_columns(df: pd.DataFrame) -> pd.DataFrame:\n","    \"\"\"\n","    Steps:\n","    1. Iterate through each unique column name in the DataFrame.\n","    2. For names appearing once, keep the series unchanged.\n","    3. For duplicates:\n","       a. Determine which series has the most non-null values.\n","       b. Use that as the primary, filling its NaNs with values from the others.\n","       c. Log the merge action (optional reporting not applicable here).\n","    4. Reconstruct and return a new DataFrame with one column per unique name.\n","    \"\"\"\n","    merged = {}  # Container for the merged Series objects\n","\n","    # Step 1: Loop over each unique column name\n","    for col in df.columns.unique():\n","        # Gather all Series objects with this column name\n","        dup_series = [df.iloc[:, i] for i, c in enumerate(df.columns) if c == col]\n","\n","        # Step 2: If only one series exists, keep it as-is\n","        if len(dup_series) == 1:\n","            merged[col] = dup_series[0]\n","\n","        # Step 3: Merge multiple series\n","        else:\n","            # 3a. Count non-null entries for each duplicate\n","            counts = [s.notna().sum() for s in dup_series]\n","            idx_fuller = counts.index(max(counts))         # Index of the most complete series\n","            fuller = dup_series[idx_fuller].copy()         # Primary series to keep\n","\n","            # 3b. Fill NaNs in primary with values from other duplicates\n","            for i, s in enumerate(dup_series):\n","                if i != idx_fuller:\n","                    fuller = fuller.combine_first(s)         # Fill missing spots\n","\n","            merged[col] = fuller                           # Store merged result\n","            # Log action (no external report file generated)\n","            logging.info(\n","                f\"Merged duplicates for column '{col}'; kept column with {max(counts)} non-null values.\"\n","            )  # This log is optional and won‚Äôt raise errors if logging isn‚Äôt configured\n","\n","    # Step 4: Reconstruct DataFrame from merged series and preserve original index\n","    new_df = pd.DataFrame(merged)\n","    new_df.index = df.index\n","    return new_df  # Return the cleaned, de-duplicated DataFrame\n"],"id":"c19a1f81"},{"cell_type":"code","execution_count":19,"metadata":{"id":"a97300c9","executionInfo":{"status":"ok","timestamp":1750152467228,"user_tz":-120,"elapsed":67,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# FUNCTION - Load and Aggregate the Data Dictionary\n","# Reads an Excel file (multiple sheets) and builds a mapping:\n","# key (lowercase) -> {values, labels, value_labels, datatype}\n","# ------------------------------------------------------------------------------\n","def load_and_aggregate_dictionary(\n","    dict_filepath: str\n",") -> Dict[str, Any]:\n","    \"\"\"\n","    Steps:\n","    1. Load the Excel workbook from path.\n","    2. For each sheet:\n","       a. Parse into DataFrame if possible.\n","       b. Verify required columns exist; otherwise skip.\n","       c. Standardize column names and normalize text fields.\n","       d. Iterate rows to populate `aggregated_dict`:\n","          - Initialize new key entries or verify datatype consistency.\n","          - Collect unique values and labels.\n","          - Map each value_label to its raw value.\n","    3. Return the aggregated dictionary mapping.\n","    \"\"\"\n","    # Step 1: Load workbook\n","    try:\n","        xlsx = pd.ExcelFile(dict_filepath)  # Open the Excel file\n","    except Exception as e:\n","        logging.error(f\"Failed to load dictionary file: {e}\")  # Log on error\n","        raise  # Propagate exception\n","\n","    aggregated_dict: Dict[str, Any] = {}  # Container for final mappings\n","\n","    # Step 2: Process each sheet in the workbook\n","    for sheet in xlsx.sheet_names:\n","        try:\n","            df_sheet = xlsx.parse(sheet)  # Parse sheet into DataFrame\n","        except Exception as e:\n","            logging.error(f\"Failed to parse sheet '{sheet}': {e}\")  # Log parse failure\n","            continue  # Skip to next sheet\n","\n","        # 2a. Verify required columns are present\n","        expected_cols = ['Key', 'Label', 'Data Type', 'Value', 'Value Label']\n","        df_sheet.columns = [col.strip() for col in df_sheet.columns]  # Trim headers\n","        if not all(col in df_sheet.columns for col in expected_cols):\n","            logging.warning(f\"Sheet '{sheet}' missing expected columns. Skipping.\")  # Warn and skip\n","            continue\n","\n","        # 2b. Rename and normalize column names\n","        df_sheet = df_sheet.rename(columns={\n","            'Key': 'key',\n","            'Label': 'label',\n","            'Data Type': 'datatype',\n","            'Value': 'value',\n","            'Value Label': 'value_label'\n","        })  # Standardize names\n","        df_sheet['key'] = df_sheet['key'].astype(str).str.strip().str.lower()       # Clean key\n","        df_sheet['value'] = df_sheet['value'].astype(str).str.strip()                # Clean raw value\n","        df_sheet['label'] = df_sheet['label'].astype(str).str.strip()                # Clean label\n","        df_sheet['value_label'] = df_sheet['value_label'].astype(str).str.strip()    # Clean value_label\n","        df_sheet['datatype'] = df_sheet['datatype'].astype(str).str.strip().str.lower()  # Clean datatype\n","\n","        # 2c. Populate aggregated_dict row by row\n","        for _, row in df_sheet.iterrows():\n","            key = row['key']  # Normalized key\n","            if pd.isna(key) or key == '':\n","                continue  # Skip empty keys\n","\n","            # Initialize or verify datatype consistency\n","            if key not in aggregated_dict:\n","                aggregated_dict[key] = {\n","                    'values': set(),\n","                    'labels': set(),\n","                    'value_labels': {},\n","                    'datatype': row['datatype']\n","                }\n","            else:\n","                if aggregated_dict[key]['datatype'] != row['datatype']:\n","                    logging.warning(\n","                        f\"Datatype inconsistency for key '{key}' in sheet '{sheet}'. Using first encountered datatype.\"\n","                    )  # Warn but keep original datatype\n","\n","            # Add raw value if valid\n","            if row['value'] not in [None, '', 'nan']:\n","                aggregated_dict[key]['values'].add(row['value'])\n","            # Add label if valid\n","            if row['label'] not in [None, '', 'nan']:\n","                aggregated_dict[key]['labels'].add(row['label'])\n","            # Map value_label to raw value if valid\n","            if row['value_label'] not in [None, '', 'nan']:\n","                aggregated_dict[key]['value_labels'][row['value_label']] = row['value']\n","\n","    logging.info(\"Completed aggregating dictionary from Excel.\")  # Log completion\n","    return aggregated_dict  # Return the final mapping\n"],"id":"a97300c9"},{"cell_type":"code","execution_count":20,"metadata":{"id":"553f507a","executionInfo":{"status":"ok","timestamp":1750152467270,"user_tz":-120,"elapsed":17,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# FUNCTION - Value Replacements\n","# Cleans columns using a base-name dictionary mapping and optionally writes a detailed report.\n","# ------------------------------------------------------------------------------\n","def clean_columns_using_base_name(\n","    df: pd.DataFrame,\n","    dict_mapping: Dict[str, Any],\n","    column_cleaning_report_filepath: Optional[str] = None,\n","    max_warnings: int = 10\n",") -> pd.DataFrame:\n","    \"\"\"\n","    For each column, remove known suffixes and:\n","      - Replace values via `value_labels` mapping.\n","      - Delete values in `labels` set (set to NaN).\n","    Optionally writes a report if `column_cleaning_report_filepath` is provided.\n","    \"\"\"\n","    # 1. Helper: derive base name by stripping suffixes\n","    def get_base_name(col: str) -> str:\n","        if col.endswith('.value'):\n","            return col[:-6]            # Remove '.value' suffix\n","        if col.endswith('.valuedischarge'):\n","            return col[:-14]           # Remove '.valuedischarge' suffix\n","        return col                     # No suffix to remove\n","\n","    # 2. Initialize reports & counters\n","    columns_with_no_dict: list[str] = []  # Columns missing mapping\n","    replaced_values_info: DefaultDict[str, list[tuple[Any, Any]]] = defaultdict(list)\n","    not_found_values_info: DefaultDict[str, list[Any]] = defaultdict(list)\n","    deleted_labels_count: DefaultDict[str, int] = defaultdict(int)\n","    columns_with_extra_missing: list[str] = []\n","\n","    # 3. Identify columns whose base name exists in the mapping\n","    columns_to_clean = [col for col in df.columns if get_base_name(col) in dict_mapping]\n","\n","    # 4. Process each column for replacements and deletions\n","    for col in columns_to_clean:\n","        base = get_base_name(col)                         # Get lookup key\n","        mapping = dict_mapping.get(base)                  # Fetch mapping\n","        if not mapping:\n","            columns_with_no_dict.append(col)              # Record missing mapping\n","            continue                                      # Skip cleaning\n","\n","        allowed = mapping.get('values', set())            # Allowed raw values\n","        val_labels = mapping.get('value_labels', {})      # Description->raw mapping\n","        labels_set = mapping.get('labels', set())         # Labels to delete\n","\n","        unique_vals = df[col].dropna().unique()           # Unique non-null entries\n","        warning_count = 0\n","\n","        for val in unique_vals:\n","            s = str(val).strip()                          # Normalize value string\n","\n","            # 4a. Skip if already allowed or valid combination\n","            if s in allowed:\n","                continue\n","            if s.startswith('{') and s.endswith('}'):\n","                parts = [x.strip() for x in s[1:-1].split(',')]\n","                if all(p in allowed for p in parts):\n","                    continue                              # Valid multi-value combo\n","\n","            # 4b. Replacement via label->value mapping\n","            if s in val_labels:\n","                new = val_labels[s]\n","                df.loc[df[col] == val, col] = new         # Replace with raw value\n","                replaced_values_info[col].append((val, new))\n","\n","            # 4c. Deletion for values in labels set\n","            elif s in labels_set:\n","                df.loc[df[col] == val, col] = np.nan       # Delete (set to NaN)\n","                deleted_labels_count[col] += 1\n","\n","            # 4d. Unknown values\n","            else:\n","                if warning_count < max_warnings:\n","                    warning_count += 1                     # Count warnings up to max\n","                not_found_values_info[col].append(val)\n","\n","        # 4e. Track columns with extra unknowns beyond max_warnings\n","        if warning_count > max_warnings:\n","            columns_with_extra_missing.append(col)\n","\n","    total_deleted = sum(deleted_labels_count.values())   # Total deletions across all columns\n","\n","    # 5. Optional: write cleaning report if filepath is provided\n","    if column_cleaning_report_filepath:\n","        try:\n","            with open(column_cleaning_report_filepath, 'w', encoding='utf-8') as f:\n","                # Section 1: Missing dictionary mappings\n","                f.write(\"Section 1 - Columns with No Dictionary Mappings\\n\")\n","                if columns_with_no_dict:\n","                    for c in columns_with_no_dict:\n","                        f.write(f\"  - {c}\\n\")\n","                else:\n","                    f.write(\"  (All columns had a mapping.)\\n\")\n","\n","                # Section 2: Replacements made\n","                f.write(\"\\nSection 2 - Replaced Values\\n\")\n","                any_rep = False\n","                for c, reps in replaced_values_info.items():\n","                    if reps:\n","                        any_rep = True\n","                        f.write(f\"\\n  Column '{c}':\\n\")\n","                        for old, new in reps:\n","                            f.write(f\"    - {old} -> {new}\\n\")\n","                if not any_rep:\n","                    f.write(\"  (No replacements made.)\\n\")\n","\n","                # Section 3: Deletions performed\n","                f.write(\"\\nSection 3 - Deleted Labels\\n\")\n","                if deleted_labels_count:\n","                    for c, cnt in deleted_labels_count.items():\n","                        f.write(f\"  - {c}: {cnt} deletions\\n\")\n","                    f.write(f\"\\nTotal deletions: {total_deleted}\\n\")\n","                else:\n","                    f.write(\"  (No deletions made.)\\n\")\n","\n","                # Section 4: Unknown values encountered\n","                f.write(\"\\nSection 4 - Unknown Values\\n\")\n","                if not_found_values_info:\n","                    for c, vals in not_found_values_info.items():\n","                        if vals:\n","                            f.write(f\"  - {c}: {len(vals)} unknowns\\n\")\n","                else:\n","                    f.write(\"  (No unknown values encountered.)\\n\")\n","\n","                f.write(\"\\nEnd of Report\\n\")\n","        except Exception:\n","            pass   # Skip report on any write error\n","\n","    return df  # Return cleaned DataFrame with replacements and deletions applied\n"],"id":"553f507a"},{"cell_type":"code","execution_count":21,"metadata":{"id":"6a53ed15","executionInfo":{"status":"ok","timestamp":1750152467309,"user_tz":-120,"elapsed":41,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# FUNCTION - Forward Fill Paired Columns & Drop Label Columns\n","# Forward-fills missing .value/.valuedischarge entries from paired label columns when numeric or datetime,\n","# then drops the label columns. Report is optional.\n","# ------------------------------------------------------------------------------\n","def forward_fill_numeric_datetime(\n","    df: pd.DataFrame,\n","    forward_fill_report_path: Optional[str] = None\n",") -> pd.DataFrame:\n","    \"\"\"\n","    Steps:\n","    1. Identify pairs of value and label columns based on suffix patterns.\n","    2. For each pair:\n","       a. Forward-fill missing numeric values from label column.\n","       b. Forward-fill remaining missing datetime values from label column.\n","       c. Record the number of filled entries and drop the label column.\n","    3. Optionally write a report if `forward_fill_report_path` is provided.\n","    4. Return the DataFrame with filled values and labels removed.\n","    \"\"\"\n","    # Step 1: Build mapping of value->label columns\n","    pairs: dict[str, str] = {}\n","    for col in df.columns:\n","        if col.endswith('.value') and col[:-6] + '.label' in df.columns:\n","            pairs[col] = col[:-6] + '.label'            # Pair .value with .label\n","        elif col.endswith('.valuedischarge') and col[:-14] + '.labeldischarge' in df.columns:\n","            pairs[col] = col[:-14] + '.labeldischarge'  # Pair .valuedischarge with .labeldischarge\n","\n","    # Prepare report container\n","    fill_report: dict[str, int] = {}\n","\n","    # Step 2: Process each value-label pair\n","    for value_col, label_col in pairs.items():\n","        # 2a. Mask rows where value is missing and label is present\n","        mask = df[value_col].isna() & df[label_col].notna()\n","\n","        # 2b. Numeric forward-fill attempt\n","        numeric_converted = pd.to_numeric(df.loc[mask, label_col], errors='coerce')  # Convert labels to numeric\n","        numeric_mask = numeric_converted.notna()  # Identify successful numeric conversions\n","        df.loc[mask, value_col] = df.loc[mask, value_col].fillna(numeric_converted)  # Fill numeric\n","\n","        # 2c. Datetime forward-fill for remaining\n","        mask_remaining = df[value_col].isna() & df[label_col].notna()  # Updated mask\n","        datetime_converted = pd.to_datetime(df.loc[mask_remaining, label_col], errors='coerce')  # Convert labels to datetime\n","        datetime_mask = datetime_converted.notna()  # Successful datetime conversions\n","        df.loc[mask_remaining, value_col] = df.loc[mask_remaining, value_col].fillna(datetime_converted)  # Fill datetime\n","\n","        # 2d. Record total fills and drop the label column\n","        fill_count = int(numeric_mask.sum() + datetime_mask.sum())  # Sum numeric + datetime fills\n","        fill_report[value_col] = fill_count\n","        df.drop(columns=[label_col], inplace=True)  # Remove label column after filling\n","\n","    # Step 3: Optional report writing\n","    if forward_fill_report_path:\n","        try:\n","            with open(forward_fill_report_path, 'w', encoding='utf-8') as f:\n","                total_cols = sum(1 for cnt in fill_report.values() if cnt > 0)  # Columns with any fills\n","                f.write(\"Forward Fill Report\\n\")\n","                f.write(\"===================\\n\\n\")\n","                f.write(f\"Total columns forward filled: {total_cols}\\n\\n\")\n","                f.write(\"Details for columns with fills:\\n\")\n","                for col, count in fill_report.items():\n","                    if count > 0:\n","                        f.write(f\"  - {col}: {count} values filled\\n\")\n","        except Exception:\n","            pass  # Skip reporting if path is invalid or write fails\n","\n","    # Step 4: Return the updated DataFrame\n","    return df  # Values filled and label columns removed\n"],"id":"6a53ed15"},{"cell_type":"code","execution_count":22,"metadata":{"id":"40138039","executionInfo":{"status":"ok","timestamp":1750152467311,"user_tz":-120,"elapsed":4,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# # FUNCTION - Drop Unwanted Columns\n","# # Drops label columns and selects the best candidate per prefix group; optional report generation.\n","# # -------------------------------------------------------------------------------------------------\n","def drop_unwanted_columns(\n","    df: pd.DataFrame,\n","    drop_report_path: Optional[str] = None\n",") -> pd.DataFrame:\n","    \"\"\"\n","    Drop unwanted columns from the DataFrame as follows:\n","\n","    1. Drop all columns with \"label\" in their name.\n","    2. For the remaining columns, group them by their prefix (the part before the first dot,\n","       or the whole name if no dot exists).\n","    3. For each group:\n","       - If the group has columns with different suffixes (e.g. \"uid.value\" and \"uid.valuedischarge\"),\n","         then keep the column ending with \".value\" and drop the others.\n","       - If the group also includes a column with the bare prefix (e.g. \"age\" alongside \"age.value\"),\n","         compare the bare column and the \".value\" column by counting non-null values. Keep the one with\n","         more non-null entries; if they are equal, keep the bare column.\n","       - If neither a bare column nor a \".value\" column exists, keep the first column in the group.\n","    4. If `drop_report_path` is provided, write a report showing, for each prefix group,\n","       the kept column and the dropped column(s).\n","    5. Return a DataFrame that contains only the kept columns.\n","    \"\"\"\n","    # Step 1: Drop all columns with \"label\" in their name.\n","    filtered_cols = [col for col in df.columns if \"label\" not in col]\n","    df = df[filtered_cols]\n","\n","    # Step 2: Group columns by prefix\n","    groups: dict[str, list[str]] = {}\n","    for col in df.columns:\n","        prefix = col.split('.')[0] if '.' in col else col\n","        groups.setdefault(prefix, []).append(col)\n","\n","    decisions: dict[str, tuple[str, list[str]]] = {}\n","\n","    # Step 3: Decide which column to keep per group\n","    for prefix, cols in groups.items():\n","        candidate_bare  = prefix if prefix in cols else None\n","        candidate_value = f\"{prefix}.value\" if f\"{prefix}.value\" in cols else None\n","\n","        if candidate_bare and candidate_value:\n","            # compare non-null counts\n","            count_bare  = df[candidate_bare].count()\n","            count_value = df[candidate_value].count()\n","            chosen = candidate_bare if count_bare >= count_value else candidate_value\n","        elif candidate_value:\n","            chosen = candidate_value\n","        elif candidate_bare:\n","            chosen = candidate_bare\n","        else:\n","            chosen = cols[0]\n","\n","        dropped = [c for c in cols if c != chosen]\n","        decisions[prefix] = (chosen, dropped)\n","\n","    # Step 4: Optional report writing\n","    if drop_report_path:\n","        report_lines = [\"Drop Columns Report\", \"===================\"]\n","        for prefix, (kept, dropped) in decisions.items():\n","            if dropped:\n","                report_lines.append(f\"Prefix '{prefix}': Kept column: {kept}\")\n","                report_lines.append(f\"   Dropped columns: {', '.join(dropped)}\")\n","            else:\n","                report_lines.append(f\"Prefix '{prefix}': Only column kept: {kept}\")\n","        report_text = \"\\n\".join(report_lines)\n","        try:\n","            with open(drop_report_path, 'w', encoding='utf-8') as f:\n","                f.write(report_text)\n","        except Exception:\n","            pass  # silently skip any write errors\n","\n","    # Step 5: Build and return final DataFrame\n","    kept_columns = [decisions[p][0] for p in decisions]\n","    final_cols = [col for col in df.columns if col in kept_columns]\n","    return df[final_cols]\n"],"id":"40138039"},{"cell_type":"code","execution_count":23,"metadata":{"id":"8cc119cb","executionInfo":{"status":"ok","timestamp":1750152467486,"user_tz":-120,"elapsed":174,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# # FUNCTION - Convert Columns to Correct Data Types (with optional report)\n","# # Attempts dictionary-based conversion, falls back to inference, and optionally generates a report.\n","# # ------------------------------------------------------------------------------\n","def convert_columns_and_create_report(\n","    df: pd.DataFrame,\n","    dict_mapping: dict[str, dict[str, any]],\n","    sample_size: int = 10000,\n","    random_state: int = 24,\n","    report_path: Optional[str] = 'datatype_conversion_report.txt'\n",") -> pd.DataFrame:\n","    # Helper: get base name - remove suffixes from df to lookup in dictionary\n","    def get_base_name(col: str) -> str:\n","        if col.endswith('.value'):\n","            return col[:-6]\n","        elif col.endswith('.valuedischarge'):\n","            return col[:-14]\n","        return col\n","\n","    # Helper: get a sample of non-missing, standardized values.\n","    def get_sample(series: pd.Series) -> pd.Series:\n","        non_missing = series[series.notnull()]\n","        if non_missing.empty:\n","            return pd.Series([], dtype=str)\n","        sample = non_missing.sample(n=min(sample_size, len(non_missing)), random_state=random_state)\n","        return sample.astype(str).str.strip()\n","\n","    # Valid boolean tokens.\n","    bool_valids = {\"true\", \"false\", \"yes\", \"no\", \"y\", \"n\"}\n","    bool_true = {\"true\", \"yes\", \"y\"}\n","\n","    # Prepare report dictionary.\n","    conv_report = {\n","        'converted_boolean': [],\n","        'should_be_boolean': {},\n","        'converted_numeric_int': [],\n","        'converted_numeric_float': [],\n","        'should_be_numeric': {},\n","        'converted_datetime': [],\n","        'should_be_datetime': {},\n","        'converted_categorical': [],\n","        'should_be_categorical': {},\n","        'converted_object': [],\n","        'columns_not_in_dict': [],\n","        'should_be_inferred': {}\n","    }\n","\n","    # Process only (.value/.valuedischarge) columns.\n","    process_columns = [col for col in df.columns if not (col.endswith('.label') or col.endswith('.labeldischarge'))]\n","\n","    # Helper: infer conversion (using a sample of non-missing values).\n","    def infer_conversion(series: pd.Series) -> (str, pd.Series):\n","        s = series.astype(str).str.strip()\n","\n","        # Try Boolean Conversion\n","        if s.str.lower().isin(bool_valids).all():\n","            conv = s.apply(lambda x: True if x.lower() in bool_true else False)\n","            return 'boolean', conv\n","\n","        # Try Numeric Conversion\n","        numeric_sample = pd.to_numeric(s, errors='coerce')\n","        if numeric_sample.notnull().all():\n","            new_series = pd.to_numeric(s, errors='coerce')\n","            non_null = new_series.dropna()\n","            if (not non_null.empty and non_null.apply(lambda x: np.isfinite(x) and np.isclose(x, round(x))).all()):\n","                return 'numeric_int', new_series.astype('Int64')\n","            else:\n","                return 'numeric_float', new_series\n","\n","        # Try Datetime Conversion\n","        dt_sample = pd.to_datetime(s, errors='coerce', infer_datetime_format=True)\n","        if dt_sample.notnull().all():\n","            return 'datetime', pd.to_datetime(s, errors='coerce', infer_datetime_format=True)\n","\n","        # Try Categorical if few unique values.\n","        if s.nunique() / len(s) <= 0.10:\n","            return 'categorical', s.astype('category')\n","\n","        # Otherwise consider as Object.\n","        return 'object', s\n","\n","    # Loop through columns.\n","    for col in process_columns:\n","        base = get_base_name(col)\n","        lower_key = base.lower()\n","        mapping = dict_mapping.get(lower_key)\n","        converted = False\n","\n","        # If the column is found in the dictionary:\n","        if mapping is not None:\n","            # Get datatype from the mapping\n","            dtype = mapping.get('datatype', '').strip().lower()\n","            # If no datatype is provided, infer the type using a sample.\n","            if dtype in ['', 'nan', 'none']:\n","                inf_type, _ = infer_conversion(get_sample(df[col]))\n","                dtype = inf_type\n","            allowed_values = mapping.get('values', set())\n","            logging.info(f\"Column '{col}' (base '{base}') using dictionary expected type '{dtype}'\")\n","\n","            # TRY DICTIONARY BASED - CONVERSION\n","            try:\n","                # ----BOOLEAN CONVERSION-------\n","                if dtype in ['boolean', 'bool']:\n","                    def to_bool(x):\n","                        if pd.isnull(x):\n","                            return x\n","                        s = str(x).strip().lower()\n","                        if s in bool_true:\n","                            return True\n","                        elif s in bool_valids:\n","                            return False\n","                        return np.nan\n","                    new_series = df[col].apply(to_bool).astype('boolean')\n","                    df[col] = new_series\n","                    conv_report['converted_boolean'].append(col)\n","\n","                # For single_select_option/dropdown with 2 allowed values,\n","                # check if allowed values are yes/no variants (including Y/N).\n","                elif (dtype in ['single_select_option', 'dropdown']) and (len(allowed_values) == 2):\n","                    allowed_values_lower = {str(v).strip().lower() for v in allowed_values if v is not None and str(v).strip() != \"\"}\n","                    yes_no_set = {\"yes\", \"no\", \"y\", \"n\", \"true\", \"false\", \"1\"}\n","                    if allowed_values_lower.issubset(yes_no_set):\n","                        def to_bool(x):\n","                            if pd.isnull(x):\n","                                return x\n","                            s = str(x).strip().lower()\n","                            if s in {\"yes\", \"y\", \"true\", \"1\"}:\n","                                return True\n","                            elif s in {\"no\", \"n\", \"false\"}:\n","                                return False\n","                            return np.nan\n","                        new_series = df[col].apply(to_bool).astype('boolean')\n","                        df[col] = new_series\n","                        conv_report['converted_boolean'].append(col)\n","                    else:\n","                        new_series = df[col].astype('category')\n","                        df[col] = new_series\n","                        conv_report['converted_categorical'].append(col)\n","\n","                # -------NUMERIC CONVERSION------\n","                elif any(sub in dtype for sub in ['number', 'numeric', 'period']):\n","                    new_series = df[col].apply(lambda x: str(x).replace(\",\", \".\") if pd.notnull(x) else x)\n","                    new_series = pd.to_numeric(new_series, errors='coerce')\n","                    non_null = new_series.dropna()\n","                    if not non_null.empty and non_null.apply(lambda x: np.isfinite(x) and np.isclose(x, round(x))).all():\n","                        new_series = new_series.astype('Int64')\n","                        num_type = 'numeric_int'\n","                    else:\n","                        num_type = 'numeric_float'\n","                    df[col] = new_series\n","                    if num_type == 'numeric_int':\n","                        conv_report['converted_numeric_int'].append(col)\n","                    else:\n","                        conv_report['converted_numeric_float'].append(col)\n","\n","                # ------------DATETIME CONVERSION----------\n","                elif any(sub in dtype for sub in ['datetime', 'date']):\n","                    new_series = df[col].apply(lambda x: pd.to_datetime(x, errors='coerce') if pd.notnull(x) else x)\n","                    df[col] = new_series\n","                    conv_report['converted_datetime'].append(col)\n","\n","                # ------------CATEGORICAL CONVERSION----------\n","                elif ('multi_select_option' in dtype or ((dtype in ['single_select_option', 'dropdown']) and (len(allowed_values) > 2))):\n","                    new_series = df[col].astype('category')\n","                    df[col] = new_series\n","                    conv_report['converted_categorical'].append(col)\n","\n","                # ------------String Conversion----------\n","                elif any(sub in dtype for sub in ['string', 'uid']):\n","                    new_series = df[col].apply(lambda x: str(x) if pd.notnull(x) else x)\n","                    df[col] = new_series\n","                    conv_report['converted_object'].append(col)\n","\n","                # ------FALLBACK TO OBJECT-------\n","                else:\n","                    new_series = df[col].apply(lambda x: str(x) if pd.notnull(x) else x)\n","                    df[col] = new_series\n","                    conv_report['converted_object'].append(col)\n","                converted = True\n","\n","            except Exception as e:\n","                logging.error(f\"Error converting '{col}' using type '{dtype}': {e}\")\n","                conv_report.setdefault('should_be_' + dtype, {})[col] = f\"Conversion error: {e}\"\n","\n","                # COLUMNS THAT FAILED DICTIONARY CONVERSION - USE SAMPLE INFERENCE CONVERSION\n","                try:\n","                    inf_type, _ = infer_conversion(get_sample(df[col]))\n","                    # ----NUMERIC CONVERSION----\n","                    if inf_type.startswith('numeric'):\n","                        new_series = pd.to_numeric(df[col].astype(str).str.strip(), errors='coerce')\n","                        non_null = new_series.dropna()\n","                        if not non_null.empty and non_null.apply(lambda x: np.isfinite(x) and np.isclose(x, round(x))).all():\n","                            new_series = new_series.astype('Int64')\n","                        df[col] = new_series\n","                        if inf_type.startswith('numeric_int'):\n","                            conv_report['converted_numeric_int'].append(col)\n","                        else:\n","                            conv_report['converted_numeric_float'].append(col)\n","\n","                    # ----DATETIME CONVERSION----\n","                    elif inf_type == 'datetime':\n","                        new_series = pd.to_datetime(df[col], errors='coerce', infer_datetime_format=True)\n","                        df[col] = new_series\n","                        conv_report['converted_datetime'].append(col)\n","\n","                    # ----BOOLEAN CONVERSION----\n","                    elif inf_type == 'boolean':\n","                        new_series = df[col].apply(lambda x: True if str(x).strip().lower() in bool_true else False if pd.notnull(x) else pd.NA).astype('boolean')\n","                        df[col] = new_series\n","                        conv_report['converted_boolean'].append(col)\n","\n","                    # ----CATEGORICAL CONVERSION----\n","                    elif inf_type == 'categorical':\n","                        new_series = df[col].astype('category')\n","                        df[col] = new_series\n","                        conv_report['converted_categorical'].append(col)\n","\n","                    # -----FALL BACK TO OBJECT--------\n","                    else:\n","                        new_series = df[col].apply(lambda x: str(x) if pd.notnull(x) else x)\n","                        df[col] = new_series\n","                        conv_report['converted_object'].append(col)\n","                except Exception as ex:\n","                    logging.error(f\"Fallback inference error for column '{col}': {ex}\")\n","                    conv_report.setdefault('should_be_inferred', {})[col] = f\"Fallback error: {ex}\"\n","        else:\n","            # COLUMNS NOT FOUND IN DICTIONARY - USE SAMPLE INFERENCE CONVERSION\n","            conv_report['columns_not_in_dict'].append(col)\n","            try:\n","                inf_type, _ = infer_conversion(get_sample(df[col]))\n","                if inf_type.startswith('numeric'):\n","                    new_series = pd.to_numeric(df[col].astype(str).str.strip(), errors='coerce')\n","                    non_null = new_series.dropna()\n","                    if not non_null.empty and non_null.apply(lambda x: np.isfinite(x) and np.isclose(x, round(x))).all():\n","                        new_series = new_series.astype('Int64')\n","                    df[col] = new_series\n","                    if inf_type.startswith('numeric_int'):\n","                        conv_report['converted_numeric_int'].append(col)\n","                    else:\n","                        conv_report['converted_numeric_float'].append(col)\n","                elif inf_type == 'datetime':\n","                    new_series = pd.to_datetime(df[col], errors='coerce', infer_datetime_format=True)\n","                    df[col] = new_series\n","                    conv_report['converted_datetime'].append(col)\n","                elif inf_type == 'boolean':\n","                    new_series = df[col].apply(lambda x: True if str(x).strip().lower() in bool_true else False if pd.notnull(x) else pd.NA).astype('boolean')\n","                    df[col] = new_series\n","                    conv_report['converted_boolean'].append(col)\n","                elif inf_type == 'categorical':\n","                    new_series = df[col].astype('category')\n","                    df[col] = new_series\n","                    conv_report['converted_categorical'].append(col)\n","                else:\n","                    new_series = df[col].apply(lambda x: str(x) if pd.notnull(x) else x)\n","                    df[col] = new_series\n","                    conv_report['converted_object'].append(col)\n","            except Exception as e:\n","                logging.error(f\"Inference conversion error for column '{col}': {e}\")\n","                conv_report.setdefault('should_be_inferred', {})[col] = f\"Inference conversion error: {e}\"\n","\n","    # Optional: write the conversion report if report_path is provided\n","    if report_path:\n","        try:\n","            with open(report_path, 'w', encoding='utf-8') as f:\n","                f.write(\"===== Combined Data Type Conversion Report =====\\n\\n\")\n","\n","                # 1. Boolean Section\n","                f.write(\"Section 1: Boolean Conversion\\n\")\n","                f.write(\"  Successfully Converted:\\n\")\n","                for c in conv_report['converted_boolean']:\n","                    f.write(f\"    - {c}\\n\")\n","                f.write(\"  Flagged as Should be Boolean:\\n\")\n","                for c in conv_report.get('should_be_boolean', {}):\n","                    f.write(f\"    - {c}\\n\")\n","                f.write(\"\\n\")\n","\n","                # 2. Numeric Section\n","                f.write(\"Section 2: Numeric Conversion\\n\")\n","                f.write(\"  Converted to Integer:\\n\")\n","                for c in conv_report['converted_numeric_int']:\n","                    f.write(f\"    - {c}\\n\")\n","                f.write(\"  Converted to Float:\\n\")\n","                for c in conv_report['converted_numeric_float']:\n","                    f.write(f\"    - {c}\\n\")\n","                f.write(\"  Flagged as Should be Numeric:\\n\")\n","                for c in conv_report.get('should_be_numeric', {}):\n","                    f.write(f\"    - {c}\\n\")\n","                f.write(\"\\n\")\n","\n","                # 3. Datetime Section\n","                f.write(\"Section 3: Datetime Conversion\\n\")\n","                f.write(\"  Successfully Converted:\\n\")\n","                for c in conv_report['converted_datetime']:\n","                    f.write(f\"    - {c}\\n\")\n","                f.write(\"  Flagged as Should be Datetime:\\n\")\n","                for c in conv_report.get('should_be_datetime', {}):\n","                    f.write(f\"    - {c}\\n\")\n","                f.write(\"\\n\")\n","\n","                # 4. Categorical Section\\n                f.write(\"Section 4: Categorical Conversion\\n\")\n","                f.write(\"  Successfully Converted:\\n\")\n","                for c in conv_report['converted_categorical']:\n","                    f.write(f\"    - {c}\\n\")\n","                f.write(\"  Flagged as Should be Categorical:\\n\")\n","                for c in conv_report.get('should_be_categorical', {}):\n","                    f.write(f\"    - {c}\\n\")\n","                f.write(\"\\n\")\n","\n","                # 5. String/Object Section\n","                f.write(\"Section 5: String/Object Conversion\\n\")\n","                for c in conv_report['converted_object']:\n","                    f.write(f\"    - {c}\\n\")\n","                f.write(\"\\n\")\n","\n","                # 6. Columns Not Found in Dictionary\\n                f.write(\"Section 6: Columns Not Found in Dictionary (Inference used):\\n\")\n","                for c in conv_report['columns_not_in_dict']:\n","                    f.write(f\"    - {c}\\n\")\n","\n","                # 7. Should Be Inferred\n","                if conv_report.get('should_be_inferred'):\n","                    f.write(\"  Flagged by Inference as Should be Converted:\\n\")\n","                    for c in conv_report['should_be_inferred']:\n","                        f.write(f\"    - {c}\\n\")\n","\n","                f.write(\"\\n===== End of Combined Data Type Conversion Report =====\\n\")\n","        except Exception:\n","            pass\n","\n","    return df\n"],"id":"8cc119cb"},{"cell_type":"code","execution_count":24,"metadata":{"id":"7d63d055","executionInfo":{"status":"ok","timestamp":1750152467516,"user_tz":-120,"elapsed":13,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# FUNCTION - Generate a Summary of the Dataframe in EXCEL\n","# For each column, extract its description and datatype, then optionally save the summary files.\n","# ------------------------------------------------------------------------------\n","def map_df_columns_to_descriptions_and_datatypes(\n","    df: pd.DataFrame,\n","    excel_filepath: str,\n","    output_csv_filepath: str,\n","    output_excel_filepath: str\n",") -> pd.DataFrame:\n","    \"\"\"\n","    Steps:\n","    1. Load the Excel dictionary workbook from `excel_filepath`.\n","    2. For each sheet:\n","       a. Parse into a DataFrame, skip on parse errors.\n","       b. Ensure 'Key' and 'Label' columns exist; skip sheet otherwise.\n","       c. Standardize 'Key' to lowercase and strip whitespace.\n","       d. Build a mapping of base column name -> description (first occurrence wins).\n","    3. For each column in `df`:\n","       a. Strip any suffix after the first dot and lowercase to get base name.\n","       b. Look up description in the mapping, defaulting to 'Not found in dictionary'.\n","       c. Record the column's pandas dtype as a string.\n","    4. Assemble a summary DataFrame with columns ['column','description','datatype'], sort by 'column'.\n","    5. Attempt to save the summary to CSV and Excel; if writing fails, skip without raising.\n","    6. Return the summary DataFrame.\n","    \"\"\"\n","    # Step 1: Load dictionary workbook\n","    try:\n","        xlsx = pd.ExcelFile(excel_filepath)\n","    except Exception as e:\n","        raise ValueError(f\"Failed to load Excel file: {e}\")\n","\n","    col_mapping: dict[str, str] = {}  # base_name -> description\n","\n","    # Step 2: Process each sheet to build the mapping\n","    for sheet in xlsx.sheet_names:\n","        try:\n","            df_sheet = xlsx.parse(sheet)  # Parse the sheet\n","        except Exception:\n","            continue  # Skip on parse failure\n","\n","        # 2b: Check for required columns\n","        if not {'Key', 'Label'}.issubset(df_sheet.columns):\n","            continue  # Skip sheets missing necessary headers\n","\n","        # 2c: Rename and clean 'Key' and 'Label'\n","        rename_dict = {}\n","        for col in df_sheet.columns:\n","            lower = str(col).strip().lower()\n","            if lower == 'key':\n","                rename_dict[col] = 'key'\n","            elif lower == 'label':\n","                rename_dict[col] = 'label'\n","        df_sheet = df_sheet.rename(columns=rename_dict)\n","        df_sheet['key'] = df_sheet['key'].astype(str).str.strip().str.lower()\n","        df_sheet['label'] = df_sheet['label'].astype(str).str.strip()\n","\n","        # 2d: Populate mapping (first occurrence retained)\n","        for _, row in df_sheet.iterrows():\n","            base = row['key']\n","            if base and base not in col_mapping:\n","                col_mapping[base] = row['label']\n","\n","    # Step 3: Build list of summary entries\n","    output_rows: list[dict[str, str]] = []\n","    for col in df.columns:\n","        base = col.split('.')[0].strip().lower()  # Remove suffix and lowercase\n","        description = col_mapping.get(base, 'Not found in dictionary')\n","        datatype = str(df[col].dtype)\n","        output_rows.append({'column': col, 'description': description, 'datatype': datatype})\n","\n","    # Step 4: Assemble summary DataFrame\n","    result_df = pd.DataFrame(output_rows).sort_values(by='column').reset_index(drop=True)\n","\n","    # Step 5: Optionally save the files, skipping on any write error\n","    try:\n","        result_df.to_csv(output_csv_filepath, index=False)\n","    except Exception:\n","        pass  # Skip CSV save on failure\n","    try:\n","        result_df.to_excel(output_excel_filepath, index=False)\n","    except Exception:\n","        pass  # Skip Excel save on failure\n","\n","    # Step 6: Return the generated summary DataFrame\n","    return result_df\n"],"id":"7d63d055"},{"cell_type":"code","execution_count":25,"metadata":{"id":"c5c969fe","executionInfo":{"status":"ok","timestamp":1750152467537,"user_tz":-120,"elapsed":17,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# FUNCTION - Forward Fill ‚ÄòNone‚Äô Values from Label Columns\n","# For each paired .value/.label or .valuedischarge/.labeldischarge column:\n","# if the value is missing but the label is one of {\"none\",\"normal\",\"norm\"}, fill the value.\n","# Optionally writes a report of fills if a valid path is provided.\n","# ------------------------------------------------------------------------------\n","def fill_missing_with_label_value(\n","    df: pd.DataFrame,\n","    forward_fill_report_path: Optional[str] = None\n",") -> pd.DataFrame:\n","    \"\"\"\n","    Steps:\n","    1. Identify pairs of value and label columns based on naming conventions.\n","    2. For each pair, create a mask where the value is NaN and label is one of the target strings.\n","    3. Fill missing values in the value column with the corresponding label entry.\n","    4. Record the count of fills per value column.\n","    5. Optionally write a report if `forward_fill_report_path` is provided and writable.\n","    6. Return the DataFrame with filled values.\n","    \"\"\"\n","    # Step 1: Build mapping of value->label column pairs\n","    pairs: dict[str, str] = {}\n","    for col in df.columns:\n","        if col.endswith('.value') and col[:-6] + '.label' in df.columns:\n","            pairs[col] = col[:-6] + '.label'  # Pair .value with .label\n","        elif col.endswith('.valuedischarge') and col[:-14] + '.labeldischarge' in df.columns:\n","            pairs[col] = col[:-14] + '.labeldischarge'  # Pair .valuedischarge with .labeldischarge\n","\n","    # Prepare fill count report\n","    fill_report: dict[str, int] = {}\n","\n","    # Step 2 and 3: Process each pair to forward-fill 'none' labels\n","    for value_col, label_col in pairs.items():\n","        # Mask: value is NaN, label is one of 'none', 'normal', 'norm'\n","        mask = (\n","            df[value_col].isna() &\n","            df[label_col].notna() &\n","            df[label_col].astype(str).str.lower().isin([\"none\",\"normal\",\"norm\"])\n","        )\n","        # Fill NaNs in the value column with the label's value\n","        df.loc[mask, value_col] = df.loc[mask, label_col]\n","        # Record how many were filled\n","        fill_report[value_col] = int(mask.sum())\n","        # Note: label columns are retained per original logic\n","\n","    # Step 5: Optionally write the fill report\n","    if forward_fill_report_path:\n","        try:\n","            with open(forward_fill_report_path, 'w', encoding='utf-8') as f:\n","                total_cols = sum(1 for cnt in fill_report.values() if cnt > 0)\n","                f.write(\"Forward Fill Report\\n\")\n","                f.write(\"===================\\n\\n\")\n","                f.write(f\"Total columns forward filled: {total_cols}\\n\\n\")\n","                f.write(\"Details for columns with fills:\\n\")\n","                for col, count in fill_report.items():\n","                    if count > 0:\n","                        f.write(f\"  - {col}: {count} values filled\\n\")\n","        except Exception:\n","            pass  # Skip report generation if path invalid or write error\n","\n","    # Step 6: Return the updated DataFrame\n","    return df  # Values filled; label columns remain in DataFrame"],"id":"c5c969fe"},{"cell_type":"markdown","metadata":{"id":"02d03a1b"},"source":["#### Numeric Validation Functions"],"id":"02d03a1b"},{"cell_type":"code","execution_count":26,"metadata":{"id":"37419a59","executionInfo":{"status":"ok","timestamp":1750152467557,"user_tz":-120,"elapsed":36,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# FUNCTION - Load & Aggregate Validation Dictionary\n","# Reads a specified sheet from an Excel file to extract numeric validation ranges.\n","# ------------------------------------------------------------------------------\n","def load_and_aggregate_validation_dictionary(\n","    dict_filepath: str,\n","    sheet_name: str\n",") -> dict[str, tuple[float, float]]:\n","    \"\"\"\n","    Steps:\n","    1. Load the specified sheet from the Excel file into a DataFrame.\n","    2. Iterate each row:\n","       a. Extract column name from first cell (column A), normalize to lowercase.\n","       b. Extract range string from fifth cell (column E).\n","       c. Skip empty or NaN ranges.\n","       d. Use regex to parse \"min - max\" (allowing '-' or '‚Äì').\n","          - On match, convert to floats and store in validation_ranges.\n","          - On parse error, log and skip.\n","          - On format mismatch, log a warning.\n","    3. Return the mapping of column names to (min_value, max_value) tuples.\n","    \"\"\"\n","    # Step 1: Attempt to read the specified sheet\n","    try:\n","        df = pd.read_excel(dict_filepath, sheet_name=sheet_name)\n","    except Exception as e:\n","        logging.error(f\"Failed to load sheet '{sheet_name}' from '{dict_filepath}': {e}\")\n","        raise  # Propagate error if file or sheet is unreadable\n","\n","    validation_ranges: dict[str, tuple[float, float]] = {}\n","\n","    # Step 2: Process each row for column name and range\n","    for idx, row in df.iterrows():\n","        # 2a. Extract and normalize column name\n","        col_name = str(row.iloc[0]).strip().lower()\n","        # 2b. Extract raw range string (column E)\n","        range_str = row.iloc[4]\n","        if pd.isna(range_str) or str(range_str).strip() == '':\n","            continue  # Skip rows without a valid range\n","\n","        # 2c. Parse range using regex, allow hyphen or en-dash\n","        pattern = r'([\\d\\.\\-]+)\\s*[-‚Äì]\\s*([\\d\\.\\-]+)'\n","        match = re.match(pattern, str(range_str).strip())\n","        if match:\n","            try:\n","                min_val = float(match.group(1))  # Convert lower bound\n","                max_val = float(match.group(2))  # Convert upper bound\n","                validation_ranges[col_name] = (min_val, max_val)\n","            except Exception as e:\n","                logging.error(\n","                    f\"Error parsing numeric range for '{col_name}': '{range_str}' -> {e}\"\n","                )  # Log conversion failure\n","        else:\n","            logging.warning(\n","                f\"Unrecognized range format for '{col_name}': '{range_str}'\"\n","            )  # Warn on format mismatch\n","\n","    logging.info(\n","        f\"Completed loading validation ranges from sheet '{sheet_name}'.\"\n","    )  # Log final status\n","    # Step 3: Return the assembled dictionary\n","    return validation_ranges  # Mapping: column_name -> (min_value, max_value)\n"],"id":"37419a59"},{"cell_type":"code","execution_count":27,"metadata":{"id":"b302655a","executionInfo":{"status":"ok","timestamp":1750152467589,"user_tz":-120,"elapsed":33,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# FUNCTION - Raise Flags for Disallowed Numeric Values\n","# Checks numeric columns against allowed ranges and optionally writes a report.\n","# ------------------------------------------------------------------------------\n","def raise_flags_numeric(\n","    df: pd.DataFrame,\n","    range_mapping: dict[str, tuple[float, float]],\n","    report_file: Optional[str] = None,\n","    skip_columns: list[str] = ['uid', 'facility', 'uniquekey', \"startedatdischarge\", \"completedatdischarge\", \"ingestedatdischarge\"]\n",") -> pd.DataFrame:\n","    \"\"\"\n","    Steps:\n","    1. Prepare skip_columns list and make a copy of the DataFrame.\n","    2. Normalize mapping keys to match DataFrame columns.\n","    3. Iterate over each column:\n","       a. Skip UID/facility/uniquekey or user-specified columns.\n","       b. If column is numeric and in mapping, flag disallowed values and replace on the copy.\n","    4. Build report_lines summarizing flagged values and skipped columns.\n","    5. Optionally write the report if `report_file` is provided and writable.\n","    6. Return the modified DataFrame copy.\n","    \"\"\"\n","    # 1. Handle default skip_columns and copy DataFrame\n","    if skip_columns is None:\n","        skip_columns = []  # No extra skips by default\n","    df_copy = df.copy()    # Work on a copy to preserve original\n","\n","    # 2. Normalize DataFrame column set and mapping keys\n","    df_cols = set(df_copy.columns)  # Existing columns\n","    normalized_mapping: dict[str, tuple[float, float]] = {}\n","    for key, value in range_mapping.items():\n","        norm_key = key.lower().strip()  # Lowercase mapping key\n","        candidate = norm_key\n","        if norm_key not in df_cols:\n","            parts = norm_key.split('.')\n","            while len(parts) > 1:\n","                parts.pop()              # Remove suffix segment\n","                candidate = '.'.join(parts)\n","                if candidate in df_cols:\n","                    break\n","        # Record first mapping for each candidate\n","        if candidate not in normalized_mapping:\n","            normalized_mapping[candidate] = value\n","        else:\n","            logging.warning(f\"Duplicate mapping for '{candidate}', using first occurrence.\")\n","\n","    report_info: dict[str, dict[str, Any]] = {}  # Hold flagged details\n","    skipped_columns_not_processed: list[str] = []\n","\n","    # 3. Process each column in the copy\n","    for col in df_copy.columns:\n","        col_lower = col.lower()\n","        # 3a. Skip identifiers and user-specified\n","        if (\n","            'uid' in col_lower\n","            or col_lower in ['facility','uniquekey']\n","            or col in skip_columns\n","        ):\n","            skipped_columns_not_processed.append(col)\n","            continue\n","\n","        # 3b. Flag and replace disallowed numeric values\n","        if pd.api.types.is_numeric_dtype(df_copy[col]) and col in normalized_mapping:\n","            min_val, max_val = normalized_mapping[col]\n","            # Helper to test validity\n","            def is_allowed(val):\n","                if pd.isnull(val):\n","                    return True  # Preserve NaNs\n","                try:\n","                    return min_val <= val <= max_val\n","                except Exception:\n","                    return False\n","\n","            mask = df_copy[col].apply(is_allowed)  # True if allowed\n","            num_replacements = (~mask).sum()       # Count disallowed\n","            bad_vals = df_copy[col][~mask].dropna().unique().tolist()\n","\n","            if num_replacements > 0:\n","                report_info[col] = {\n","                    'allowed_range': (min_val, max_val),\n","                    'disallowed_values': bad_vals,\n","                    'rows_affected': int(num_replacements)\n","                }\n","                # Replace disallowed with NaN\n","                df_copy[col] = df_copy[col].where(mask, other=np.nan)\n","\n","    # 4. Build report lines\n","    report_lines: list[str] = []\n","    report_lines.append(\"Numeric Range Disallowed Values Report\")\n","    report_lines.append(\"========================================\")\n","    report_lines.append(\"\")\n","    if report_info:\n","        for col, info in report_info.items():\n","            report_lines.append(f\"Column: {col}\")\n","            report_lines.append(f\"  Allowed Range: {info['allowed_range']}\")\n","            report_lines.append(f\"  Disallowed Values: {info['disallowed_values']}\")\n","            report_lines.append(f\"  Rows Affected: {info['rows_affected']}\")\n","            report_lines.append(\"\")\n","    else:\n","        report_lines.append(\"No disallowed numeric values found in any column.\")\n","        report_lines.append(\"\")\n","\n","    if skipped_columns_not_processed:\n","        report_lines.append(\"Columns Skipped:\")\n","        report_lines.append(\"----------------\")\n","        for sc in skipped_columns_not_processed:\n","            report_lines.append(f\"  - {sc}\")\n","        report_lines.append(\"\")\n","\n","    # 5. Optional report writing\n","    if report_file:\n","        try:\n","            with open(report_file, 'w', encoding='utf-8') as f:\n","                f.write(\"\\n\".join(report_lines))  # Overwrite report\n","        except Exception:\n","            pass  # Skip report on any write error\n","\n","    # 6. Return the modified copy\n","    return df_copy  # DataFrame with disallowed numeric values set to NaN\n"],"id":"b302655a"},{"cell_type":"code","execution_count":28,"metadata":{"id":"4a4cf8ff","executionInfo":{"status":"ok","timestamp":1750152467635,"user_tz":-120,"elapsed":42,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# FUNCTION - Validate Numeric Ranges\n","# Cleans a DataFrame by replacing out-of-range numeric values with NaN and optionally writes a report.\n","# ------------------------------------------------------------------------------\n","def validate_numeric_ranges(\n","    df: pd.DataFrame,\n","    range_mapping: dict[str, tuple[float, float]],\n","    report_file: Optional[str] = None,\n","    skip_columns: list[str] = ['uid', 'facility', 'uniquekey', \"startedatdischarge\", \"completedatdischarge\", \"ingestedatdischarge\"]\n",") -> pd.DataFrame:\n","    \"\"\"\n","    Steps:\n","    1. Initialize skip_columns list and logging imports assumed.\n","    2. Normalize DataFrame columns and mapping keys to match.\n","    3. Iterate each column:\n","       a. Skip UID/facility/uniquekey or user-specified columns.\n","       b. If numeric and in mapping, flag and replace out-of-range values.\n","    4. Build report_lines summarizing disallowed values and skipped columns.\n","    5. Optionally write report if `report_file` is provided and writable.\n","    6. Return modified DataFrame.\n","    \"\"\"\n","    # Step 1: Prepare skip list and imports\n","    if skip_columns is None:\n","        skip_columns = []  # Default no extra skips\n","\n","    # Step 2: Normalize mapping keys to DataFrame columns\n","    df_cols = set(df.columns)\n","    normalized_mapping: dict[str, tuple[float, float]] = {}\n","    for key, (min_val, max_val) in range_mapping.items():\n","        norm_key = key.lower().strip()  # Lowercase and trim\n","        candidate = norm_key\n","        if norm_key not in df_cols:\n","            parts = norm_key.split('.')\n","            while len(parts) > 1:\n","                parts.pop()  # Remove last segment\n","                candidate = '.'.join(parts)\n","                if candidate in df_cols:\n","                    break\n","        if candidate not in normalized_mapping:\n","            normalized_mapping[candidate] = (min_val, max_val)\n","        else:\n","            logging.warning(f\"Duplicate mapping for '{candidate}', keeping first mapping.\")\n","\n","    report_info: dict[str, dict[str, Any]] = {}  # Store flagged details per column\n","    skipped_columns_not_processed: list[str] = []  # Columns skipped\n","\n","    # Step 3: Process each DataFrame column\n","    for col in df.columns:\n","        col_lower = col.lower()\n","        # 3a. Skip identifiers and user-specified\n","        if (\n","            'uid' in col_lower\n","            or col_lower in ['facility', 'uniquekey']\n","            or col in skip_columns\n","        ):\n","            skipped_columns_not_processed.append(col)\n","            continue\n","\n","        # 3b. Flag and replace out-of-range numeric values\n","        if pd.api.types.is_numeric_dtype(df[col]) and col in normalized_mapping:\n","            min_val, max_val = normalized_mapping[col]\n","            # Helper to test validity\n","            def is_allowed(val):\n","                if pd.isnull(val):\n","                    return True  # Preserve NaNs\n","                try:\n","                    return min_val <= val <= max_val\n","                except Exception:\n","                    return False\n","\n","            mask_allowed = df[col].apply(is_allowed)  # True where allowed\n","            num_flags = (~mask_allowed).sum()         # Count of disallowed values\n","            unique_bad = df[col][~mask_allowed].dropna().unique().tolist()\n","\n","            if num_flags > 0:\n","                report_info[col] = {\n","                    'allowed_range': (min_val, max_val),\n","                    'disallowed_values': unique_bad,\n","                    'rows_affected': int(num_flags)\n","                }\n","                # Replace disallowed with NaN\n","                df[col] = df[col].where(mask_allowed, other=np.nan)\n","\n","    # Step 4: Build report lines\n","    report_lines: list[str] = []\n","    report_lines.append(\"Numeric Range Disallowed Values Report\")\n","    report_lines.append(\"========================================\")\n","    report_lines.append(\"\")\n","    if report_info:\n","        for col, info in report_info.items():\n","            report_lines.append(f\"Column: {col}\")\n","            report_lines.append(f\"  Allowed Range: {info['allowed_range']}\")\n","            report_lines.append(f\"  Disallowed Values: {info['disallowed_values']}\")\n","            report_lines.append(f\"  Rows Affected: {info['rows_affected']}\")\n","            report_lines.append(\"\")\n","    else:\n","        report_lines.append(\"No disallowed numeric values found in any column.\")\n","        report_lines.append(\"\")\n","\n","    if skipped_columns_not_processed:\n","        report_lines.append(\"Columns Skipped:\")\n","        report_lines.append(\"----------------\")\n","        for sc in skipped_columns_not_processed:\n","            report_lines.append(f\"  - {sc}\")\n","        report_lines.append(\"\")\n","\n","    # Step 5: Optional report writing\n","    if report_file:\n","        try:\n","            with open(report_file, 'w', encoding='utf-8') as f:\n","                f.write(\"\\n\".join(report_lines))  # Write out the report\n","        except Exception:\n","            pass  # Silently skip report on error\n","\n","    # Step 6: Return DataFrame with disallowed numeric values removed\n","    return df  # DataFrame modified in place"],"id":"4a4cf8ff"},{"cell_type":"code","execution_count":29,"metadata":{"id":"93d5212a","executionInfo":{"status":"ok","timestamp":1750152467644,"user_tz":-120,"elapsed":3,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# # FUNCTION - Convert weight to grams\n","# # Normalizes mixed integer/float weights by assuming decimals indicate kilograms.\n","# # ------------------------------------------------------------------------------\n","# def convert_weight_to_grams(\n","#     df: pd.DataFrame,\n","#     cols: str | list[str],\n","#     kg_threshold: int = 10\n","# ) -> pd.DataFrame:\n","#     \"\"\"\n","#     Steps:\n","#     1. Ensure `cols` is a list of column names.\n","#     2. Define `convert_value` to:\n","#        a. Return NaN for missing values.\n","#        b. Detect if string has no decimal; parse as int, convert to grams if < kg_threshold.\n","#        c. If decimal part is all zeros, treat like integer above.\n","#        d. Otherwise assume value in kilograms, multiply by 1000 precisely.\n","#     3. Apply `convert_value` to each specified column, logging errors for missing columns.\n","#     4. Return the DataFrame with converted weight columns.\n","#     \"\"\"\n","#     # 1. Normalize `cols` to list\n","#     if isinstance(cols, str):\n","#         cols = [cols]\n","\n","#     # 2. Helper to convert a single cell's value\n","#     def convert_value(x):\n","#         if pd.isna(x):\n","#             return np.nan  # Preserve missing values\n","#         s = str(x).strip()  # Normalize to trimmed string\n","\n","#         # 2b. No decimal point => integer-like\n","#         if '.' not in s:\n","#             try:\n","#                 val = int(s)\n","#                 return val * 1000 if val < kg_threshold else val\n","#             except ValueError:\n","#                 logging.warning(f\"Value '{s}' cannot be converted to int.\")\n","#                 return np.nan\n","\n","#         # 2c. Split integer and fractional parts\n","#         before, _, after = s.partition('.')\n","#         # If fractional part empty or all zeros, treat as integer\n","#         if after == '' or after.strip('0') == '':\n","#             try:\n","#                 val = int(before)\n","#                 return val * 1000 if val < kg_threshold else val\n","#             except ValueError:\n","#                 logging.warning(f\"Value '{s}' cannot be converted to int.\")\n","#                 return np.nan\n","#         # 2d. Nonzero fractional => treat as kilograms precisely\n","#         try:\n","#             converted = Decimal(s) * Decimal('1000')  # Precise arithmetic\n","#             return int(converted)\n","#         except Exception as e:\n","#             logging.warning(f\"Value '{s}' could not be converted: {e}\")\n","#             return np.nan\n","\n","#     # 3. Apply conversion to each column\n","#     for col in cols:\n","#         if col not in df.columns:\n","#             logging.error(f\"Column '{col}' not found in DataFrame.\")\n","#             continue  # Skip missing columns without stopping\n","#         df[col] = df[col].apply(convert_value)  # Convert all values in the column\n","\n","#     # 4. Return DataFrame with updated weight columns\n","#     return df\n"],"id":"93d5212a"},{"cell_type":"code","execution_count":30,"metadata":{"id":"5a2b68c5","executionInfo":{"status":"ok","timestamp":1750152467675,"user_tz":-120,"elapsed":28,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# # FUNCTION - Convert weight to grams\n","# # Normalizes mixed integer/float weights by assuming decimals indicate kilograms.\n","# # ------------------------------------------------------------------------------\n","def convert_weight_to_kgs(\n","    df: pd.DataFrame,\n","    cols: str | list[str],\n","    kg_threshold: int = 10\n",") -> pd.DataFrame:\n","    \"\"\"\n","    For each specified column:\n","      ‚Ä¢ Leave NaNs untouched.\n","      ‚Ä¢ Parse the value (int/float/str ‚Üí Decimal for precision).\n","      ‚Ä¢ If the numeric value > kg_threshold, divide by 1000.\n","        Otherwise keep the numeric value as-is.\n","    Returns the same DataFrame with adjusted columns.\n","    \"\"\"\n","\n","    # 1. Normalise `cols` to a list\n","    if isinstance(cols, str):\n","        cols = [cols]\n","\n","    # 2. Helper for one cell\n","    def _convert(x):\n","        if pd.isna(x):\n","            return np.nan\n","\n","        s = str(x).strip()\n","        try:\n","            val = Decimal(s)\n","        except (InvalidOperation, ValueError):\n","            logging.warning(f\"Value '{s}' could not be parsed as a number.\")\n","            return np.nan\n","\n","        return float(val / Decimal('1000')) if val > kg_threshold else float(val)\n","\n","    # 3. Apply to each requested column\n","    for col in cols:\n","        if col not in df.columns:\n","            logging.error(f\"Column '{col}' not found in DataFrame.\")\n","            continue\n","        df[col] = df[col].apply(_convert)\n","\n","    # 4. Return the (now-modified) DataFrame\n","    return df\n"],"id":"5a2b68c5"},{"cell_type":"code","execution_count":31,"metadata":{"id":"591a89ae","executionInfo":{"status":"ok","timestamp":1750152467677,"user_tz":-120,"elapsed":3,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# FUNCTION - Purify Columns by Deleting Disallowed Data Types\n","# Cleans specified columns by removing values incompatible with a target dtype and optionally writes a report.\n","# ------------------------------------------------------------------------------\n","def purify_column_num(\n","    df: pd.DataFrame,\n","    dtype: str,\n","    cols: list[str] = None,\n","    report_path: Optional[str] = None\n",") -> pd.DataFrame:\n","    \"\"\"\n","    Steps:\n","    1. Determine which columns to process, skipping 'facility', 'uid', and 'uniquekey'.\n","    2. Define helper to compare old vs new values safely, including NaNs.\n","    3. For each target column:\n","       a. If Boolean type, apply boolean conversion and track deletions.\n","       b. Otherwise, test each cell against the target type, delete invalid entries.\n","       c. After cleaning, cast the column to the desired pandas dtype.\n","       d. Record deletion counts and old‚Üínew mappings.\n","    4. Optionally write a detailed report if `report_path` is provided.\n","    5. Return the DataFrame with purified columns.\n","    \"\"\"\n","    # 1. Identify columns to purify\n","    skip_cols = {'facility', 'uid', 'uniquekey', 'startedatdischarge', 'completedatdischarge', 'ingestedatdischarge'}\n","    if not cols:\n","        cols = [c for c in df.columns if c.lower() not in skip_cols]\n","    else:\n","        cols = [c for c in cols if c.lower() not in skip_cols]\n","\n","    # 2. Helper: detect changes, including NaN differences\n","    def values_differ(old_val, new_val):\n","        if pd.isna(old_val) and pd.isna(new_val):\n","            return False\n","        if pd.isna(old_val) or pd.isna(new_val):\n","            return True\n","        return old_val != new_val\n","\n","    report_entries: list[str] = []  # Gather report lines\n","\n","    # 3. Process each column\n","    for col in cols:\n","        if col not in df.columns:\n","            raise KeyError(f\"Column '{col}' does not exist.\")\n","\n","        mapping_counts: dict[tuple[Any, Any], int] = {}\n","        deletion_counts: dict[Any, int] = {}\n","\n","        # 3a. Boolean conversion\n","        if dtype.lower() == 'boolean':\n","            def to_bool(x):\n","                if pd.isnull(x):\n","                    return pd.NA\n","                s = str(x).strip().lower()\n","                if s in {'yes','y','true','1','in'}:\n","                    return True\n","                if s in {'no','n','false','out'}:\n","                    return False\n","                return pd.NA\n","\n","            original = df[col].copy()\n","            df[col] = df[col].apply(to_bool).astype('boolean')\n","\n","            # Track changes\n","            for idx, new in df[col].items():\n","                old = original.loc[idx]\n","                if values_differ(old, new):\n","                    mapping_counts[(old,new)] = mapping_counts.get((old,new),0) + 1\n","                    if pd.isna(new) and pd.notna(old):\n","                        deletion_counts[old] = deletion_counts.get(old,0) + 1\n","\n","            # Report section\n","            total_deleted = sum(deletion_counts.values())\n","            report_entries.append(f\"Column: {col} (Boolean)\")\n","            report_entries.append(f\"Total deleted/converted values: {total_deleted}\")\n","            report_entries.append(\"Deleted values:\")\n","            if deletion_counts:\n","                for val, cnt in deletion_counts.items():\n","                    report_entries.append(f\"  {val}: {cnt}\")\n","            else:\n","                report_entries.append(\"  None\")\n","            report_entries.append(\"Old ‚Üí New mappings:\")\n","            if mapping_counts:\n","                for (o,n),cnt in mapping_counts.items():\n","                    report_entries.append(f\"  {o} ‚Üí {n}: {cnt}\")\n","            else:\n","                report_entries.append(\"  No changes\")\n","            report_entries.append(\"\")\n","\n","        # 3b. Other data types\n","        else:\n","            for idx, val in df[col].items():\n","                if pd.isnull(val):\n","                    continue\n","                orig = val\n","                valid = True\n","                if dtype.lower() == 'datetime':\n","                    try:\n","                        pd.to_datetime(val, errors='raise')\n","                    except:\n","                        valid = False\n","                elif dtype.lower() == 'numeric':\n","                    try:\n","                        pd.to_numeric(val, errors='raise')\n","                    except:\n","                        valid = False\n","                elif dtype.lower() == 'categorical':\n","                    if not isinstance(val, str):\n","                        valid = False\n","                else:\n","                    raise ValueError(\"Unsupported dtype. Use datetime, numeric, Boolean, or categorical.\")\n","\n","                if not valid:\n","                    deletion_counts[orig] = deletion_counts.get(orig,0) + 1\n","                    mapping_counts[(orig,pd.NA)] = mapping_counts.get((orig,pd.NA),0) + 1\n","                    df.at[idx, col] = pd.NA\n","\n","            # 3c. Cast to target dtype\n","            if dtype.lower() == 'numeric':\n","                df[col] = pd.to_numeric(df[col], errors='coerce')\n","            elif dtype.lower() == 'datetime':\n","                df[col] = pd.to_datetime(df[col], errors='coerce')\n","            elif dtype.lower() == 'categorical':\n","                df[col] = df[col].astype('category')\n","\n","            # Report section\n","            total_deleted = sum(deletion_counts.values())\n","            report_entries.append(f\"Column: {col}\")\n","            report_entries.append(f\"Total deleted values: {total_deleted}\")\n","            report_entries.append(\"Deleted values:\")\n","            if deletion_counts:\n","                for val, cnt in deletion_counts.items():\n","                    report_entries.append(f\"  {val}: {cnt}\")\n","            else:\n","                report_entries.append(\"  None\")\n","            report_entries.append(\"Old ‚Üí New mappings:\")\n","            if mapping_counts:\n","                for (o,n),cnt in mapping_counts.items():\n","                    report_entries.append(f\"  {o} ‚Üí {n}: {cnt}\")\n","            else:\n","                report_entries.append(\"  No changes\")\n","            report_entries.append(\"\")\n","\n","    # 4. Optional report writing\n","    if report_path:\n","        try:\n","            with open(report_path, 'w', encoding='utf-8') as f:\n","                f.write(\"\\n\".join(report_entries))\n","        except:\n","            pass  # Skip if unable to write\n","\n","    # 5. Return the purified DataFrame\n","    return df\n"],"id":"591a89ae"},{"cell_type":"markdown","metadata":{"id":"c1c1c7fd"},"source":["#### Boolean Validation Functions"],"id":"c1c1c7fd"},{"cell_type":"code","execution_count":32,"metadata":{"id":"aff47c9e","executionInfo":{"status":"ok","timestamp":1750152467693,"user_tz":-120,"elapsed":15,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# FUNCTION - Purify Columns by Deleting Disallowed Data Types\n","# Cleans specified columns by removing values incompatible with a target dtype and optionally writes a report.\n","# -------------------------------------------------------------------------------------------------------------\n","def purify_column_bool(\n","    df: pd.DataFrame,\n","    dtype: str,\n","    cols: list[str] = None,\n","    report_path: Optional[str] = None\n",") -> pd.DataFrame:\n","    \"\"\"\n","    Steps:\n","    1. Determine which columns to process, skipping 'facility', 'uid', and 'uniquekey'.\n","    2. Define helper to compare old vs new values safely, including NaNs.\n","    3. For each target column:\n","       a. If Boolean type, apply boolean conversion and track deletions.\n","       b. Otherwise, test each cell against the target type, delete invalid entries.\n","       c. After cleaning, cast the column to the desired pandas dtype.\n","       d. Record deletion counts and old‚Üínew mappings.\n","    4. Optionally write a detailed report if `report_path` is provided.\n","    5. Return the DataFrame with purified columns.\n","    \"\"\"\n","    # 1. Identify columns to purify\n","    skip_cols = {'facility', 'uid', 'uniquekey', 'startedatdischarge', 'completedatdischarge', 'ingestedatdischarge'}\n","    if not cols:\n","        cols = [c for c in df.columns if c.lower() not in skip_cols]\n","    else:\n","        cols = [c for c in cols if c.lower() not in skip_cols]\n","\n","    # 2. Helper: detect changes, including NaN differences\n","    def values_differ(old_val, new_val):\n","        if pd.isna(old_val) and pd.isna(new_val):\n","            return False\n","        if pd.isna(old_val) or pd.isna(new_val):\n","            return True\n","        return old_val != new_val\n","\n","    report_entries: list[str] = []  # Gather report lines\n","\n","    # 3. Process each column\n","    for col in cols:\n","        if col not in df.columns:\n","            raise KeyError(f\"Column '{col}' does not exist.\")\n","\n","        mapping_counts: dict[tuple[Any, Any], int] = {}\n","        deletion_counts: dict[Any, int] = {}\n","\n","        # 3a. Boolean conversion\n","        if dtype.lower() == 'boolean':\n","            def to_bool(x):\n","                if pd.isnull(x):\n","                    return pd.NA\n","                s = str(x).strip().lower()\n","                if s in {'yes','y','true','1','in'}:\n","                    return True\n","                if s in {'no','n','false','out'}:\n","                    return False\n","                return pd.NA\n","\n","            original = df[col].copy()\n","            df[col] = df[col].apply(to_bool).astype('boolean')\n","\n","            # Track changes\n","            for idx, new in df[col].items():\n","                old = original.loc[idx]\n","                if values_differ(old, new):\n","                    mapping_counts[(old,new)] = mapping_counts.get((old,new),0) + 1\n","                    if pd.isna(new) and pd.notna(old):\n","                        deletion_counts[old] = deletion_counts.get(old,0) + 1\n","\n","            # Report section\n","            total_deleted = sum(deletion_counts.values())\n","            report_entries.append(f\"Column: {col} (Boolean)\")\n","            report_entries.append(f\"Total deleted/converted values: {total_deleted}\")\n","            report_entries.append(\"Deleted values:\")\n","            if deletion_counts:\n","                for val, cnt in deletion_counts.items():\n","                    report_entries.append(f\"  {val}: {cnt}\")\n","            else:\n","                report_entries.append(\"  None\")\n","            report_entries.append(\"Old ‚Üí New mappings:\")\n","            if mapping_counts:\n","                for (o,n),cnt in mapping_counts.items():\n","                    report_entries.append(f\"  {o} ‚Üí {n}: {cnt}\")\n","            else:\n","                report_entries.append(\"  No changes\")\n","            report_entries.append(\"\")\n","\n","        # 3b. Other data types\n","        else:\n","            for idx, val in df[col].items():\n","                if pd.isnull(val):\n","                    continue\n","                orig = val\n","                valid = True\n","                if dtype.lower() == 'datetime':\n","                    try:\n","                        pd.to_datetime(val, errors='raise')\n","                    except:\n","                        valid = False\n","                elif dtype.lower() == 'numeric':\n","                    try:\n","                        pd.to_numeric(val, errors='raise')\n","                    except:\n","                        valid = False\n","                elif dtype.lower() == 'categorical':\n","                    if not isinstance(val, str):\n","                        valid = False\n","                else:\n","                    raise ValueError(\"Unsupported dtype. Use datetime, numeric, Boolean, or categorical.\")\n","\n","                if not valid:\n","                    deletion_counts[orig] = deletion_counts.get(orig,0) + 1\n","                    mapping_counts[(orig,pd.NA)] = mapping_counts.get((orig,pd.NA),0) + 1\n","                    df.at[idx, col] = pd.NA\n","\n","            # 3c. Cast to target dtype\n","            if dtype.lower() == 'numeric':\n","                df[col] = pd.to_numeric(df[col], errors='coerce')\n","            elif dtype.lower() == 'datetime':\n","                df[col] = pd.to_datetime(df[col], errors='coerce')\n","            elif dtype.lower() == 'categorical':\n","                df[col] = df[col].astype('category')\n","\n","            # Report section\n","            total_deleted = sum(deletion_counts.values())\n","            report_entries.append(f\"Column: {col}\")\n","            report_entries.append(f\"Total deleted values: {total_deleted}\")\n","            report_entries.append(\"Deleted values:\")\n","            if deletion_counts:\n","                for val, cnt in deletion_counts.items():\n","                    report_entries.append(f\"  {val}: {cnt}\")\n","            else:\n","                report_entries.append(\"  None\")\n","            report_entries.append(\"Old ‚Üí New mappings:\")\n","            if mapping_counts:\n","                for (o,n),cnt in mapping_counts.items():\n","                    report_entries.append(f\"  {o} ‚Üí {n}: {cnt}\")\n","            else:\n","                report_entries.append(\"  No changes\")\n","            report_entries.append(\"\")\n","\n","    # 4. Optional report writing\n","    if report_path:\n","        try:\n","            with open(report_path, 'w', encoding='utf-8') as f:\n","                f.write(\"\\n\".join(report_entries))\n","        except:\n","            pass  # Skip if unable to write\n","\n","    # 5. Return the purified DataFrame\n","    return df\n"],"id":"aff47c9e"},{"cell_type":"markdown","metadata":{"id":"72be55e8"},"source":["#### Categorical & Object Validation Functions"],"id":"72be55e8"},{"cell_type":"code","execution_count":33,"metadata":{"id":"32a98f5e","executionInfo":{"status":"ok","timestamp":1750152467777,"user_tz":-120,"elapsed":65,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# FUNCTION - Load and Aggregate Data Dictionary\n","# Reads an Excel file (multiple sheets) and builds a mapping:\n","# key (lowercase) -> {values, label_to_value, datatype}\n","# ------------------------------------------------------------------------------\n","def load_and_aggregate_data_dictionary2(\n","    dict_filepath: str\n",") -> Dict[str, Any]:\n","    \"\"\"\n","    Steps:\n","    1. Load the Excel workbook from the given filepath.\n","    2. Iterate through each sheet:\n","       a. Attempt to parse the sheet into a DataFrame; skip on errors.\n","       b. Ensure required columns ['Key','Label','Data Type','Value','Value Label'] are present; skip if not.\n","       c. Rename headers to lowercase keys: key, label, datatype, value, value_label.\n","       d. Normalize text fields: strip whitespace, unify casing.\n","       e. For each row, skip empty keys, then:\n","          - Initialize a new entry if key unseen, storing empty sets/mappings and datatype.\n","          - Check for datatype consistency on repeated keys.\n","          - Add raw 'value' to the values set if valid.\n","          - Map both 'value_label' and 'label' back to the raw 'value'.\n","    3. Log completion and return the aggregated dictionary.\n","    \"\"\"\n","    # Step 1: Load workbook\n","    try:\n","        xlsx = pd.ExcelFile(dict_filepath)  # Open the Excel file\n","    except Exception as e:\n","        logging.error(f\"Failed to load dictionary file: {e}\")\n","        raise  # Propagate failure\n","\n","    aggregated_dict: Dict[str, Any] = {}  # Final output container\n","\n","    # Step 2: Process each sheet\n","    for sheet in xlsx.sheet_names:\n","        try:\n","            df_sheet = xlsx.parse(sheet)  # Read sheet into DataFrame\n","        except Exception as e:\n","            logging.error(f\"Failed to parse sheet '{sheet}': {e}\")\n","            continue  # Skip this sheet on parse errors\n","\n","        # 2b: Verify presence of expected columns\n","        expected_cols = ['Key', 'Label', 'Data Type', 'Value', 'Value Label']\n","        df_sheet.columns = [col.strip() for col in df_sheet.columns]  # Trim header whitespace\n","        if not all(col in df_sheet.columns for col in expected_cols):\n","            logging.warning(f\"Sheet '{sheet}' missing expected columns. Skipping.\")\n","            continue\n","\n","        # 2c: Rename to consistent lowercase keys\n","        df_sheet = df_sheet.rename(columns={\n","            'Key': 'key',\n","            'Label': 'label',\n","            'Data Type': 'datatype',\n","            'Value': 'value',\n","            'Value Label': 'value_label'\n","        })\n","        # 2d: Normalize text fields\n","        df_sheet['key'] = df_sheet['key'].astype(str).str.strip().str.lower()\n","        df_sheet['value'] = df_sheet['value'].astype(str).str.strip()\n","        df_sheet['label'] = df_sheet['label'].astype(str).str.strip()\n","        df_sheet['value_label'] = df_sheet['value_label'].astype(str).str.strip()\n","        df_sheet['datatype'] = df_sheet['datatype'].astype(str).str.strip().str.lower()\n","\n","        # 2e: Populate aggregated_dict row by row\n","        for _, row in df_sheet.iterrows():\n","            key = row['key']  # Normalized dictionary key\n","            if pd.isna(key) or key == '':\n","                continue  # Skip empty or NaN keys\n","\n","            # Initialize new entry if unseen\n","            if key not in aggregated_dict:\n","                aggregated_dict[key] = {\n","                    'values': set(),\n","                    'label_to_value': {},\n","                    'datatype': row['datatype']\n","                }\n","            else:\n","                # Warn if datatype differs from first encountered\n","                if aggregated_dict[key]['datatype'] != row['datatype']:\n","                    logging.warning(\n","                        f\"Datatype inconsistency for key '{key}' in sheet '{sheet}'. \"\n","                        \"Using first encountered datatype.\"\n","                    )\n","\n","            # Add valid raw values\n","            if row['value'] not in [None, '', 'nan']:\n","                aggregated_dict[key]['values'].add(row['value'])\n","            # Map both 'value_label' and 'label' back to raw 'value'\n","            if row['value_label'] not in [None, '', 'nan']:\n","                aggregated_dict[key]['label_to_value'][row['value_label']] = row['value']\n","            if row['label'] not in [None, '', 'nan']:\n","                aggregated_dict[key]['label_to_value'][row['label']] = row['value']\n","\n","    logging.info(\"Completed aggregating data dictionary from Excel.\")  # Final log\n","    return aggregated_dict  # Return assembled mapping\n"],"id":"32a98f5e"},{"cell_type":"code","execution_count":34,"metadata":{"id":"0cac1fce","executionInfo":{"status":"ok","timestamp":1750152467781,"user_tz":-120,"elapsed":5,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# FUNCTION - Raise Flags for Disallowed Non-Numeric Values\n","# Checks non-numeric columns against allowed values specified in dict_mapping; flags values but does not modify DataFrame.\n","# ------------------------------------------------------------------------------\n","def raise_flags_non_numeric(\n","    df: pd.DataFrame,\n","    dict_mapping: dict[str, Any],\n","    report_file: Optional[str] = None,\n","    skip_columns: list[str] = ['uid', 'facility', 'uniquekey', \"startedatdischarge\", \"completedatdischarge\", \"ingestedatdischarge\"]\n",") -> pd.DataFrame:\n","    \"\"\"\n","    Steps:\n","    1. Initialize skip_columns if not provided.\n","    2. Lowercase the dictionary keys for case-insensitive lookup.\n","    3. Lowercase skip_columns for consistency.\n","    4. Prepare containers for report_info and skipped columns.\n","    5. Iterate over DataFrame columns:\n","       a. Skip columns containing 'uid', exactly 'facility' or 'uniquekey', or in skip_columns.\n","       b. Process only non-numeric columns.\n","       c. If column key exists in dict_mapping, retrieve allowed_values set.\n","       d. Define is_value_allowed(val) to check membership.\n","       e. Apply mask to flag disallowed entries.\n","       f. Count flagged rows and unique bad values, record if any.\n","    6. Build text report lines summarizing flags and skipped columns.\n","    7. Optionally write report to report_file, skipping on errors.\n","    8. Return original DataFrame unchanged.\n","    \"\"\"\n","    # 1. Default skip_columns to empty list\n","    if skip_columns is None:\n","        skip_columns = []\n","\n","    # 2. Create lowercase dict mapping\n","    aggregated_dict_lower = {k.lower(): v for k, v in dict_mapping.items()}\n","\n","    # 3. Lowercase skip_columns for matching\n","    skip_columns_lower = [c.lower() for c in skip_columns]\n","\n","    # 4. Report containers\n","    report_info: dict[str, dict[str, Any]] = {}\n","    skipped_columns: list[str] = []\n","\n","    # 5. Iterate through columns\n","    for col in df.columns:\n","        col_lower = col.lower()\n","        # 5a. Skip identifiers and user-specified columns\n","        if (\n","            'uid' in col_lower or\n","            col_lower in ['facility','uniquekey'] or\n","            col_lower in skip_columns_lower\n","        ):\n","            skipped_columns.append(col)\n","            continue\n","\n","        # 5b. Only non-numeric columns\n","        if not pd.api.types.is_numeric_dtype(df[col]):\n","            # 5c. Check if mapping exists\n","            if col_lower in aggregated_dict_lower:\n","                allowed_values = aggregated_dict_lower[col_lower]['values']\n","\n","                # 5d. Helper to test value validity\n","                def is_value_allowed(val: Any) -> bool:\n","                    if pd.isnull(val):\n","                        return True\n","                    if val in allowed_values:\n","                        return True\n","                    # Handle combination strings\n","                    if isinstance(val, str) and val.startswith('{') and val.endswith('}'):\n","                        parts = [item.strip() for item in val[1:-1].split(',')]\n","                        return all(part in allowed_values for part in parts)\n","                    return False\n","\n","                # 5e. Apply mask to identify disallowed\n","                mask = df[col].apply(lambda x: not is_value_allowed(x))  # True for disallowed\n","                num_flagged = int(mask.sum())\n","                bad_values = df[col][mask].dropna().unique().tolist()\n","\n","                # 5f. Record if flags present\n","                if num_flagged > 0:\n","                    report_info[col] = {\n","                        'rows_flagged': num_flagged,\n","                        'disallowed_values': bad_values\n","                    }\n","\n","    # 6. Build report text\n","    lines: list[str] = []\n","    lines.append('Disallowed Values Flags Report')\n","    lines.append('================================')\n","    lines.append('')\n","    if report_info:\n","        for col, info in report_info.items():\n","            vals = ', '.join(f'\"{v}\"' for v in info['disallowed_values'])\n","            lines.append(f'Column: \"{col}\"')\n","            lines.append(f'  Rows Flagged: {info[\"rows_flagged\"]}')\n","            lines.append(f'  Disallowed Values: [{vals}]')\n","            lines.append('')\n","    else:\n","        lines.append('No disallowed values found in any column.')\n","        lines.append('')\n","\n","    if skipped_columns:\n","        lines.append('Columns Skipped (UID, Facility, UniqueKey, or User-Specified):')\n","        lines.append('---------------------------------------------------------------')\n","        for c in skipped_columns:\n","            lines.append(f'  - {c}')\n","        lines.append('')\n","\n","    report_text = '\\n'.join(lines)\n","\n","    # 7. Optional report writing\n","    if report_file:\n","        try:\n","            with open(report_file, 'w', encoding='utf-8') as f:\n","                f.write(report_text)\n","        except Exception:\n","            pass  # Skip report on write error\n","\n","    # 8. Return unchanged DataFrame\n","    return df\n"],"id":"0cac1fce"},{"cell_type":"code","execution_count":35,"metadata":{"id":"a6426803","executionInfo":{"status":"ok","timestamp":1750152467838,"user_tz":-120,"elapsed":54,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# FUNCTION - Clean DataFrame with Dictionary\n","# Cleans and standardizes column values based on a reference dictionary, with optional report.\n","# ---------------------------------------------------------------------------------------------\n","def clean_dataframe_with_dictionary(\n","    df: pd.DataFrame,\n","    dict_input: pd.DataFrame | str,\n","    report_path: Optional[str] = None,\n","    cols_to_skip: list[str] = ['uid', 'facility', 'uniquekey', \"startedatdischarge\", \"completedatdischarge\", \"ingestedatdischarge\"]\n",") -> pd.DataFrame:\n","    \"\"\"\n","    Steps:\n","    1. Initialize cols_to_skip list.\n","    2. Load or concatenate the dictionary into a DataFrame.\n","    3. Build mappings for each column key:\n","       - val_to_label: raw value -> description\n","       - label_to_val: description -> raw value\n","    4. Prepare report tracking structures:\n","       - replacements_made, unknown_values, columns_without_mapping, special_values_not_replaced.\n","    5. Copy the original DataFrame to avoid in-place modification.\n","    6. For each column (skipping cols_to_skip):\n","       a. If no mapping, record and skip.\n","       b. For each unique non-null value:\n","          i.   If combination string (\"{...}\"), split into items.\n","          ii.  For each item:\n","               - If special token \"none\"/\"norm\"/\"normal\", record it.\n","               - Else map via val_to_label or label_to_val, or record as unknown.\n","          iii. Rebuild combination and replace all occurrences if changed.\n","          iv.  For single values, apply similar mapping logic.\n","       c. Remove unused categories for categorical columns.\n","    7. Final pass to clean up all categorical columns.\n","    8. Build report text in sections.\n","    9. If report_path provided, attempt to write report; skip silently on failure.\n","   10. Return the cleaned DataFrame.\n","    \"\"\"\n","    # 1. Default skip list\n","    if cols_to_skip is None:\n","        cols_to_skip = []\n","\n","    # 2. Load dictionary DataFrame\n","    if isinstance(dict_input, str) and os.path.exists(dict_input):\n","        xls = pd.ExcelFile(dict_input)\n","        dict_df_list = [pd.read_excel(xls, sheet_name=sheet) for sheet in xls.sheet_names]\n","        dict_df = pd.concat(dict_df_list, ignore_index=True)\n","    elif isinstance(dict_input, pd.DataFrame):\n","        dict_df = dict_input.copy()\n","    else:\n","        raise ValueError(\"dict_input must be a file path or pandas DataFrame.\")\n","\n","    # 3. Build column mappings\n","    from collections import defaultdict\n","    col_mappings: dict[str, dict[str, dict]] = defaultdict(lambda: {\"val_to_label\":{}, \"label_to_val\":{}})\n","    for _, row in dict_df.iterrows():\n","        if not all(k in row for k in [\"Key\",\"Value\",\"Value Label\"]):\n","            continue\n","        key = str(row[\"Key\"]).strip().lower()\n","        val = str(row[\"Value\"]).strip()\n","        label = str(row[\"Value Label\"]).strip()\n","        col_mappings[key][\"val_to_label\"][val] = label\n","        col_mappings[key][\"label_to_val\"][label] = val\n","\n","    # 4. Initialize report trackers\n","    replacements_made: dict[str, list[tuple[Any,Any]]] = defaultdict(list)\n","    unknown_values: dict[str, set] = defaultdict(set)\n","    columns_without_mapping: list[str] = []\n","    special_values_not_replaced: dict[str, dict[str,int]] = defaultdict(lambda: defaultdict(int))\n","\n","    # 5. Copy DataFrame\n","    cleaned_df = df.copy()\n","\n","    # 6. Process each column\n","    for col in cleaned_df.columns:\n","        if col in cols_to_skip:\n","            continue\n","        key = col.split('.')[0].strip().lower()\n","        mapping = col_mappings.get(key)\n","        # 6a. Skip if no mapping available\n","        if not mapping or (not mapping[\"val_to_label\"] and not mapping[\"label_to_val\"]):\n","            columns_without_mapping.append(col)\n","            continue\n","\n","        val_to_label = mapping[\"val_to_label\"]\n","        label_to_val = mapping[\"label_to_val\"]\n","        unique_vals = cleaned_df[col].dropna().unique()\n","\n","        # 6b. Iterate unique values\n","        for orig in unique_vals:\n","            s = str(orig).strip()\n","            # 6b.i: Handle combination\n","            if s.startswith(\"{\") and s.endswith(\"}\"):\n","                items = [x.strip().strip(\"'\\\"\") for x in s[1:-1].split(',')]\n","                new_items = []\n","                for item in items:\n","                    low = item.lower()\n","                    if low in (\"none\",\"norm\",\"normal\"):\n","                        special_values_not_replaced[col][low] += 1\n","                        new_items.append(item)\n","                    elif item in val_to_label:\n","                        new_items.append(item)\n","                    elif item in label_to_val:\n","                        new_items.append(label_to_val[item])\n","                    else:\n","                        new_items.append(item)\n","                        unknown_values[col].add(item)\n","                new_s = \"{\" + \",\".join(new_items) + \"}\"\n","                if new_s != s:\n","                    cleaned_df[col].replace(orig, new_s, inplace=True)\n","                    replacements_made[col].append((orig, new_s))\n","\n","            else:\n","                # 6b.iv: Single value\n","                low = s.lower()\n","                if low in (\"none\",\"norm\",\"normal\"):\n","                    special_values_not_replaced[col][low] += 1\n","                elif s in val_to_label:\n","                    continue\n","                elif s in label_to_val:\n","                    new_val = label_to_val[s]\n","                    cleaned_df[col].replace(orig, new_val, inplace=True)\n","                    replacements_made[col].append((orig, new_val))\n","                else:\n","                    unknown_values[col].add(s)\n","\n","        # 6c: Cleanup categories\n","        if pd.api.types.is_categorical_dtype(cleaned_df[col]):\n","            cleaned_df[col] = cleaned_df[col].cat.remove_unused_categories()\n","\n","    # 7. Final cleanup for all categorical\n","    for col in cleaned_df.columns:\n","        if pd.api.types.is_categorical_dtype(cleaned_df[col]):\n","            cleaned_df[col] = cleaned_df[col].cat.remove_unused_categories()\n","\n","    # 8. Build report sections\n","    lines: list[str] = []\n","    # Section 1\n","    lines.append(\"Section 1 - Replacements\")\n","    if not replacements_made:\n","        lines.append(\"  (No replacements made.)\")\n","    else:\n","        for c, repls in replacements_made.items():\n","            lines.append(f\"Column '{c}':\")\n","            seen = set()\n","            for o,n in repls:\n","                if (o,n) not in seen:\n","                    lines.append(f\"  - {o} -> {n}\")\n","                    seen.add((o,n))\n","            lines.append(\"\")\n","    # Section 2\n","    lines.append(\"Section 2 - Unknown Values\")\n","    if not unknown_values:\n","        lines.append(\"  (No unknown values.)\")\n","    else:\n","        for c, vals in unknown_values.items():\n","            lines.append(f\"Column '{c}': {len(vals)} unknown values: {list(vals)}\")\n","    # Section 3\n","    lines.append(\"Section 3 - No Mapping Available\")\n","    if not columns_without_mapping:\n","        lines.append(\"  (All columns had mappings.)\")\n","    else:\n","        for c in columns_without_mapping:\n","            lines.append(f\"  - {c}\")\n","    # Section 4\n","    lines.append(\"Section 4 - Special Values Not Replaced\")\n","    if not special_values_not_replaced:\n","        lines.append(\"  (None encountered.)\")\n","    else:\n","        for c, d in special_values_not_replaced.items():\n","            lines.append(f\"Column '{c}': {d}\")\n","\n","    report_text = '\\n'.join(lines)\n","\n","    # 9. Optional report writing\n","    if report_path:\n","        try:\n","            with open(report_path, 'w', encoding='utf-8') as f:\n","                f.write(report_text)\n","        except Exception:\n","            pass\n","\n","    # 10. Return cleaned DataFrame\n","    return cleaned_df\n"],"id":"a6426803"},{"cell_type":"code","execution_count":36,"metadata":{"id":"632f7d05","executionInfo":{"status":"ok","timestamp":1750152467845,"user_tz":-120,"elapsed":5,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# FUNCTION - Align Datasets\n","# Compares two country-specific dictionaries (ZIM vs. MWI) and optionally generates an alignment report.\n","# ------------------------------------------------------------------------------\n","def align_datasets(\n","    dict_input_zim: pd.DataFrame | str,\n","    dict_input_mwi: pd.DataFrame | str,\n","    report_path: Optional[str] = None\n",") -> None:\n","    \"\"\"\n","    Steps:\n","    1. Build value mappings for each country:\n","       a. If `dict_input` is a filepath, read and concatenate all sheets into one DataFrame.\n","       b. Else if it's a DataFrame, copy it.\n","       c. Assemble mapping: key (lowercased) -> set of values.\n","    2. Identify common keys between ZIM and MWI mappings.\n","    3. For each common key where the allowed-value sets differ:\n","       a. Compute three sorted lists: values in both, only in ZIM, only in MWI.\n","       b. Append formatted lines to `report_lines`.\n","    4. If `report_path` is provided, attempt to write the report; skip silently on failure.\n","    \"\"\"\n","    # 1. Helper to build mapping\n","    def build_mapping(input_data: pd.DataFrame | str) -> dict[str, set[str]]:\n","        # a. Load from Excel if path\n","        if isinstance(input_data, str) and os.path.exists(input_data):\n","            xls = pd.ExcelFile(input_data)\n","            df_list = [pd.read_excel(xls, sheet_name=sheet) for sheet in xls.sheet_names]\n","            dict_df = pd.concat(df_list, ignore_index=True)\n","        # b. Copy if DataFrame\n","        elif isinstance(input_data, pd.DataFrame):\n","            dict_df = input_data.copy()\n","        else:\n","            raise ValueError(\"dict_input must be a file path or pandas DataFrame.\")\n","\n","        mapping: dict[str, set[str]] = {}\n","        # c. Assemble mapping\n","        for _, row in dict_df.iterrows():\n","            if \"Key\" not in row or \"Value\" not in row:\n","                continue\n","            key = str(row[\"Key\"]).strip().lower()\n","            value = str(row[\"Value\"]).strip()\n","            mapping.setdefault(key, set()).add(value)\n","        return mapping\n","\n","    # Build mappings for both countries\n","    mapping_zim = build_mapping(dict_input_zim)\n","    mapping_mwi = build_mapping(dict_input_mwi)\n","\n","    # 2. Find common keys\n","    common_keys = set(mapping_zim.keys()) & set(mapping_mwi.keys())\n","\n","    report_lines: list[str] = [\"Alignment Report\"]\n","    # 3. Compare allowed values\n","    for key in sorted(common_keys):\n","        zim_vals = mapping_zim[key]\n","        mwi_vals = mapping_mwi[key]\n","        # Skip identical sets\n","        if zim_vals == mwi_vals:\n","            continue\n","        both = sorted(zim_vals & mwi_vals, key=str.lower)\n","        only_zim = sorted(zim_vals - mwi_vals, key=str.lower)\n","        only_mwi = sorted(mwi_vals - zim_vals, key=str.lower)\n","\n","        report_lines.append(f\"Column: {key}\")\n","        report_lines.append(f\"  Found in both: {both}\")\n","        report_lines.append(f\"  Found in ZIM only: {only_zim}\")\n","        report_lines.append(f\"  Found in MWI only: {only_mwi}\")\n","        report_lines.append(\"\")  # Blank line\n","\n","    # 4. Optional report writing\n","    if report_path:\n","        try:\n","            with open(report_path, 'w', encoding='utf-8') as f:\n","                f.write(\"\\n\".join(report_lines))\n","        except Exception:\n","            pass  # Skip on write failure\n"],"id":"632f7d05"},{"cell_type":"code","execution_count":37,"metadata":{"id":"6a53663b","executionInfo":{"status":"ok","timestamp":1750152467846,"user_tz":-120,"elapsed":3,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# FUNCTION - Remove Disallowed Values with Optional Report\n","# Replaces specified disallowed values with NaN and optionally writes a detailed report.\n","# ------------------------------------------------------------------------------\n","def remove_disallowed_values_with_report(\n","    values_to_delete: dict[str, Collection[Any]] | list[tuple[Collection[str], Collection[Any]]],\n","    df: pd.DataFrame,\n","    report_filepath: Optional[str] = None\n",") -> pd.DataFrame:\n","    \"\"\"\n","    Steps:\n","    1. Normalize `values_to_delete` into `col_to_disallowed`: column -> set of disallowed values.\n","    2. Record original value counts for each mapped column; track missing columns.\n","    3. Replace occurrences of each disallowed value with NaN in-place.\n","    4. Build report lines detailing skipped columns and counts of deleted values per column.\n","    5. If `report_filepath` is provided, attempt to write the report; skip silently on failure.\n","    6. Return the modified DataFrame.\n","    \"\"\"\n","    # 1. Normalize input to column->disallowed set\n","    col_to_disallowed: dict[str, set[Any]] = {}\n","    if isinstance(values_to_delete, dict):\n","        items = values_to_delete.items()\n","    elif isinstance(values_to_delete, list):\n","        items = values_to_delete\n","    else:\n","        raise ValueError(\"values_to_delete must be a dict or list of pairs.\")\n","\n","    for key, disallowed in items:\n","        # If key is a list/tuple of columns, apply same disallowed set to each\n","        if isinstance(key, (list, tuple)):\n","            for col in key:\n","                col_to_disallowed.setdefault(col, set()).update(disallowed)\n","        else:\n","            col_to_disallowed.setdefault(key, set()).update(disallowed)\n","\n","    # 2. Track processing and original counts\n","    processed_columns: list[str] = []\n","    skipped_columns: list[str] = []\n","    original_counts: dict[str, dict[Any,int]] = {}\n","    for col, dis_set in col_to_disallowed.items():\n","        if col in df.columns:\n","            processed_columns.append(col)\n","            original_counts[col] = df[col].value_counts(dropna=False).to_dict()\n","        else:\n","            skipped_columns.append(col)\n","\n","    # 3. Replace disallowed values with NaN\n","    for col in processed_columns:\n","        dis_list = list(col_to_disallowed[col])\n","        df.loc[df[col].isin(dis_list), col] = np.nan\n","\n","    # 4. Build report lines\n","    lines: list[str] = []\n","    lines.append(\"Skipped columns:\")\n","    if skipped_columns:\n","        for col in skipped_columns:\n","            lines.append(f\"  - {col}\")\n","    else:\n","        lines.append(\"  None\")\n","\n","    lines.append(\"\\nDeleted Values:\")\n","    for col in sorted(processed_columns):\n","        orig = original_counts.get(col, {})\n","        final = df[col].value_counts(dropna=False).to_dict()\n","\n","        lines.append(f\"Column: {col}\")\n","        lines.append(\"  Deleted Values:\")\n","        deleted_any = False\n","        for val in sorted(col_to_disallowed[col], key=lambda x: str(x)):\n","            before = orig.get(val, 0)\n","            after = final.get(val, 0)\n","            delta = before - after\n","            if delta > 0:\n","                lines.append(f\"    {val!r}: {delta}\")\n","                deleted_any = True\n","        if not deleted_any:\n","            lines.append(\"    None\")\n","        lines.append(\"\")  # blank line\n","\n","    report_text = \"\\n\".join(lines)\n","\n","    # 5. Optional report writing\n","    if report_filepath:\n","        try:\n","            with open(report_filepath, 'w', encoding='utf-8') as f:\n","                f.write(report_text)\n","        except Exception:\n","            pass  # skip on write error\n","\n","    # 6. Return the DataFrame\n","    return df"],"id":"6a53663b"},{"cell_type":"code","execution_count":38,"metadata":{"id":"3ee518e1","executionInfo":{"status":"ok","timestamp":1750152467863,"user_tz":-120,"elapsed":16,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# FUNCTION - Apply Value Mappings with Optional Report\n","# Replaces specified old values with new values (including within combinations) and optionally writes a report.\n","# ------------------------------------------------------------------------------\n","def apply_value_mappings_with_report(\n","    df: pd.DataFrame,\n","    mappings: dict[str, dict[Any, list[Any]]],\n","    report_path: Optional[str] = None\n",") -> pd.DataFrame:\n","    \"\"\"\n","    Steps:\n","    1. Initialize an empty list `report_lines` to accumulate report entries.\n","    2. Define helper `count_occurrences_in_cell` to count occurrences of a target value,\n","       handling both single entries and comma-separated combinations (with or without braces).\n","    3. Iterate through each column and its mapping in `mappings`:\n","       a. Skip if the column does not exist in the DataFrame.\n","       b. Build `rep_dict` mapping each old_val -> new_val for fast lookup.\n","       c. Count how many replacements will occur for each old_val by summing occurrences.\n","       d. Record lines for values with non-zero replacement counts.\n","       e. Define `map_cell` to apply replacements in each cell:\n","          - Direct replace if exactly matches old_val.\n","          - For comma-separated strings, replace within tokens and rebuild with braces if needed.\n","       f. Apply `map_cell` to the entire column in-place.\n","       g. Append the recorded lines for this column to `report_lines` if any.\n","    4. Join `report_lines` into a single string `report_text`.\n","    5. If `report_path` is provided, attempt to write `report_text` to file; skip on any error.\n","    6. Return the modified DataFrame.\n","    \"\"\"\n","    # 1. Prepare report container\n","    report_lines: list[str] = []\n","\n","    # 2. Helper to count occurrences of a target in a cell\n","    def count_occurrences_in_cell(cell: Any, target: Any) -> int:\n","        cell_str = str(cell)\n","        if cell_str == str(target):\n","            return 1\n","        if \",\" in cell_str:\n","            has_braces = cell_str.startswith(\"{\") and cell_str.endswith(\"}\")\n","            inner = cell_str[1:-1] if has_braces else cell_str\n","            tokens = [tok.strip() for tok in inner.split(\",\")]\n","            return tokens.count(str(target))\n","        return 0\n","\n","    # 3. Iterate mappings\n","    for column, col_mapping in mappings.items():\n","        if column not in df.columns:\n","            continue  # 3a. Skip missing columns\n","\n","        # 3b. Build flat old->new dictionary\n","        rep_dict: dict[str, Any] = {}\n","        for new_val, old_list in col_mapping.items():\n","            for old_val in old_list:\n","                rep_dict[str(old_val)] = new_val\n","\n","        # 3c. Count how many replacements for each old_val\n","        column_report: list[str] = []\n","        for old_str, new_val in rep_dict.items():\n","            count_replaced = sum(\n","                count_occurrences_in_cell(cell, old_str) for cell in df[column]\n","            )\n","            if count_replaced > 0:\n","                column_report.append(f\"  {old_str} -> {new_val}: {count_replaced}\")\n","\n","        # 3e. Define mapping function for a single cell\n","        def map_cell(cell: Any) -> Any:\n","            cell_str = str(cell)\n","            # Direct replacement\n","            if cell_str in rep_dict:\n","                return rep_dict[cell_str]\n","            # Handle comma-separated values\n","            if \",\" in cell_str:\n","                has_braces = cell_str.startswith(\"{\") and cell_str.endswith(\"}\")\n","                inner = cell_str[1:-1] if has_braces else cell_str\n","                tokens = [tok.strip() for tok in inner.split(\",\")]\n","                mapped = [rep_dict.get(tok, tok) for tok in tokens]\n","                new_inner = \", \".join(mapped)\n","                return f\"{{{new_inner}}}\" if has_braces else new_inner\n","            return cell\n","\n","        # 3f. Apply mapping to column\n","        df[column] = df[column].apply(map_cell)\n","\n","        # 3g. Append column report if entries exist\n","        if column_report:\n","            report_lines.append(f\"Column: {column}\")\n","            report_lines.extend(column_report)\n","            report_lines.append(\"\")\n","\n","    # 4. Build final report text\n","    report_text = \"\\n\".join(report_lines)\n","\n","    # 5. Optional report writing\n","    if report_path:\n","        try:\n","            with open(report_path, 'w', encoding='utf-8') as f:\n","                f.write(report_text)\n","        except Exception:\n","            pass  # Skip on write error\n","\n","    # 6. Return updated DataFrame\n","    return df"],"id":"3ee518e1"},{"cell_type":"code","execution_count":39,"metadata":{"id":"62892c55","executionInfo":{"status":"ok","timestamp":1750152467992,"user_tz":-120,"elapsed":3,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# FUNCTION - Replace Unallowed with \"OTHER\"\n","# Replaces values not in the allowed set with \"OTHER\" and optionally writes a report.\n","# ------------------------------------------------------------------------------\n","def replace_unallowed_with_other(\n","    df: pd.DataFrame,\n","    allowed_values_by_col: dict[str, Collection[Any]],\n","    report_path: Optional[str] = None\n",") -> pd.DataFrame:\n","    \"\"\"\n","    Steps:\n","    1. Initialize an empty list to accumulate report lines.\n","    2. For each column in allowed_values_by_col:\n","       a. Skip if the column doesn't exist in df.\n","       b. Convert allowed_values to a set for fast lookup.\n","       c. Initialize a counter for total replacements.\n","       d. Define process_cell to:\n","          - Return the original cell if it's in allowed_set.\n","          - Otherwise increment the counter and return \"OTHER\".\n","       e. Apply process_cell to the entire column.\n","       f. If replacements occurred, append report entries for this column.\n","    3. If report_path is provided, attempt to write the report; silently skip on failure.\n","    4. Return the modified DataFrame.\n","    \"\"\"\n","    # 1. Prepare report container\n","    report_lines: list[str] = []\n","\n","    # 2. Iterate over each mapping\n","    for col, allowed_values in allowed_values_by_col.items():\n","        # 2a. Skip missing columns\n","        if col not in df.columns:\n","            continue\n","        # 2b. Build a set of allowed values\n","        allowed_set = set(allowed_values)\n","        # 2c. Counter for replacements in this column\n","        total_replacements = 0\n","\n","        # 2d. Cell processing function\n","        def process_cell(cell: Any) -> Any:\n","            nonlocal total_replacements\n","            # Keep allowed values\n","            if cell in allowed_set:\n","                return cell\n","            # Replace disallowed with \"OTHER\"\n","            total_replacements += 1\n","            return \"OTHER\"\n","\n","        # 2e. Apply to column\n","        df[col] = df[col].apply(process_cell)\n","\n","        # 2f. Record in report if replacements happened\n","        if total_replacements > 0:\n","            report_lines.append(f\"Column: {col}\")\n","            report_lines.append(f\"  TOTAL REPLACED: {total_replacements}\")\n","            report_lines.append(\"\")\n","\n","    # 3. Optional report writing\n","    if report_path:\n","        try:\n","            with open(report_path, 'w', encoding='utf-8') as f:\n","                f.write(\"\\n\".join(report_lines))\n","        except Exception:\n","            pass  # Skip on write errors\n","\n","    # 4. Return DataFrame\n","    return df"],"id":"62892c55"},{"cell_type":"markdown","metadata":{"id":"a153df45"},"source":["#### Datetime Validation Functions"],"id":"a153df45"},{"cell_type":"code","execution_count":40,"metadata":{"id":"4745320e","executionInfo":{"status":"ok","timestamp":1750152468031,"user_tz":-120,"elapsed":19,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# FUNCTION - Purify Columns by Data Type with Optional Report\n","# Removes values that cannot be cast to the specified dtype and optionally writes a report.\n","# ------------------------------------------------------------------------------\n","def purify_column_dt(\n","    df: pd.DataFrame,\n","    dtype: str,\n","    cols: list[str] | None = None,\n","    report_path: str | None = None\n",") -> pd.DataFrame:\n","    \"\"\"\n","    Steps:\n","    1. Determine which columns to process (exclude 'facility','uid','uniquekey').\n","    2. For each column:\n","       a. If targeting Boolean, apply a safe conversion function and record changes.\n","       b. Else, validate each non-null cell against the target dtype and delete invalid values.\n","       c. Track counts of deletions and old->new mappings.\n","    3. Compile report lines summarizing per-column deletions and mappings.\n","    4. If `report_path` is provided, attempt to write the report; silently skip on error.\n","    5. Return the modified DataFrame.\n","    \"\"\"\n","    import pandas as pd\n","    import numpy as np\n","\n","    # 1. Identify columns to process (skip always-facility/uid/uniquekey)\n","    skip_cols = {\"facility\", \"uid\", \"uniquekey\", \"startedatdischarge\", \"completedatdischarge\", \"ingestedatdischarge\"}\n","    if not cols:\n","        target_cols = [c for c in df.columns if c.lower() not in skip_cols]\n","    else:\n","        target_cols = [c for c in cols if c in df.columns and c.lower() not in skip_cols]\n","\n","    # Helper to compare old vs new values safely\n","    def values_differ(old, new):\n","        if pd.isna(old) and pd.isna(new):\n","            return False\n","        if pd.isna(old) or pd.isna(new):\n","            return True\n","        return old != new\n","\n","    report_lines: list[str] = []\n","\n","    # 2. Process each target column\n","    for col in target_cols:\n","        mapping_counts: dict[tuple[Any,Any], int] = {}\n","        deleted_counts: dict[Any,int] = {}\n","\n","        # 2a. Boolean conversion\n","        if dtype.lower() == \"boolean\":\n","            # Safe converter: yes/y/true -> True; no/n/false -> False; else pd.NA\n","            def to_bool(x):\n","                if pd.isnull(x):\n","                    return pd.NA\n","                s = str(x).strip().lower()\n","                if s in {\"yes\",\"y\",\"true\"}:\n","                    return True\n","                if s in {\"no\",\"n\",\"false\"}:\n","                    return False\n","                return pd.NA\n","\n","            original = df[col].copy()\n","            # Apply and cast to pandas BooleanDtype\n","            df[col] = df[col].apply(to_bool).astype('boolean')\n","\n","            # Count conversions & deletions\n","            for idx, new_val in df[col].items():\n","                old_val = original.iat[idx]\n","                if values_differ(old_val, new_val):\n","                    mapping_counts[(old_val, new_val)] = mapping_counts.get((old_val, new_val), 0) + 1\n","                    if pd.isna(new_val) and pd.notna(old_val):\n","                        deleted_counts[old_val] = deleted_counts.get(old_val, 0) + 1\n","\n","            total_deleted = sum(deleted_counts.values())\n","            report_lines.append(f\"Column: {col} (Boolean)\")\n","            report_lines.append(f\"  Total deleted/converted: {total_deleted}\")\n","\n","        # 2b. Other types: datetime, numeric, categorical\n","        else:\n","            for idx, val in df[col].items():\n","                if pd.isnull(val):\n","                    continue\n","                original_val = val\n","                is_valid = True\n","                # Validate by dtype\n","                if dtype.lower() == \"datetime\":\n","                    try:\n","                        pd.to_datetime(val, errors='raise')\n","                    except Exception:\n","                        is_valid = False\n","                elif dtype.lower() == \"numeric\":\n","                    try:\n","                        pd.to_numeric(val, errors='raise')\n","                    except Exception:\n","                        is_valid = False\n","                elif dtype.lower() == \"categorical\":\n","                    is_valid = isinstance(val, str)\n","                else:\n","                    raise ValueError(f\"Unsupported dtype '{dtype}'\")\n","\n","                # Delete invalid\n","                if not is_valid:\n","                    df.at[idx, col] = pd.NA\n","                    deleted_counts[original_val] = deleted_counts.get(original_val, 0) + 1\n","                    mapping_counts[(original_val, pd.NA)] = mapping_counts.get((original_val, pd.NA), 0) + 1\n","\n","            total_deleted = sum(deleted_counts.values())\n","            report_lines.append(f\"Column: {col} ({dtype})\")\n","            report_lines.append(f\"  Total deleted: {total_deleted}\")\n","\n","        # 2c. Append detailed counts\n","        if deleted_counts:\n","            report_lines.append(\"Deleted Values and Counts:\")\n","            for val, cnt in deleted_counts.items():\n","                report_lines.append(f\"  {val!r}: {cnt}\")\n","        else:\n","            report_lines.append(\"  None deleted.\")\n","\n","        report_lines.append(\"Old->New mappings:\")\n","        if mapping_counts:\n","            for (old, new), cnt in mapping_counts.items():\n","                report_lines.append(f\"  {old!r} -> {new!r}: {cnt}\")\n","        else:\n","            report_lines.append(\"  No changes.\")\n","        report_lines.append(\"\")  # blank line\n","\n","    # 3. Compile report text\n","    full_report = \"\\n\".join(report_lines)\n","\n","    # 4. Optional: write report\n","    if report_path:\n","        try:\n","            with open(report_path, 'w', encoding='utf-8') as f:\n","                f.write(full_report)\n","        except Exception:\n","            pass  # skip on write failure\n","\n","    # 5. Return the purified DataFrame\n","    return df"],"id":"4745320e"},{"cell_type":"markdown","metadata":{"id":"2837b7af"},"source":["#### Merging Functions"],"id":"2837b7af"},{"cell_type":"code","execution_count":41,"metadata":{"id":"b842e219","executionInfo":{"status":"ok","timestamp":1750152468089,"user_tz":-120,"elapsed":56,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# FUNCTION - Merge Validated DataFrames\n","# -----------------------------------------------\n","\n","def merge_validated_data(\n","    df_numeric: pd.DataFrame,\n","    df_boolean: pd.DataFrame,\n","    df_category: pd.DataFrame,\n","    df_object: pd.DataFrame,\n","    df_datetime: pd.DataFrame,\n","    on: list, #= [\"facility\", \"uid\", \"uniquekey\", \"startedatdischarge\", \"completedatdischarge\", \"ingestedatdischarge\", \"datetimedischarge.value\"],\n","    how: str = \"inner\",\n","    csv_path: str = None,\n","    pkl_path: str = None\n",") -> pd.DataFrame:\n","    \"\"\"\n","    Sequentially merge five DataFrames of different types into a single DataFrame.\n","\n","    Parameters\n","    ----------\n","    df_numeric : pd.DataFrame\n","        First DataFrame (numeric).\n","    df_boolean : pd.DataFrame\n","        Second DataFrame (boolean).\n","    df_category : pd.DataFrame\n","        Third DataFrame (categorical).\n","    df_object : pd.DataFrame\n","        Fourth DataFrame (object/string).\n","    df_datetime : pd.DataFrame\n","        Fifth DataFrame (datetime).\n","    on : list of str, default [\"facility\",\"uid\",\"uniquekey\"]\n","        Column names to join on for each merge.\n","    how : str, default \"inner\"\n","        Type of merge to perform (\"inner\", \"left\", \"outer\", etc.).\n","    csv_path : str, optional\n","        If provided, path to save the merged DataFrame as CSV.\n","    pkl_path : str, optional\n","        If provided, path to save the merged DataFrame as a pickle.\n","\n","    Returns\n","    -------\n","    pd.DataFrame\n","        The fully merged DataFrame.\n","    \"\"\"\n","    # Merge numeric + boolean\n","    df_merged = pd.merge(df_numeric, df_boolean, on=on, how=how)\n","\n","    # Add category\n","    df_merged = pd.merge(df_merged, df_category, on=on, how=how)\n","\n","    # Add object\n","    df_merged = pd.merge(df_merged, df_object, on=on, how=how)\n","\n","    # Add datetime\n","    df_merged = pd.merge(df_merged, df_datetime, on=on, how=how)\n","\n","    # Optionally write out\n","    if csv_path:\n","        df_merged.to_csv(csv_path, index=False)\n","    if pkl_path:\n","        df_merged.to_pickle(pkl_path)\n","\n","    return df_merged\n"],"id":"b842e219"},{"cell_type":"markdown","metadata":{"id":"3a3e0420"},"source":["###\n","---"],"id":"3a3e0420"},{"cell_type":"markdown","metadata":{"id":"e7bed21b"},"source":["### CLEANING"],"id":"e7bed21b"},{"cell_type":"markdown","metadata":{"id":"861d73df"},"source":["#### 1st Stage Cleaning"],"id":"861d73df"},{"cell_type":"code","execution_count":null,"metadata":{"id":"c4d04313"},"outputs":[],"source":["# LOAD DATA - CSVs & DICTIONARIES\n","\n","# Read CSV with proper NA handling. This ensures that empty strings, \"nan\", \"NA\", etc. are treated as NaN.\n","df = pd.read_csv(csv_filepath, delimiter=\",\")\n","logging.info(f\"Loaded CSV with shape: {df.shape}\")\n","\n","# Load and aggregate data dictionaries.\n","dict_mapping = load_and_aggregate_dictionary(cfg[\"dict_filepath\"])\n","logging.info(f\"Loaded dictionary with keys: {list(dict_mapping.keys())[:10]}\")\n","\n","# Generate a pre-processing report.\n","get_report(df, None)\n","logging.info(\"Pre-processing report generated.\")\n","\n","# Optional Reports"],"id":"c4d04313"},{"cell_type":"code","execution_count":null,"metadata":{"id":"de1ebfb4","executionInfo":{"status":"aborted","timestamp":1750152468393,"user_tz":-120,"elapsed":67,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# Remove frame shifted rows\n","df_removed_frame_shift = remove_frame_shift(df, None, None, None)\n","\n","# Optional Reports"],"id":"de1ebfb4"},{"cell_type":"code","execution_count":null,"metadata":{"id":"194f4efc","executionInfo":{"status":"aborted","timestamp":1750152468396,"user_tz":-120,"elapsed":3375,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# Clean column headers.\n","df_clean_cols = clean_columns(df_removed_frame_shift)\n"],"id":"194f4efc"},{"cell_type":"code","execution_count":null,"metadata":{"id":"a6b0e6e2","executionInfo":{"status":"aborted","timestamp":1750152468444,"user_tz":-120,"elapsed":3423,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# Merge duplicate columns.\n","df_merge_cols = merge_duplicate_columns(df_clean_cols)"],"id":"a6b0e6e2"},{"cell_type":"code","execution_count":null,"metadata":{"id":"b2186b0b","executionInfo":{"status":"aborted","timestamp":1750152468446,"user_tz":-120,"elapsed":3425,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# Clean impure columns using dictionary mappings.\n","df_pure_values = clean_columns_using_base_name(df_merge_cols, dict_mapping, None)\n","logging.info(\"Column cleaning report generated.\")\n","\n","# Optional Reports\n","# column_cleaning_report_filepath_zim"],"id":"b2186b0b"},{"cell_type":"code","execution_count":null,"metadata":{"id":"f9138360","executionInfo":{"status":"aborted","timestamp":1750152468448,"user_tz":-120,"elapsed":3426,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["## FOWARD FILL NONES\n","df_forward_fill_nones = fill_missing_with_label_value(df_pure_values, None)\n","\n","# Optional Reports\n","# forward_fill_nones_report_zim"],"id":"f9138360"},{"cell_type":"code","execution_count":null,"metadata":{"id":"ef09dd5e","executionInfo":{"status":"aborted","timestamp":1750152468449,"user_tz":-120,"elapsed":3427,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# Forward fill missing .value columns using corresponding .label columns.\n","df_forward_fill = forward_fill_numeric_datetime(df_forward_fill_nones, None)\n","logging.info(\"Forward fill complete.\")\n","\n","# Optional Reports\n","# forward_fill_report_path_zim"],"id":"ef09dd5e"},{"cell_type":"code","execution_count":null,"metadata":{"id":"4f73b8c3","executionInfo":{"status":"aborted","timestamp":1750152468452,"user_tz":-120,"elapsed":3430,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# drop autopopulating/repeat columns\n","df_dropped_autopops = drop_unwanted_columns(df_forward_fill, None )"],"id":"4f73b8c3"},{"cell_type":"code","execution_count":null,"metadata":{"id":"1b3dd966","executionInfo":{"status":"aborted","timestamp":1750152468454,"user_tz":-120,"elapsed":3432,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# Convert columns to correct datatypes using the new combined function.\n","df_dtypes_fixed = convert_columns_and_create_report(df_dropped_autopops,dict_mapping, report_path= None)\n","logging.info(\"Data type conversion complete.\")\n","\n","# Optional Report\n","# dtype_conversion_report_filepath_zim"],"id":"1b3dd966"},{"cell_type":"code","execution_count":null,"metadata":{"id":"44e8a5fd","executionInfo":{"status":"aborted","timestamp":1750152468455,"user_tz":-120,"elapsed":3428,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# Final post-processing report.\n","get_report(df_dtypes_fixed, None)\n","logging.info(\"Post-processing report generated.\")\n","\n","# Optional Report\n","# post_process_report_filepath_zim"],"id":"44e8a5fd"},{"cell_type":"code","execution_count":null,"metadata":{"id":"52265663","executionInfo":{"status":"aborted","timestamp":1750152468456,"user_tz":-120,"elapsed":3428,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# Aggregate and save the column datatypes & descriptions.\n","summary_df = map_df_columns_to_descriptions_and_datatypes(df_dtypes_fixed, cfg[\"dict_filepath\"], None, None)\n","\n","# Optional Reports\n","# output_csv_filepath_zim, output_excel_filepath_zim"],"id":"52265663"},{"cell_type":"code","execution_count":null,"metadata":{"id":"4e87f314","executionInfo":{"status":"aborted","timestamp":1750152468456,"user_tz":-120,"elapsed":3422,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# The merge columns\n","on_cols = [\"facility\", \"uid\", \"uniquekey\", \"startedatdischarge\", \"completedatdischarge\", \"ingestedatdischarge\"]\n","\n","\n","# Calculate the number of missing values for each row\n","# create a temporary column to store this count\n","df_dtypes_fixed['missing_count'] = df_dtypes_fixed.isnull().sum(axis=1)\n","\n","# Sort the DataFrame. This is crucial.\n","# First, sort by your 'on_cols' to group duplicates together.\n","# Second, sort by 'missing_count' in ascending order (fewer missing values first).\n","# Third, the default stable sort will keep the original order for rows with the same missing_count.\n","df_sorted_for_deduplication = df_dtypes_fixed.sort_values(\n","    by=on_cols + ['missing_count'],\n","    ascending=True # Sort missing_count ascending so fewer missing values come first\n",")\n","\n","#Drop duplicates based on 'on_cols', keeping the 'first' one.\n","# Because of the previous sort, 'first' will now correspond to:\n","# 1. The record with the fewest missing values among the duplicates.\n","# 2. If missing values are tied, the one that appeared first in the original (or input to sort_values) order.\n","df_deduplicated_by_missingness = df_sorted_for_deduplication.drop_duplicates(\n","    subset=on_cols,\n","    keep='first'\n",")\n","\n","# Drop the temporary 'missing_count' column\n","df_deduplicated_by_missingness = df_deduplicated_by_missingness.drop(columns=['missing_count'])\n","\n","\n","print(f\"Original DataFrame shape: {df_dtypes_fixed.shape}\")\n","print(f\"Deduplicated DataFrame shape: {df_deduplicated_by_missingness.shape}\")\n","\n","# Verify if any duplicates remain based on on_cols (should be 0)\n","print(f\"Duplicates remaining after deduplication: {df_deduplicated_by_missingness.duplicated(subset=on_cols).sum()}\")\n","\n","# inspect the first few rows of the cleaned DataFrame\n","# print(df_deduplicated_by_missingness.head())"],"id":"4e87f314"},{"cell_type":"code","execution_count":null,"metadata":{"id":"5e740248","executionInfo":{"status":"aborted","timestamp":1750152468457,"user_tz":-120,"elapsed":3416,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["df_first_stage_cleaned_data = df_deduplicated_by_missingness\n","df_first_stage_cleaned_data.shape\n"],"id":"5e740248"},{"cell_type":"markdown","metadata":{"id":"30a0ee0d"},"source":["###\n","---"],"id":"30a0ee0d"},{"cell_type":"markdown","metadata":{"id":"1e89e08b"},"source":["### VALIDATION"],"id":"1e89e08b"},{"cell_type":"markdown","metadata":{"id":"78898ec5"},"source":["#### Numeric Validation"],"id":"78898ec5"},{"cell_type":"code","execution_count":null,"metadata":{"id":"70d58d2b","executionInfo":{"status":"aborted","timestamp":1750152468458,"user_tz":-120,"elapsed":3416,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# LOAD DATA - CSVs & DICTIONARIES\n","\n","# Read CSV with proper NA handling. This ensures that empty strings, \"nan\", \"NA\", etc. are treated as NaN.\n","df_num = df_first_stage_cleaned_data\n","\n","# Load and aggregate validation dictionary.\n","sheet_name = sheet_name # <---------------- specify Excel sheet name with the numerical validations\n","feature_dict_mapping = load_and_aggregate_validation_dictionary(feature_dict_filepath, sheet_name)"],"id":"70d58d2b"},{"cell_type":"code","execution_count":null,"metadata":{"id":"a16861f2","executionInfo":{"status":"aborted","timestamp":1750152468459,"user_tz":-120,"elapsed":3417,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# Assign Correct set of features\n","df_features_of_interest_num = df_num[cfg[\"num\"]]"],"id":"a16861f2"},{"cell_type":"code","execution_count":null,"metadata":{"id":"7f76ad50","executionInfo":{"status":"aborted","timestamp":1750152468460,"user_tz":-120,"elapsed":3417,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# Remove Suffixes from features\n","df_removed_suffixes_num = remove_suffixes(df_features_of_interest_num)"],"id":"7f76ad50"},{"cell_type":"code","execution_count":null,"metadata":{"id":"99fa41fd","executionInfo":{"status":"aborted","timestamp":1750152468461,"user_tz":-120,"elapsed":3418,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# Delete disallowed dtypes\n","dtype = 'numeric' # 'numeric', 'datetime', 'boolean', 'categorical'\n","cols_to_clean_num = []\n","df_purified_cols_num = purify_column_num(df_removed_suffixes_num, dtype, cols= cols_to_clean_num, report_path=None)\n","\n","# Optional Report\n","# report_dtypes_disallowed_ZIM"],"id":"99fa41fd"},{"cell_type":"code","execution_count":null,"metadata":{"id":"6d5f451e","executionInfo":{"status":"aborted","timestamp":1750152468461,"user_tz":-120,"elapsed":3418,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# Raise Flags  Numeric\n","skip_columns= []\n","numeric_flags= raise_flags_numeric(df_purified_cols_num, feature_dict_mapping, None, skip_columns=skip_columns)\n","\n","# Optional Reports\n","# report_numeric_flags_zim"],"id":"6d5f451e"},{"cell_type":"code","execution_count":null,"metadata":{"id":"823e5c4a","executionInfo":{"status":"aborted","timestamp":1750152468462,"user_tz":-120,"elapsed":3418,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# Format the weight features to make all values (grams) because some values are grams & some are Kgs.\n","df_weight_fixed = convert_weight_to_kgs(df_purified_cols_num, cols = cfg[\"weight_cols\"])"],"id":"823e5c4a"},{"cell_type":"code","execution_count":null,"metadata":{"id":"2b9504ed","executionInfo":{"status":"aborted","timestamp":1750152468463,"user_tz":-120,"elapsed":3419,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# Remove disallowed values from numeric features - FIX Clinically Implausible values\n","# skip_columns_zim = ['age','bloodsugarmmol', 'dischrr', 'hr', 'rr', 'length', 'nnuadmtemp', 'wcc1r']\n","# skip_columns_mwi = ['age', 'bloodsugarmg', 'bloodsugarmmol', 'dischhr', 'dischrr', 'bsmg', 'hr', 'rr', 'dischtemp']\n","df_removed_numeric_invalids = validate_numeric_ranges(df_weight_fixed, feature_dict_mapping, None, skip_columns= cfg[\"skip_columns\"])\n","\n","# Optional Reports\n","# report_numeric_disallowed_ZIM"],"id":"2b9504ed"},{"cell_type":"code","execution_count":null,"metadata":{"id":"c9026d0c","executionInfo":{"status":"aborted","timestamp":1750152468463,"user_tz":-120,"elapsed":3419,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# Remove duplicates from the numeric DataFrame after validation.\n","df_validated_numeric = df_removed_numeric_invalids.drop_duplicates()"],"id":"c9026d0c"},{"cell_type":"markdown","metadata":{"id":"009431e7"},"source":["#### Boolean Validation"],"id":"009431e7"},{"cell_type":"code","execution_count":null,"metadata":{"id":"66e7a272","executionInfo":{"status":"aborted","timestamp":1750152468464,"user_tz":-120,"elapsed":3419,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# LOAD DATA - CSVs & DICTIONARIES\n","# Read CSV with proper NA handling. This ensures that empty strings, \"nan\", \"NA\", etc. are treated as NaN.\n","df_bool = df_first_stage_cleaned_data"],"id":"66e7a272"},{"cell_type":"code","execution_count":null,"metadata":{"id":"b64f483a","executionInfo":{"status":"aborted","timestamp":1750152468465,"user_tz":-120,"elapsed":3420,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# Assign Correct set of features\n","df_features_of_interest_bool = df_bool[cfg[\"bool\"]]"],"id":"b64f483a"},{"cell_type":"code","execution_count":null,"metadata":{"id":"3b3bd6f7","executionInfo":{"status":"aborted","timestamp":1750152468465,"user_tz":-120,"elapsed":3420,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# Remove Suffixes from features\n","df_removed_suffixes_bool = remove_suffixes(df_features_of_interest_bool)"],"id":"3b3bd6f7"},{"cell_type":"code","execution_count":null,"metadata":{"id":"3a7118cc","executionInfo":{"status":"aborted","timestamp":1750152468466,"user_tz":-120,"elapsed":3421,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# Delete disallowed dtypes\n","dtype = 'boolean' # 'numerical', 'datetime', 'boolean', 'categorical'\n","cols_to_clean_bool = [] # \"disccovidrisk\", \"haart\", \"nvpgiven\", \"phototherapy\", \"hyposymptoms\"\n","df_purified_cols_bool = purify_column_bool(df_removed_suffixes_bool, dtype, cols= cols_to_clean_bool, report_path=None)\n","\n","# Optional Reports\n","# report_dtypes_disallowed_ZIM"],"id":"3a7118cc"},{"cell_type":"code","execution_count":null,"metadata":{"id":"9d48fabd","executionInfo":{"status":"aborted","timestamp":1750152468468,"user_tz":-120,"elapsed":3423,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# Remove duplicates from the boolean DataFrame after validation.\n","df_validated_bool = df_purified_cols_bool.drop_duplicates()"],"id":"9d48fabd"},{"cell_type":"markdown","metadata":{"id":"6acf50d7"},"source":["#### Categorical Validation"],"id":"6acf50d7"},{"cell_type":"code","execution_count":null,"metadata":{"id":"8bf90453","executionInfo":{"status":"aborted","timestamp":1750152468469,"user_tz":-120,"elapsed":3423,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# LOAD DATA - CSVs & DICTIONARIES\n","\n","# Read Data\n","df_cat = df_first_stage_cleaned_data\n","\n","# Load and aggregate data dictionary.\n","dict_mapping = load_and_aggregate_data_dictionary2(cfg[\"dict_filepath\"])\n","logging.info(f\"Loaded dictionary with keys: {list(dict_mapping.keys())[:10]}\")\n"],"id":"8bf90453"},{"cell_type":"code","execution_count":null,"metadata":{"id":"74f325f7","executionInfo":{"status":"aborted","timestamp":1750152468470,"user_tz":-120,"elapsed":3424,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# Assign Correct set of features\n","df_features_of_interest_cat = df_cat[cfg[\"cat\"]]"],"id":"74f325f7"},{"cell_type":"code","execution_count":null,"metadata":{"id":"ce2a2808","executionInfo":{"status":"aborted","timestamp":1750152468470,"user_tz":-120,"elapsed":3424,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# Remove Suffixes from features\n","df_removed_suffixes_cat = remove_suffixes(df_features_of_interest_cat)"],"id":"ce2a2808"},{"cell_type":"code","execution_count":null,"metadata":{"id":"25c620fc","executionInfo":{"status":"aborted","timestamp":1750152468471,"user_tz":-120,"elapsed":3425,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["\n","cols_to_skip = [\"facility\",\"uid\",\"uniquekey\",\"startedatdischarge\", \"completedatdischarge\", \"ingestedatdischarge\"]\n","\n","df_value_replacements_cat = clean_dataframe_with_dictionary(df_removed_suffixes_cat, cfg[\"dict_filepath\"], None, cols_to_skip)\n","\n","# Optional Reports\n","#  report_zim"],"id":"25c620fc"},{"cell_type":"code","execution_count":null,"metadata":{"id":"51d4e7db","executionInfo":{"status":"aborted","timestamp":1750152468472,"user_tz":-120,"elapsed":3425,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["df_delete_disallowed_values_cat = remove_disallowed_values_with_report(cfg[\"values_to_delete\"], df_value_replacements_cat, None)\n","\n","# Optional Reports\n","# disallowed_deletes_zim = r\"C:\\Users\\kmeck\\OneDrive - Imperial College London\\Desktop\\Disallowed_deletes_zim.txt\"\n","# disallowed_deletes_mwi = r\"C:\\Users\\kmeck\\OneDrive - Imperial College London\\Desktop\\Disallowed_deletes_mwi.txt\""],"id":"51d4e7db"},{"cell_type":"code","execution_count":null,"metadata":{"id":"bda87ebf","executionInfo":{"status":"aborted","timestamp":1750152468472,"user_tz":-120,"elapsed":3425,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# Apply the mappings and generate the report.\n","df_aligned = apply_value_mappings_with_report(df_delete_disallowed_values_cat, value_mappings_zim, None)\n","\n","# Optional Reports\n","# report_file_path"],"id":"bda87ebf"},{"cell_type":"code","execution_count":null,"metadata":{"id":"6084ce6c","executionInfo":{"status":"aborted","timestamp":1750152468473,"user_tz":-120,"elapsed":3426,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# Remove 'nan' Category - Replace with NaN\n","# List of columns to exclude\n","cols_to_exclude = [\"facility\",\"uid\",\"uniquekey\",\"startedatdischarge\", \"completedatdischarge\", \"ingestedatdischarge\"]\n","\n","# ZIM\n","# Loop over columns except the excluded ones\n","for col in [c for c in df_aligned.columns if c not in cols_to_exclude]:\n","    if pd.api.types.is_categorical_dtype(df_aligned[col]):\n","        if 'nan' in df_aligned[col].cat.categories:\n","            # Remove the 'nan' category, converting its entries to NaN\n","            df_aligned[col] = df_aligned[col].cat.remove_categories(['nan'])"],"id":"6084ce6c"},{"cell_type":"code","execution_count":null,"metadata":{"id":"1da95291","executionInfo":{"status":"aborted","timestamp":1750152468474,"user_tz":-120,"elapsed":3427,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["df_validated_cat = df_aligned.drop_duplicates()"],"id":"1da95291"},{"cell_type":"markdown","metadata":{"id":"8c01d4a1"},"source":["#### Object Validation"],"id":"8c01d4a1"},{"cell_type":"code","execution_count":null,"metadata":{"id":"57052c7b","executionInfo":{"status":"aborted","timestamp":1750152468475,"user_tz":-120,"elapsed":3427,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# LOAD DATA - CSVs & DICTIONARIES\n","\n","# Read CSV with proper NA handling. This ensures that empty strings, \"nan\", \"NA\", etc. are treated as NaN.\n","df_obj = df_first_stage_cleaned_data\n","\n","# Load and aggregate data dictionary.\n","dict_mapping = load_and_aggregate_data_dictionary2(cfg[\"dict_filepath\"])\n","logging.info(f\"Loaded dictionary with keys: {list(dict_mapping.keys())[:10]}\")\n"],"id":"57052c7b"},{"cell_type":"code","execution_count":null,"metadata":{"id":"bcd9e123","executionInfo":{"status":"aborted","timestamp":1750152468476,"user_tz":-120,"elapsed":3427,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# Assign Correct set of features\n","df_features_of_interest_obj = df_obj[cfg[\"obj\"]]"],"id":"bcd9e123"},{"cell_type":"code","execution_count":null,"metadata":{"id":"4d77d998","executionInfo":{"status":"aborted","timestamp":1750152468546,"user_tz":-120,"elapsed":3496,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# Remove Suffixes from features\n","df_removed_suffixes_obj = remove_suffixes(df_features_of_interest_obj)"],"id":"4d77d998"},{"cell_type":"code","execution_count":null,"metadata":{"id":"9e75006c","executionInfo":{"status":"aborted","timestamp":1750152468547,"user_tz":-120,"elapsed":3497,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["cols_to_skip = [\"facility\",\"uid\",\"uniquekey\",\"startedatdischarge\", \"completedatdischarge\", \"ingestedatdischarge\"]\n","\n","df_value_replacements_obj = clean_dataframe_with_dictionary(df_removed_suffixes_obj, cfg[\"dict_filepath\"], None, cols_to_skip)\n","\n","# Optional Reports\n","# report_zim"],"id":"9e75006c"},{"cell_type":"code","execution_count":null,"metadata":{"id":"1967faad","executionInfo":{"status":"aborted","timestamp":1750152468548,"user_tz":-120,"elapsed":3497,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["df_validated_obj = df_value_replacements_obj.drop_duplicates()"],"id":"1967faad"},{"cell_type":"markdown","metadata":{"id":"67b1d4e7"},"source":["#### Datetime Validation"],"id":"67b1d4e7"},{"cell_type":"code","execution_count":null,"metadata":{"id":"74b777f6","executionInfo":{"status":"aborted","timestamp":1750152468549,"user_tz":-120,"elapsed":3498,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# LOAD DATA - CSVs & DICTIONARIES\n","\n","# Read CSV with proper NA handling. This ensures that empty strings, \"nan\", \"NA\", etc. are treated as NaN.\n","df_dt = df_first_stage_cleaned_data"],"id":"74b777f6"},{"cell_type":"code","execution_count":null,"metadata":{"id":"5050eced","executionInfo":{"status":"aborted","timestamp":1750152468550,"user_tz":-120,"elapsed":3499,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# Assign Correct set of features\n","df_features_of_interest_dt = df_dt[cfg[\"dt\"]]"],"id":"5050eced"},{"cell_type":"code","execution_count":null,"metadata":{"id":"c9616e7c","executionInfo":{"status":"aborted","timestamp":1750152468553,"user_tz":-120,"elapsed":3501,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# Remove Suffixes from features\n","df_removed_suffixes_dt = remove_suffixes(df_features_of_interest_dt)"],"id":"c9616e7c"},{"cell_type":"code","execution_count":null,"metadata":{"id":"3b64bcd5","executionInfo":{"status":"aborted","timestamp":1750152468555,"user_tz":-120,"elapsed":3503,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# Delete disallowed dtypes\n","dtype = 'datetime' # 'numerical', 'datetime', 'boolean', 'categorical'\n","cols_to_clean_dt = []\n","df_purified_cols_dt = purify_column_dt(df_removed_suffixes_dt, dtype, cols= cols_to_clean_dt, report_path=None)\n","\n","# Optional Reports\n","# report_dtypes_disallowed_ZIM\n"],"id":"3b64bcd5"},{"cell_type":"code","execution_count":null,"metadata":{"id":"983c9976","executionInfo":{"status":"aborted","timestamp":1750152468555,"user_tz":-120,"elapsed":3502,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# REPLACE 'nan' with NaT\n","\n","cols_to_exclude = [\"facility\",\"uid\",\"uniquekey\"]\n","\n","# Loop over columns except the excluded ones\n","for col in [c for c in df_purified_cols_dt.columns if c not in cols_to_exclude]:\n","    if pd.api.types.is_datetime64_any_dtype(df_purified_cols_dt[col]):\n","        # Convert the column to object to safely replace 'nan'\n","        df_purified_cols_dt[col] = df_purified_cols_dt[col].astype(object).replace('nan', pd.NaT)\n","        # Convert back to datetime, coercing errors to NaT if necessary\n","        df_purified_cols_dt[col] = pd.to_datetime(df_purified_cols_dt[col], errors='coerce')\n"],"id":"983c9976"},{"cell_type":"code","execution_count":null,"metadata":{"id":"e428e6a0","executionInfo":{"status":"aborted","timestamp":1750152468556,"user_tz":-120,"elapsed":3503,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["df_validated_dt = df_purified_cols_dt.drop_duplicates()"],"id":"e428e6a0"},{"cell_type":"markdown","metadata":{"id":"ffa44a7f"},"source":["###\n","---"],"id":"ffa44a7f"},{"cell_type":"markdown","metadata":{"id":"0d60f74a"},"source":["### DATA MERGING & CLEANED OUTPUTS"],"id":"0d60f74a"},{"cell_type":"code","execution_count":null,"metadata":{"id":"b8b7ea77","executionInfo":{"status":"aborted","timestamp":1750152468557,"user_tz":-120,"elapsed":3504,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# Merge the Validated Data Into one Dataframe\n","\n","df_merged = merge_validated_data(\n","    df_validated_numeric,\n","    df_validated_bool,\n","    df_validated_cat,\n","    df_validated_obj,\n","    df_validated_dt,\n","    on=[\"facility\", \"uid\", \"uniquekey\",\"startedatdischarge\", \"completedatdischarge\", \"ingestedatdischarge\"],\n","    how=\"inner\",\n","    csv_path= merged_data_csv,\n","    pkl_path= merged_data_pkl\n",")"],"id":"b8b7ea77"},{"cell_type":"markdown","metadata":{"id":"7e914769"},"source":["###\n","---"],"id":"7e914769"},{"cell_type":"markdown","metadata":{"id":"ce621721"},"source":["## OUTPUT"],"id":"ce621721"},{"cell_type":"code","execution_count":null,"metadata":{"id":"d7913642","executionInfo":{"status":"aborted","timestamp":1750152468557,"user_tz":-120,"elapsed":3498,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["df_merged"],"id":"d7913642"},{"cell_type":"code","execution_count":null,"metadata":{"id":"4dcb74a0","executionInfo":{"status":"aborted","timestamp":1750152468558,"user_tz":-120,"elapsed":3492,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"outputs":[],"source":["# CODE RUNTIME\n","#-----------------\n","notebook_end_time = time.time()\n","total_runtime = notebook_end_time - notebook_start_time\n","minutes = int(total_runtime // 60)\n","seconds = int(total_runtime % 60)\n","print(f\"‚è±Ô∏è Notebook runtime: {minutes} min {seconds} sec\")"],"id":"4dcb74a0"},{"cell_type":"markdown","metadata":{"id":"894ebc5b"},"source":["# ~END"],"id":"894ebc5b"},{"cell_type":"code","source":[],"metadata":{"id":"bFSF5HWhKhCo","executionInfo":{"status":"aborted","timestamp":1750152468581,"user_tz":-120,"elapsed":3514,"user":{"displayName":"Kevin Meck","userId":"09368900277097431106"}}},"id":"bFSF5HWhKhCo","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"ml4ns-env","language":"python","name":"ml4ns-env"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}